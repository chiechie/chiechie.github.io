---
title: 从DeepAR谈论paper到工程化的距离
author: chiechie
mathjax: true
date: 2021-04-18 11:56:59
tags:
- DeepAR
- AIOps
- 知识分子
- RNN
- 时间序列
categories:
- AI
---

## DeepAR探索

以DeepAR为例，回忆下我们从探索到落地的过程：

- Student： 网上找到了pytorch版本的deepAR版（发于2017年4月），训练过程很慢要7个小时，虽然paper里面也号称要7个小时，是优化效率还是直接上线呢？
- Teacher： 生产环境，这种性能实在不能接受。既然amazon将DeepAR作为时序预测场景的主推算法方案，这种性能肯定拿不出手，后续肯定做了的优化，去查一下近3年的amazon关于DeepAR相关的资料，重点看看有没有更优秀的实现方案。
- Student：找到了--基于MXNET的gluon-ts（于2019年6月发表），mxnet在electricty上的训练和预测时间大概是10min左右。
- Teacher： nice！

最开始把gluon-ts实现的DeepAR算法当黑盒用的时候，发现效果很神奇，不调参，换不同数据进去效果都还挺好，周期性和趋势性都还能学到。

但是就觉得很奇怪，按照自己构建RNN的经验，调参就要反反复复好多次，最后效果还不一定好。使用gluon-TS的代码我没有调参的，并且默认的网络结构很简单 不太可能过拟合.

看了源代码之后，有一个很大的感受就是，**完成论文demo和完成一个业界可用方案，两者的工作量 ，可能是0到1再到100的距离。**

按照paper所说，整个网络结构非常之简单，1层rnn + 1层 dense层（输出分布参数），构成2层rnn，大概十几行代码。 但是，**真正大量的代码都在做预处理以及 模型训练的一些小trick。 这些小trick，对提升模型效果非常有帮助，但是paper里面是没有的，应该随是工程团队总结出的最佳实践**。

- best practice1: 把每个样本（历史依赖）的scale 取对数，放入样本。 （因为所有的lags项在归一化之后已经丢失了scale信息，为了避免这个信息的丢失，还是把log scale 放入特征，用于区别不同的曲线的模式）
- best practice2: 在rnn中，每一层rnn后都接了一个residual和一个dropout。（原文没有提到）
- best practice3: 为了学习到数据的周期性模式，除了一定要有的context_length，代码还会跳着取lag , 类似空洞卷积。这些周期性的lag项，是被硬编码在代码里面的，比如年/月/日的周期性。

（我甚至怀疑论文的作者都不知道，因为作者自己写了一个pytorch版本，没有见到相关的处理，并且训练时长是gluon-TS的100倍以上）


## 对于AIOps建设的启发

对于根因分析，日志关联等大部分场景，业界没有可参考的工具，转而求助学术界，可参考论文以及kaggle比赛等。

但是要注意，这些方案只是demo，没有在工业界的数据和真实需求上验证过，实际上真实业务环境会比做实验要复杂得多。

从调研demo到完成一个业务上高可用的方案，有大量的优化工作。评估时间的时候，需要预留buffer，来验证和调优，可能需要迭代多轮，有不确定性。


## 从paper到工程落地的距离？

- 实验室方案不能直接搬到 真实业务的 几个原因：
    - 样本数据不一致：
        - 实验室的数据有一些 标记质量差，以及标记的标准不一致。
        - 实验室的数据模式单一，很多是模拟生成的，没法模拟出真实业务上复杂的模式
        - 真实环境中数据质量差：历史数据不够，缺失值多，有用特征缺失。
        - 实验室数据格式 千奇百怪，要适当预处理才能给 业务使用
    - 评估指标不一致
        - 实验室只能基于已有的封闭数据集，业务上的指标是开放式的跟业务运营相关的
    - 效果不一致
        - 实验 中效果好的方案 到真实环境中 效果不好
    - 性能要求不一致：
        - 实验室方案运行效率到不到真实环境的标准
        - eg，kaggle比赛常用模型ensenmble方案，在工业界就很少会用，计算效率的问题å

- 正常的开发流程是：
    - 理解论文原理
    - 复现论文：开发demo，
    - 工程优化（beta）：初步上线，demo性能优化，覆盖现实中可能的异常
    - 稳定上线（正式）： 在真实环境上测试一段时间。
    - 产品化：给其他无背景的人用
    