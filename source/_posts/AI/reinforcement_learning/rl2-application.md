---
title: 强化学习2 应用
author: chiechie
mathjax: true
date: 2021-04-21 08:22:04
tags:
- 强化学习
- 人工智能
- auto ml
- NAS
categories:
- AI
---

## 应用场景

1. 商品推荐: 每个用户的信息作为state，推荐的商品是action，长期看用户的点击率/浏览时长/消费额reward
2. 网约车调度：<地址，时间>为state为，每个地址在每个时间的冷门程度是value。
3. 自动生成SQL语句：
4. 神经网络结构搜索(NAS)：
   
    > NAS is closely related to hyperparameter optimization and is a subfield of automated machine learning (AutoML)
5. 游戏：生成NPC
6. 交易机器人
7. 控制机器人


### 商品推荐

强化学习可以讲一个很美好的故事：数据驱动的自动化运营。

实际落地时，强化学习的效果如何？强化学习很难调，实际效果没有监督学习好。

系统太复杂又是个黑盒，人hold不住。


facebook在内容推荐技术的很多细分场景上开始应用强化学习，eg消息推送时，以用户的长期活跃为目标，而不是短期打开率为目标。 

增强学习的好处是把两万名算法工程师从调参工作中解放出来，让分发策略在长期创造更多价值。 


### 网约车调度

滴滴的paper是这么写的，不知道生产系统是不是这么用的。

### 神经网络结构搜索（NAS）

就好像下一盘棋一样，一局玩完可能要很久，所以每走一步，都可以，判断当前这步棋走的水平如何。

但是只有当一盘都下完了，我们才真的知道具体的reward。

NAS是auto-ml的子领域，为了让机器去设计神经网络。
2017年谷歌发了一篇paper，使用基于强化学习的思路，去设计网络结构。
具体思路是这样的，假设要设计一个20层的CNN，超参个数为20*3，即每一层卷积，需要设置：

- 卷积核数量
- 卷积核大小
- 步长大小

怎么定义为强化学习的问题呢？

- state：截至目前，已经确定的超参。
- action：超参取值
- policy value function：选择某组超参，长期收益，也就是验证集上的准确率。
- 策略网络：一个RNN。

输入向量 $x_t$ 是对上一个超参数 $a_{t−1}$ 做 Embedding 得到的

策略函数为什么不用监督算法拟合（like 贝叶斯优化里面的的高斯过程回归）？
因为超参有可能是离散的，也就是说，损失和奖励对于策略网络参数$\theta$不可微。(btw,贝叶斯优化本身也处理不了离散的超参。)


应用强化学习的代价？需要大量的训练样本，至少上万个奖励，即从初始化开始训练几万个CNN，计算量非常大。

后面学术界提出了更优秀的NAS方法--DARTS 及其变体。


### 交易机器人

使用强化学习构建股票交易机器人，看到github有一个项目 [使用强化学习炒股](https://github.com/wangshub/RL-Stock)。

大概建模思路是这样，state是当前的市场行情数据，action是买/卖/hold，reward就是盈利。

这个思路是可以的，监督学习最多可以构建一个简单的基于预测的交易策略，比如高买低卖。强化学习可能可以学到更多的人意想不到的交易策略。

但是，因为模型的黑盒和不可预测性，类似无人驾驶，无人运维，很难让人放心把这么重要的事情交给一个机器人手里。


## 什么制约强化学习的应用？

到目前为止，强化学习最成功的应用仍然是Atari游戏、围棋游戏、星际争霸游戏。
工业界有很多应用尝试过强化学习，但是其中成功的并不多。
究竟是什么在制约深度强化学习的落地应用呢？

### 所需要的样本数量过大

举几个简单例子

1. Atari游戏属于最简单的电子游戏，Rainbow DQN需要1800万帧才能达到人类玩家水平。
2. AlphaGo Zero用了2900万局自我博弈，每一局约有100个状态和动作。
3. TD3算法在MuJoCo物理仿真环境中训练模拟机器人时，虽然只有几个关节需要控制，但是在样本数量100万时尚未收敛

复杂的场景，需要的样本更多。

在电子游戏中获取上亿万的样本并不难，但在现实问题中每获取一个样本都是比较 困难的。比如

1. 在NAS的例子中，每获取一个奖励，需要训练一个 CNN，从初始 化到梯度算法收敛，需要一个 GPU 约一小时的计算量
2. 机械手臂抓取一个物体至少需要几秒钟时间，那么一天只能收集1w个样本;同时用十个机械手臂，连续运转一百天，才能收集到1000w个样本，未必够训练一个深度强化学习模型。

强化学习所需的样本量太大，这会限制强化学习在现实中 的应用。


### 探索阶段代价太大

1. 强化学习要求智能体与环境交互，用收集到的经验去更新策略。在交互的过程中，智能体会改变环境.
2. 在仿真、游戏的环境中，智能体对环境造成任何影响都无所谓。但是在现实世界中，智能体对环境的影响可能会造成巨大的代价。
3. 在强化学习初始的探索阶段，策略几乎是随机的。如果是物理世界中的应用，智能 体的动作难免造成很大的代价。如果应用到推荐系统中，如果上线一个随机的推荐策略， 那么用户的体验会极差，很低的点击率也会给网站造成收入的损失。如果应用到自动驾 驶中，随机的控制策略会导致车辆撞毁。如果应用到医疗中，随机的治疗方案会致死致 残。
4. 在物理世界的应用中，不能直接让初始的随机策略与环境交互，而应该先对策略做预训练，再在真实环境中部署。一种方法是事先准备一个数据集，用行为克隆等监督学习方法做预训练。另一种方法是搭建模拟器，在模拟器中预训练策略。比如阿里巴巴提 出的“虚拟淘宝”系统是对真实用户的模仿，用这样的模拟器预训练推荐策略。离线强化学习 (Offline RL) 是一个热门而又有价值的研究方向.


### 调参难


强化学习的超参数分两种:神经网络结构超参数、算法超参数。这两类超参数的设置都严重影响实验效果：

- 结构超参数: 神经网络结构超参数包括层数、宽度、激活函数。在监督学习中，在隐层中用不同的激活函数(比如 ReLU、 Leaky ReLU)对结果影响很小，因此用ReLU就可以。但是在深度强化学习中，隐层激活函数对结果的影响很大;有时ReLU远好于Leaky ReLU，有时Leaky ReLU远好于ReLU。由于这种不一致性，在实践中不得不尝试不同的激活函数。
- 算法超参数: 强化学习中的算法超参数很多，包括学习率、批大小 (Batch Size)、经验回放的参数、探索用的噪声，比如Rainbow调了超过10种算法超参数
     
   - 学习率(即梯度算法的步长)对结果的影响非常大，并且DDPG、 TD3、A2C等方法中不止有一个学习率。策略网络、价值网络、目标网络中都有各自的学习率。
   - 如果用经验回放，还有几个超参：回放数组的大小、经验回放的起始时间等。论文[37]中的实验显示回放数组的大小对结果有影响，过大或者过小都不好。经验回放的起始时间也需要调，比如Rainbow在收集到8万条四元组时开始经验回放，而标准的DQN则最好是在收集到20万条之后开始经验回放[49]。
   - 在探索阶段，DQN、DPG等方法的动作中应当加入一定噪声。「噪声的大小」是需要调的超参数，它可以平衡Exploration和Exploitation。除了设置初始的噪声幅度，还要设置噪声的衰减率，让噪声逐渐变小。
   
   

#### 实验效果严重依赖于实现的好坏

上面的讨论目的在于说明超参数对结果有重大影响。对于相同的方法，不同的人会有不同的实现，比如用不同的网络结构、激活函数、训练算法、学习率、经验回放、噪声。哪怕是一些细微的区别，也会影响最终的效果。论文[48]使用了几个比较有名的开源代码，它们都有TRPO和DDPG方法在Half-Cheetah环境中的实验。论文使用了它们的默认设置，比较了实验结果，如图 19.18 所示。很显然， 相同的方法，不同人的编程实现，实验效果差距巨大。

![19.18 ](img_1.png)


#### 实验对比的可靠性问题

 如果一篇学术论文提出一种新的方法，往往要在Atari、 MuJoCo 等标准的实验环境中做实验，并与DQN、DDPG、TD3、A2C、TRPO 等有名的基线做实验对照。通常只有当新的方法效果显著优于基线时，论文才有可能发表。但是基线算法的表现严重依赖编程实现的好坏。如果你提出一种新方法，可以把自己的方法实现得非常好，然后从开源的实现中选一个不那么好的基线做实验对比，就可以轻松打败基线算法啦。



### 稳定性极差

强化学习训练的过程中充满了随机性。除了环境的随机性之外，随机性还来自于神经网络随机初始化、决策的随机性、经验回放的随机性。想必大家都有这样的经历:用完全相同的程序和超参数，仅更改随机种子 (Random Seed)，就会导致训练的效果有天壤之别。

如图 19.19所示，如果重复训练十次，往往会有几次完全不收敛。哪怕是非常简单的问题，也会出现这种不收敛的情形。


![图 19.19](img.png)

在监督学习中，由于随机初始化和随机梯度中的随机性，即使用同样的超参数，训练出的模型表现也会不一致，测试准确率可能会差**几个百分点**。但是监督学习中几乎不会出现图 19.19 中这种情形;如果出现了，几乎可以肯定代码中有错。但是强化学习确实会出现完全不收敛的情形，哪怕代码和超参数都是对的。


## 参考
1. [rct的混沌球算法](https://rct.ai/zh-hans/blog/the-key-technology-behind-morpheus-engine)
2. [DRL-wangshusen中文教材](https://github.com/wangshusen/DRL/tree/master/Notes_CN)
3. [NAS-介绍](https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html)
4. [NAS-wiki](https://en.wikipedia.org/wiki/Neural_architecture_search)
5. [DARTS: Differentiable Architecture Search-paper](https://arxiv.org/abs/1806.09055)
6. [chiechie: 贝叶斯优化](https://chiechie.github.io/2021/03/24/technology/bayes-optimization//)
7. [使用强化学习炒股](https://github.com/wangshub/RL-Stock)