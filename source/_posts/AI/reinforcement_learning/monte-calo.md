---
title: 强化学习5 蒙特卡洛树搜索
author: chiechie
mathjax: true
date: 2021-06-02 16:58:05
tags:
- 强化学习
- 人工智能
categories:
- AI
---



# 总结
1. 蒙特卡洛树搜索（MCTS）是一种基于模型的强化学习方法。
2. 蒙特卡洛树搜索比价值学习和策略学习更难。
3. AlphaGp是一个围棋AI，依靠MCTS做决策，决策过程中需要策略网络和价值网络辅助。
4. 假设已经有了策略网络和价值网络，AlphaGo真正跟人下棋的时候，做决策的不是策略网络或者价值网络，而是MCTS。
MCTS不需要训练，可以直接做决策，训练策略网络和价值网络的目的是辅助MCTS。
   
## MCTS的基本思想
1. 思考，高手是怎么下棋的？高手一般都会往前看几步，越是高手，看的越远。
2. 高手下棋，一般是动态看待局势，确保自己几步之后占据优势。如果只根据当前格局做判断，不考虑长远，肯定赢不了高手。
3. 同理，AI下棋也应该跟高手一样，多往前看几步，枚举维阿利可能发生的情况，从而判断当前执行什么动作的胜算最大，这样远浩宇用策略网络输出一个动作。
4. AlphaGo每走一步，就要用MCTS做成千上万次模拟，基本思想如下：假设当前有3种可选的动作，每次模拟从三种动作中选出一种，然后将游戏进行到底，从而拿到最终结果。
重复成千上万次模拟，统计每种动作的胜负概率，发现胜率分别是48%、56%、52%。那么 AlphaGo 应当执行第二种动作，因为 它的胜算最大。然而实际做起来还有很多难点需要解决。

## MCTS的四个步骤

MCTS每次模拟都要经历四个步骤：选择（selection），扩展（expansion），估值（evaluation），回溯（backup）

### 选择

1. 观察当前局势，找出符合规则的所有走位中最有胜算的那部分走位。
如何评估走位优劣？综合两方面：
   
1. 看走位（a）的胜率，就是动作价值$Q(a)$；
2. 看策略网络给a的评分,$\pi(a|s)$

### 扩展

想要用 MCTS 做决策，必须要有模拟器，而搭建模拟器的关键在于构造正确的状态 转移函数 p(sk+1|sk, ak)。从搭建模拟器的角度来看，围棋是非常简单的问题:由于围棋 的对称性，可以用策略网络作为状态转移函数。但是对于大多数的实际问题，构造状态 转移函数是非常困难的。比如机器人、无人车等应用，状态转移的构造需要物理模型，要 考虑到力、运动、以及外部世界的干扰。如果物理模型不够准确，导致状态转移函数偏 离事实太远，那么 MCTS 的模拟结果就不可靠。


# 参考
1. [HotSpot-英文paper](https://netman.aiops.org/wp-content/uploads/2018/12/sunyq_IEEEAccess2018_HotSpot.pdf)
2. [HotSpot-中文](https://mp.weixin.qq.com/s/Kj309bzifIv4j80nZbGVZw)