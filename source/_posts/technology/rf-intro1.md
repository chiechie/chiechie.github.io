---
title: zhoubolei-强化学习1-基本概念
author: chiechie
mathjax: true
date: 2021-04-18 20:17:45
tags:
- 强化学习
- 人工智能
categories:
- 技术
---

### why

可能可以超越人类。监督学习 只能模仿，不能超越

### 强化学习三要素

- model： 状态转移，模拟计算reward2个收益: 长期和即时收益。这两个收益都是随机变量，怎么利用收益函数来 引导agent做出好的策略？对收益求期望，也就是价值函数
- value：3个价值函数: 行动价值函数，最优行动价值函数，状态价值函数
- policy：策略

  
### 监督学习 和 强化学习的区别

- 监督学习
    - 要求数据 是独立同分布的
    - 学习过程需要被告知，labels是什么
- 强化学习
    - 数据不用iid
    - 正确的action不能获取即时的正反馈
- 两者对比
    - 强化学习需要序列数据作为输入（不是iid）
    - 强化学习本身不会被告知正确的action是什么，而是需要自己去试错，找到具备最大长期reward的action
    - 试错机制--在探索 和 利用中间寻找平衡
    - 强化学习没有一个导师（supervisor），只有一个奖励信号，并且还是有延迟的。

### 强化学习的特点

- 强化学习中随机性的两个来源：action可以是随机的，状态转移可以是随机的
- 试错机制
- 延迟reward
- 时间会产生影响
- agent的行为会影响后续收到的数据（agent的action改变了环境）
    - 类似我们的模型自动更新，担心时间会对模型造成影响
    - 如果是增量学习，会造成影响
    
