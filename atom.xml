<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chiechie&#39;s Mini World</title>
  
  <subtitle>Set your course by the stars, not by the lights of passing ships. —— Omar Bradley</subtitle>
  <link href="https://chiechie.github.io/atom.xml" rel="self"/>
  
  <link href="https://chiechie.github.io/"/>
  <updated>2021-07-12T03:40:14.390Z</updated>
  <id>https://chiechie.github.io/</id>
  
  <author>
    <name>Chiechie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据结构2 非线性数据结构</title>
    <link href="https://chiechie.github.io/2021/07/11/data_structure/ds2-nonelinear-data-structure/"/>
    <id>https://chiechie.github.io/2021/07/11/data_structure/ds2-nonelinear-data-structure/</id>
    <published>2021-07-11T05:20:51.000Z</published>
    <updated>2021-07-12T03:40:14.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总结">总结</h2><ol type="1"><li>非线形数据结构包括堆，Union Find或者disjoint set。</li></ol><h2 id="堆和优先队列">堆和优先队列</h2><ol type="1"><li><p>优先队列（priority queue）是什么？优先队列是一个抽象数据类型（ADT），跟一般的normal很类似，除了一点，优先队列中的每个元素都有固定的优先级，优先级越高的元素，越先出队（dequeue）。</p></li><li><p>注意，优先队列只能存储可比较的数据。</p></li><li><p>优先队列出队（poll）是如何找到最优先的成员的？借助堆（heap）。</p></li><li><p>优先队列通常是使堆实现，因为对优先队列进行操作时，使用堆构造的优先队列时间复杂度最低。除了堆还有别的数据结构也可以实现优先队列，例如一个无序的链表，但是时间复杂度较高。</p></li><li><p>堆（heap）是什么？ 堆是一个基于树的数据结构，它满足堆不变性质（也叫堆属性）：堆中的所有的父节点的优先级都大于或者孩子节点的value。最大堆（max heap）是指，父节点的value都大于等于孩子节点。最小堆（min heap）是指，小于或者等于。 <img src="./image-20210711093249240.png" /></p></li><li><p>什么时候需要使用优先队列？在图算法中会经常用到</p><ol type="1"><li>最短路径搜索算法--Dijkstra;s</li><li>动态获取下一个最好的或者最差的元素</li><li>huffman编码（无损数据压缩）</li><li>图遍厉中的Best优先搜索算法，例如A*，使用优先队列持续获取下一个最有潜力的节点</li><li>最小生成树（Mininum Spannning Tree， MST）算法中会用到。</li></ol></li><li><p>将最小优先队列转换为最大优先队列：很多编程语言的标准库只提供最小优先队列的抽象数据类型，即按照值越小，优先级越高，但是实际应用中我们也会需要用到最大优先队列。如何使用已有的最小优先队列来满足我们构造最大优先队列的需求呢？将元素的值取相反数，然后使用最小优先队列排序。</p></li><li><p>堆又很多种，binary heap/binomial heap等，最常用的堆是<strong>二叉堆</strong>（binary heap）。二叉堆是一个二叉树，并且支持堆不变性，二叉树中，每个节点都有2个孩子。</p></li><li><p>完全二叉树：除了最后一层，每一层都是完全填充的，并且节点都会尽量往左边排。完全二叉树在数据插入的时候很有用</p><p><img src="./image-20210711100432168.png" /></p></li><li><p>怎么表示二叉堆？--数组。</p><p><img src="./image-20210711102017358.png" /></p></li><li><p>找到某个节点的父亲节点和孩子节点，利用左下方的公式计算相应的父节点/子节点的索引，然后去数组查找该索引对应的取值。 <img src="./image-20210711102047154.png" /></p></li><li><p>在二叉堆中添加元素：先将该元素添加到最后一层的，最后一个节点的右边，然后不断的使用冒泡的方法，跟父亲节点互换位置,</p></li><li><p>从二叉堆中删除顶端元素，时间复杂度为O（logn），从二叉堆中删除某个值的元素，时间复杂度为O(n), 因为时间花在找到这个元素上面了。</p></li><li><p>有没有办法可以提升查找二叉堆中某个元素速度的方法呢？hash table，即提前将value和索引pair存到一个table中，要搜索的时候去查这个table就好了。这个表叫哈希表（hashtable）， key是hash（value），value是索引。如果有多个相同的值，就存成一个index set</p><p><img src="./image-20210711104111050.png" /></p></li></ol><h2 id="union-find">Union-Find</h2><ol type="1"><li><p>Union-Find 也叫非连通集合（disjoint set），也是一种数据结构，主要操作是find 合uninion</p><ol type="1"><li><p>find：给定一个element，union find 返回该element所属的group</p></li><li><p>uninon：merge 2 groups，将其中一个group的root变为另外一个group的root的子节点</p><p><img src="./image-20210711110158015.png" /></p></li></ol></li><li><p>union find在哪里会被用到？最小生成树算法--kruskal：</p><p><img src="./image-20210711110757010.png" /></p></li><li><p>Union find这个数据结构可以使用hashtable + 数组实现</p><p><img src="./image-20210711115717168.png" /></p></li><li><p>union find 应用之--path压缩, 每次union的时候，将属于某个pattern的所有节点全部指向root</p></li></ol><h2 id="二叉树-和-二叉查找树">二叉树 和 二叉查找树</h2><ol type="1"><li><p>树是什么？一个无向图，无环的连通图，有N个节点，N-1条边</p></li><li><p>树中任意一个节点都能成为一个root节点。</p></li><li><p>子树（subtrees）可能是一个叶子结点</p></li><li><p>二叉树：是一种特殊的树，树中每个节点最多只有2个子节点。</p></li><li><p>二叉查找树（binary seach tree）是一种特殊的二叉树，树中的所有子树都满足二叉查找树不变性（BST invariant），即左边的树更小，右边的树更大。</p></li><li><p>二叉查找树有什么用？</p><ol type="1"><li>可以实现很多抽象的数据类型。例如map，set</li><li>可以用来实现平衡二叉查找树（balanced binar用search trees）</li><li>实现语法树：编译器和计算机会用到</li><li>Treap--一个基于概率的数据结构，使用了一个随机的二叉搜索树。</li></ol></li><li><p>如何在二叉查找树中插入一个元素？二叉查找树要求元素可比较大小，先查找插入位置，然后插入，注意这是一个贪婪的对此迭代的过程，总共有4种位置以及对应的行动</p><ol type="1"><li>if 插入值&lt; case: recurse down 左子树</li><li>if 插入值&gt; case:recurse down 右子树</li><li>if 插入值== case: do something</li><li>if 无节点可供插入值比较:的创建一个新的节点</li></ol></li><li><p>平均来看，在二叉查找树种插入一个元素需要对数时间，在最坏情况下（如下图， 需要平衡二叉搜索树），插入一个元素退化为线性时间，</p><figure><img src="./image-20210712095559490.png" alt="image-20210712095559490" /><figcaption aria-hidden="true">image-20210712095559490</figcaption></figure></li><li><p>在二叉查找树中删除一个元素的步骤：</p><ol type="1"><li><p>找到元素在树种的位置：</p><ol type="1"><li>使用贪婪算法从根节点点开始持续查找，当我们找到一个null戒电视，迭代结束，意味着这个二叉树中没有这个元素。</li><li>每遍历到一个点，比较当前值和value的大小，当相等时，返回；当value小于该节点时，去左子树继续找；当value=该节点时，去右子树继续找。</li></ol></li><li><p>找到了二叉查找树种想删除节点，要删除该节点了，也面临4种情况：这个节点在是叶子结点；这个节点只有右子树；这个节点只有左子树；既有左子树又有右子树。</p><figure><img src="./image-20210712100625802.png" alt="image-20210712100625802" /><figcaption aria-hidden="true">image-20210712100625802</figcaption></figure><ul><li>叶子节点直接删除，节点只有一个子树的，将子树顶上来</li><li>最复杂的是被删除的节点有两个子树，可以找左子树的最大的节点/右子树最小的节点，将这个值 复制到root（上图中的⭕️），注意 左子树的最大的节点/右子树最小的节点 是左子树最右边路径的叶子，是右子树最左边路径的叶子，删除该节点的操作必然输入case1/2/3的一种，即这个节点最多包含一个子树。</li></ul></li></ol></li></ol><h2 id="树遍历算法">树遍历算法</h2><ol type="1"><li><p>树遍历的三个算法：先序遍历(preorder traversal)，中序遍历（inorder traversal）后序遍历（postorder traversal）。递归调用，开始遍历到一个节点，将这个节点入栈，该节点遍历完，出栈。</p></li><li><p>先序遍历(preorder traversal)：先打印当前节点的值，再开始遍历左右节点。top down</p><figure><img src="./image-20210712105721400.png" alt="image-20210712105721400" /><figcaption aria-hidden="true">image-20210712105721400</figcaption></figure></li><li><p>中序遍历（inorder traversal）：把左子树遍历完，打印当前值，开始遍历右子树。bottom up， 块状优先</p><figure><img src="./image-20210712105759814.png" alt="image-20210712105759814" /><figcaption aria-hidden="true">image-20210712105759814</figcaption></figure></li><li><p>后序遍历（postorder traversal）：遍历完子节点后，打印当前节点的值，bottem up的方式。<img src="./image-20210712105840283.png" alt="image-20210712105840283" /></p></li><li><p>level order 遍历，需要广度优先搜索(Breadth First Search) 以top down的方式遍历</p></li><li><p>怎么实现广度优先搜索(Breadth First Search)?</p><ol type="1"><li>构造一个队列，放置当前待访问的节点。初始值是root，</li><li>每访问一个节点，让这个节点出列（dequeue），将该节点的所有子节点入列（enqueue）。</li><li>直到访问到最后一层，只dequeue没有enque，queue变为空时，就访问完成</li></ol></li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.youtube.com/watch?v=RBSGKlAvoiM&amp;t=11600s">Data Structures Easy to Advanced Course - Full Tutorial from a Google Engineer</a></li><li></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;非线形数据结构包括堆，Union Find或者disjoint set。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;堆和优先队列&quot;&gt;堆和优先队列&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;优先队列（</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="图数据" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"/>
    
    <category term="编程" scheme="https://chiechie.github.io/tags/%E7%BC%96%E7%A8%8B/"/>
    
    <category term="数据结构" scheme="https://chiechie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>SVD和compression</title>
    <link href="https://chiechie.github.io/2021/07/10/AI/svd-and-coding/"/>
    <id>https://chiechie.github.io/2021/07/10/AI/svd-and-coding/</id>
    <published>2021-07-10T03:40:31.000Z</published>
    <updated>2021-07-12T04:10:47.367Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>大矩阵怎么存？最简单的方式就是矩阵分块, 然后分chunk存/读/算，下一个chunk覆盖上一个chunk，始终占用一个chunk的内存。</p><p>如果还想去噪，可以用svd，下面推演一下用svd可以节省多少空间</p></blockquote><p>SVD技术可以应用于信号压缩</p><p>SVD是这样的, 随意一个矩阵A可以做如下分解</p><p><span class="math display">\[A = U\Lambda V^T\]</span>==&gt;<span class="math display">\[U^T A = \Lambda V^T  \]</span></p><p>可以把<span class="math inline">\(U^T\)</span>看成是encoding, <span class="math inline">\(U\)</span>看成是decoding, 压缩之后的信号是<span class="math inline">\(\Lambda V^T\)</span></p><figure><img src="./奇异值分解.png" alt="奇异值分解" /><figcaption aria-hidden="true">奇异值分解</figcaption></figure><p>detail</p><p>假设<span class="math inline">\(A \in R^{n*m}, U\in R^{n*n}, \Lambda \in R^{n*m}, V \in R^{m*m}\)</span></p><p><span class="math inline">\(\Lambda\)</span>中的元素从大到小排列，前面k个元素绝对值大于0</p><p>对三个矩阵分块，图中的阴影部分是我们要存储的部分</p><p><span class="math display">\[\Lambda = [\Lambda_k, \mathbf{0_{m-k}}], \Lambda_k \in R^{n * k}, \Lambda_{(m-k)} \in R^{n * (m-k)}\]</span></p><p><span class="math display">\[V = [ \mathbf{v_k},  \mathbf{v_{m-k}}],  \mathbf{v_k} \in R^{m*k},  \mathbf{v_k} \in R^{m*(m-k)} \]</span></p><p><span class="math display">\[\Lambda V^T = [\Lambda_k, \mathbf{0_{m-k}}].[v_k, v_{m-k}]^T = \Lambda_k.v_k^T \]</span></p><p>经过U做encoding之后的信息仅有 <span class="math inline">\(\Lambda_k\)</span>和<span class="math inline">\(v_k\)</span></p><p>其中<span class="math inline">\(\Lambda_k\)</span>可以存为稀疏矩阵，含k个非零元素</p><p><span class="math inline">\(v_k \in R^{m*k}\)</span>占用空间m*k</p><p>U做类似的分解占用空间n*k</p><p>一起需要存储的元素个数为<span class="math inline">\(k + m*k + n*k\)</span></p><p>不做encoding的话，需要存储m * n 个元素</p><p>总结一下:用了压缩算法之后，空间复杂度从O(n*m)降低到O(m)或O(n)</p><p>当m,n很大，又很容冗余信息（k很小），svd分解能大量降低矩阵存储空间</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;大矩阵怎么存？最简单的方式就是矩阵分块, 然后分chunk存/读/算，下一个chunk覆盖上一个chunk，始终占用一个chunk的内存。&lt;/p&gt;
&lt;p&gt;如果还想去噪，可以用svd，下面推演一下用svd可以节省多少空间&lt;/p&gt;
&lt;/blockquo</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="SVD" scheme="https://chiechie.github.io/tags/SVD/"/>
    
    <category term="信息论" scheme="https://chiechie.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"/>
    
    <category term="效率" scheme="https://chiechie.github.io/tags/%E6%95%88%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>深度学习10 非监督学习</title>
    <link href="https://chiechie.github.io/2021/07/09/deeplearning/dl10_lvm/"/>
    <id>https://chiechie.github.io/2021/07/09/deeplearning/dl10_lvm/</id>
    <published>2021-07-09T10:20:08.000Z</published>
    <updated>2021-07-12T00:27:44.908Z</updated>
    
    <content type="html"><![CDATA[<h2 id="非监督学习">非监督学习</h2><ol type="1"><li><p>非监督学习是指，从原始数据中学习rich模式，以一种label free的方式。</p></li><li><p>非监督学习又分为两类：生成模型和自监督学习。</p></li><li><p>生成模型指的是, 学习原始数据的分布，例如基于高斯分布的异常检测</p></li><li><p>自监督学习指的是, 学到关于原始输入的一些reprentation，可以做到语意理解，可能在未来的有一些任务中会有用，有点像做题训练（puzzle）。例如， Bert训练需要解决两个puzzle：遮挡词预测；下一个句子预测。再例如，对图像翻转90度/180度，喂给neural network，让nn来预测对应的操作（90/180），这个似乎在监督学习中用的很多</p></li><li><p>非监督学习有哪些应用？</p><ol type="1"><li><p>生成正常数据，比如生成回测中的合成数据</p></li><li><p>条件的合成技术,包括WaveNet和GAN-pix2pix，比如talktotranfomer.com</p></li><li><p>数据压缩</p></li><li><p>【最重要】在监督学习之前做自监督学习可以提升下游任务的准确率。</p><blockquote><p>这个技术在工业界已经有应用了：谷歌搜索是由bert支持的</p></blockquote></li></ol></li><li><p>What is true now,but not true ayear before?</p><blockquote><p>以前自监督的预训练比监督模型在检测和识别中效果更差，现在效果赶超有监督了</p></blockquote></li><li><p>目前已经成熟的技术：语言生成模型（GPT3），图像生成模型（VIT），预训练的语言模型（Bert），预训练的图像模型。</p></li><li><p>目前还没有研究成熟的方向：自回归密度模型（AgressiveRgression Density Modelings），Flows，VAE，UnsupervisedLearning for RL。</p></li></ol><h2 id="生成模型0-summary">生成模型0-summary</h2><ol type="1"><li><p>简单的生成模型比如直方图/高斯分布。复杂的生成模型比如自回归模型（Autoregressive Models）</p></li><li><p>自回归模型的技术关键词有参数化分布（Parameterized distributions） 和最大似然（maximum likelihood）。</p></li><li><p>常见的两个自回归模型有循环神经网络（Recurrent Neural Nets）和遮挡模型（Masking-based Models）</p></li><li><p>基于likelihood的模型要解决的任务是什么？</p></li><li><p>生成数据：合成的图像，合成的语音，合成的文本，合成的视频</p></li><li><p>数据压缩：对原始文件高效编码，原始数据的entropy越大，压缩比例就越低，怎么知道当前数据的entropy有多大从而使用合适的压缩比例呢？需要去学习一个模型，这个模型可以输出当前数据的信息含量。</p></li><li><p>异常检测：比如在自动驾驶的场景中，如果遇到了一个训练样本中未见过的样本，就不一样让机器强行决策，正确的方法是，使用异常检测--将该场景先和训练样本先对比一下--如果该场景是异常的，就让人来接手。</p></li></ol><pre><code>&gt; Good!!!可以用在回测中</code></pre><ol start="5" type="1"><li><p>基于likelihood的模型怎么实现？从样本数据中估计出样本分布的参数。</p></li><li><p>The field of deep generative models is concerned with jointly designing these ingredients to train <em>flexible and powerful</em> models <span class="math display">\[p_\theta\]</span> capable of approximating distributions over high-dimensional data <span class="math display">\[\mathbf{x}\]</span>.</p></li><li><p>怎么设计分布函数？effectively represent complex joint distributions over x,并且yet remain easy to train</p></li><li><p>设计好分布函数族后（一般假设是高斯分布），使用相应的训练方法hand-in-hand去估计参数，损失函数=真实分布和拟合分布的距离：</p></li><li><p>MLE的损失函数为样本的对数概率的相反数，记为：</p></li></ol><p>​ <span class="math display">\[\arg \min _{\theta} \operatorname{loss}\left(\theta, \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\right)=\frac{1}{n} \sum_{i=1}^{n}-\log p_{\theta}\left(\mathbf{x}^{(i)}\right)\]</span></p><p>等价于 <span class="math display">\[  \arg \min _{\theta} \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}}\left[-\log p_{\theta}(\mathbf{x})\right]  \]</span> 可以用神经网络对<span class="math inline">\(p_{\theta}\)</span>建模，并使用SGD求解.</p><ol start="13" type="1"><li>如何设计一个神经网络来代替<span class="math inline">\(p_{\theta}\)</span>,主要想法是这样的：先把生成数据的过程中，变量间的关系表达为一个贝叶斯网络（ <strong>Bayes net</strong>），并且使用神经网络来对条件概率分布建模（边）。</li><li>于是将最小化logO**转换为，学习一个可以输出条件概率的nn，输入条件变量（一个变量就是一个节点）的值输，输出另外一个变量的概率分布。就是构建一个nn去拟合边的属性---即在条件变量取不同的值时，相应的变量的概率分布是如何变化的。</li></ol><figure><img src="./K1rwN9LAHJs509x8IvCcix_7CkI6u8c5YsPUT0R1eoxuP2WvACnAQTVQQVAuqoWMtO5JIflxLnrNl_eFPnF1F3XBT658a6wyLfOSaynjAjyyWf5nFBwSww-TDvy7oYpwNCD3bRHo3Rc.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ol start="11" type="1"><li><p>跟最大似然等价的问题为最小KL--经验分布和拟合分布的距离</p><p>经验分布假设样本来自iid，则每个观测值的分布都为1（就是直方图）</p><p><span class="math display">\[\hat{p}_{\text {data }}(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \mathbf{1}\left[\mathbf{x}=\mathbf{x}^{(i)}\right]\]</span></p><p>那么经验分布和猜想分布<span class="math inline">\(p_{\theta}\)</span>之间的kl divergence为：</p><p><span class="math display">\[\mathrm{KL}\left(\hat{p}_{\mathrm{data}} \| p_{\theta}\right)=\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\mathrm{data}}}\left[-\log p_{\theta}(\mathbf{x})\right]-H\left(\hat{p}_{\mathrm{data}}\right)\]</span></p></li></ol><h2 id="生成模型1--自回归模型">生成模型1- 自回归模型</h2><ol type="1"><li>给定贝叶斯网络，将条件概率分布设置为nn, 会得到一个可处理的对数likelihood和梯度，很好训练,</li></ol><p><span class="math display">\[\log p_\theta(\mathbf{x}) = \sum_{i} \log p_\theta(x_i \,|\, \mathrm{parents}(x_i))\]</span></p><ol start="2" type="1"><li><p>但是用nn建模出来的条件概率的表达能力够强吗？任何联合概率分布都可以表达为条件概率的乘积。</p></li><li><p>自回归模型(<strong>autoregressive model</strong>): <span class="math display">\[\log p(\mathbf{x})=\sum_{i=1}^{d} \log p\left(x_{i} \mid \mathbf{x}_{1: i-1}\right)\]</span> 一个使用nn预测条件概率的表达充分的贝叶斯网络，可以对p(x)构建一个表达充分的模型，同时也很好训练</p></li><li><p>一个简单的自回归模型：</p><p>两个随机变量: x1, x2</p><p>对联合分布建模：Model: p(x1, x2) = p(x1) p(x2|x1)</p><p>边际概率分布分布p(x1)是一个直方图</p><p>条件概率分布p(x2|x1)是一个mlp，输入是<span class="math inline">\(x_1\)</span>, 输出是<span class="math inline">\(x_2\)</span>的logits</p></li><li><p>如何让不同的边--即不同的标间概率之间共享信息？RNN和masking方法</p></li><li><p>[Karpathy, 2015]提出char-rnn方法</p><figure><img src="./image-20210710152800599.png" alt="image-20210710152800599" /><figcaption aria-hidden="true">image-20210710152800599</figcaption></figure></li></ol><h2 id="生成模型2-隐变量模型">生成模型2-隐变量模型</h2><ol type="1"><li><p>为什么需要latent variable模型?对比下自回归模型：采样的计算量太大了，采样只能按照顺序进行。</p></li><li><p>latent varible只依赖latent varible，采样效率更高，相对于ar模型。</p></li><li><p>如果知道生成数据的因果过程，就可以设计一个latent varible了。</p></li><li><p>一般来说，我们不知道隐变量是什么，以及这些latent variables是怎么跟observation进行互动的。 确定隐变量的最好的方法，仍未有定论。</p></li><li><p>latent variable模型更像是一种更抽象的层次更高的认知。</p></li><li><p>隐变量模型的一个例子：如下，Z是一个K维度的随机向量，X是一个L维的随机变量</p><p>对Z抽样得到一个K维的0/1向量z，通过一个函数（DNN）映射为一个参数<span class="math inline">\(\theta\)</span>, X的分布变为一个参数为<span class="math inline">\(\theta\)</span>的bernoulli分布，从该分布中采样得到一个L维的0/1向量x。举例子，z代表股市是牛市/熊市，X代表股票上涨/下跌，牛市时X的分布的均值要高于熊市时X的分布均值</p><figure><img src="./image-20210709233421079.png" alt="image-20210709233421079" /><figcaption aria-hidden="true">image-20210709233421079</figcaption></figure></li><li><p>如何训练上面的DNN？最大似然估计，再往前可以追朔到亚里士多德的三段论，大前提，小前提==&gt; 结论。</p><p>大前提：我们观测到的一系列<span class="math inline">\(x^{(i)}\)</span>, 是客观存在的，不是伪造的</p><p>小前提：有一个隐变量X，服从某个bernouli分布，有一个可观测的随机变量X服从某个bernouli分布，并且该bernouli分布的参数跟X有关</p><p>结论: 一系列<span class="math inline">\(x^{(i)}\)</span>的概率很大</p><p>为了使得三段论成立，我们希望找到一个最优的DNN，使得<span class="math inline">\(x^{(i)}\)</span>的likelihood尽可能大。</p><p>其实，即使likelihood很大，也不能一定保证小前提成立，这里只是折中罢了。</p><figure><img src="./image-20210709234327994.png" alt="image-20210709234327994" /><figcaption aria-hidden="true">image-20210709234327994</figcaption></figure></li><li><p>求最大似然的时候，z的维度太多了怎么办？采样。</p></li><li><p>如果z的分布p很难采样怎么办？分布变换：将分布p转换为一个简单的分布q（q中也有待估计的参数，经常把q设计成一个高斯分布），然后在q中采样，这就是变分法,。「变分」强行理解成「改变分布」。</p><figure><img src="./image-20210710154905312.png" alt="image-20210710154905312" /><figcaption aria-hidden="true">image-20210710154905312</figcaption></figure></li><li><p>求解Inference问题,相当于积分掉无关变量，求边际分布。如果变量维度过高，积分非常困难。一般来说，有两种方法，一种是蒙特卡洛模拟，一种是变分推断。前者是unbiased &amp;&amp;high variance，后者是biased-low&amp;&amp; variance。举个例子，求P(z|x)时，使用贝叶斯公式，可以转换为联合分布除以P(x), 求P(x)涉及到对z求多重积分，特别困难。</p><blockquote><p>还有一些技术如（mean field）可将复杂的多元积分变成简单的多个一元积分的乘积，从而使得求多重积分变得可行。</p></blockquote><p>下面看一下有偏差的低方差的方法---变分推断。。</p></li></ol><h3 id="变分推断">变分推断</h3><blockquote><p>Any procedure which uses optimization to approximate a density can be termed ``variational inference''.---Jordan (2008) 对 Variational Inference 的定义</p></blockquote><ol type="1"><li>简单来说，变分就是用简单的分布q去近似复杂的分布p。</li></ol><figure><img src="./image-20210710155220160.png" alt="image-20210710155220160" /><figcaption aria-hidden="true">image-20210710155220160</figcaption></figure><ol start="2" type="1"><li>如何找到满意的<span class="math inline">\(q_{\phi}\)</span>？对于每个样本<span class="math inline">\(x^{(i)}\)</span>, q测量的z的分布都跟p测量的z的后验分布很接近。也就是说，希望找到一个q使得，n个KL之和最小。</li></ol><figure><img src="./image-20210710171726674.png" alt="image-20210710171726674" /><figcaption aria-hidden="true">image-20210710171726674</figcaption></figure><ol start="3" type="1"><li><p>由于KL(q, p) + 变分下界(VLB) =logP(x) , 不管q如何变化，KL+ELBP是固定的，所以最小化KL等价于最大化$ ELBO<span class="math inline">\(。KL因为含有后验分布\)</span>p_{z|x}$不好优化所以转为优化ELBO。</p><blockquote><p>为什么要叫lower bound，因为KL&gt;=0, 所以VLB&lt;=logP(x), 所以VLB是logP(x)的一个下界，所以叫Lower bound</p></blockquote></li><li><p>至此，vae的损失函数(ELBO)等价于两部分=重构误差+正则项，</p></li></ol><p><span class="math display">\[E L B O=\mathbb{E}_q\left[\log p\left(x \mid z\right)\right]-\mathbb{K} \mathbb{L}\left(q\left(z \mid x\right) \| p(z)\right)\]</span></p><p><img src="https://miro.medium.com/max/1400/1*Q5dogodt3wzKKktE0v3dMQ@2x.png" alt="img" /> 5. 其中正则项(regularity)是为了保证 latent space 更regular，即，希望生成模型实现连续性（<strong>continuity</strong>）和完备性（completness，不能胡说八道）。如果没有正则项，就跟auto-encoder一样专注于最小化重构误差，很有可能过拟合，没有泛化性。 <img src="https://miro.medium.com/max/2000/1*83S0T8IEJyudR_I5rI9now@2x.png" alt="img" /></p><ol start="6" type="1"><li><p>auto-encoder和variational autoencoders的区别：注意decoder，vae的decoder将z映射为x，是确定性的，不是随机的。随机性之存在于对encoder的结果（u, sigma）中采样出z。</p><figure><img src="https://miro.medium.com/max/2000/1*ejNnusxYrn1NRDZf4Kg2lw@2x.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure></li><li><p>训练好了VAE后，单独拿decoder出来做生成模型。</p><blockquote><p>KL衡量的是p相对于q的距离，是一种非对称的local 距离。</p></blockquote></li><li><p>将pathwise derivative应用到变分推断，得到VAE</p><figure><img src="./image-20210710180433539.png" alt="image-20210710180433539" /><figcaption aria-hidden="true">image-20210710180433539</figcaption></figure><figure><img src="./I0yVIbKz1a-74JbNr5P31z5ePUf1g7NBzEtvk5K3chlmwzySQPyvzqx1umTDG_1ynr1IiYA9t1cwI38vSvmLda_EeQA8Q5gbjZ9J_Ej1NCwkIDSnMo8HJAhoVBA5Mjliy4V_185bk5Y.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure></li></ol><h2 id="参考">参考</h2><ol type="1"><li>https://sites.google.com/view/berkeley-cs294-158-sp20/home</li><li><a href="https://www.youtube.com/watch?v=V9Roouqfu-M">L1 Introduction--CS294-158-SP20-youtube</a></li><li><a href="https://drive.google.com/file/d/1zWvkB5BNFs1IzyXarsf6ItXpfEc2OfZc/view">L1 Introduction--CS294-158-SP20-slide</a></li><li>https://www.youtube.com/watch?v=V9Roouqfu-M</li><li><a href="https://www.youtube.com/watch?v=iyEOk8KCRUw">L2 Autoregressive Models -- CS294-158-SP20-youtube</a></li><li>https://www.zhihu.com/question/41765860</li><li>https://www.youtube.com/watch?v=uaaqyVS9-rM</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;非监督学习&quot;&gt;非监督学习&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;非监督学习是指，从原始数据中学习rich模式，以一种label free的方式。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;非监督学习又分为两类：生成模型和自监督学习。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="深度学习" scheme="https://chiechie.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="深度学习" scheme="https://chiechie.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="backtesting" scheme="https://chiechie.github.io/tags/backtesting/"/>
    
  </entry>
  
  <entry>
    <title>数据结构1 线性数据结构</title>
    <link href="https://chiechie.github.io/2021/07/09/data_structure/ds1-linear-data-structure/"/>
    <id>https://chiechie.github.io/2021/07/09/data_structure/ds1-linear-data-structure/</id>
    <published>2021-07-09T04:13:11.000Z</published>
    <updated>2021-07-12T02:50:05.213Z</updated>
    
    <content type="html"><![CDATA[<h1 id="总结">总结</h1><ol type="1"><li>数组（array），链表（linked list），栈（stack），队列（queue）是最常用的线性的数据结构。</li><li>数组是一片连续的存储空间中，链表在物理空间中不是连续地存在一起的。</li></ol><h1 id="通用的数据结构">通用的数据结构</h1><h2 id="数组array">数组(array)</h2><ol type="1"><li><p>数组是一个很基本的数据结构，使用数组和pointer可以构建出所有的数据结构</p></li><li><p>数组分为静态数组（static array）和动态数组（dynamic array）两种.静态数组大小固定，动态数组可以扩展。</p><blockquote><p>wangshusen的视频里面分别叫array和向量vector，下面还是以动/静为标准。</p></blockquote></li><li><p>静态数组(static array)是一个固定长度的容器（container），包含了n个元素，每个元素有一个索引，索引值从[0,n-1]</p></li><li><p>动态数组（dynamic array）的大小可以增加或者减少。</p></li><li><p>indexable是什么意思？数组中的每一个槽位(slot)/索引(index)，都能关联到（be referenced with）一个数字。索引就好像给每个元素取的名字，是一个阿拉伯数字。</p></li><li><p>静态数组是存储在内存中的一片连续的区域。</p></li><li><p>什么时候使用静态数组：</p><ul><li>依次（sequentially）存储元素/读取（accessing）元素</li><li>在读入/输出流式数据的时候，一次性读入数据量太大，所以分chunk读入数据，并且用buffer去读取每个chunk的数据，buffer就是用array实现的</li><li>在lookuptable中，也会用到数组，因为可以按照index读取数据</li><li>如果某个函数只允许返回1个，但是有多个信息需要返回，可以使用array构造一个workaround，将多个信息打包成一个array，然后返回这个array的指针或者reference</li><li>动态规划：用于缓存子问题的answer，例如背包问题（knapsack problem）和 应变变换问题。</li></ul></li><li><p>静态数组和动态数组的时间复杂度</p><ul><li>读取的时间复杂度: 都是O(1)，因为数组是indexable的</li><li>查找的时间复杂度：都是O（n），有可能查找的值找不到</li><li>插入某个元素的时间复杂度：静态数组不允许插入（记住，他是一个长度固定的容器），动态数组插入的时间复杂度是O（n）- 添加的时间复杂度：</li><li>删除：</li></ul></li><li><p>获取静态数组中元素的唯一方式，就是通过index去reference（数组的成员只对外暴露他们的index编号）。</p></li><li><p>operations on 动态数组：静态数组上能做的操作，动态数组都能做，还能做的更多，比如增，删。</p></li><li><p>怎么实现一个动态数组？</p><ul><li>方法1--使用静态数组实现：<ul><li>初始化：创建一个静态数组，有一个初始容量，此后一遍增加元素一遍跟踪容器中元素的个数。</li><li>添加新元素时，如果容器没满，直接加入； 如果容器满了，自动扩容--创建一个新的静态数组，大小为当前容量的2倍，将老的静态数组的元素复制到新的静态数组中，然后加入该元素。</li></ul></li></ul></li></ol><h2 id="链表linked-list">链表(linked list)</h2><ol type="1"><li>链表上相邻的元素在物理存储上并不是相邻的，每个元素的物理未知只存在于他的上下游节点中。</li><li>链表分为单链表（singly linked lists）和双链表(doubly linked lists）。</li><li>按顺序查找链表也很快。</li><li>链表是一种数组的组织形式，将数据组织成一个有序的一系列节点（nodes）的形式，每个节点（node）代表一个数据，每个节点都指向其他代表数据的节点。</li><li>下面是一个单链表（singly linked），每个节点都包含一个数据，同时包含一个指针，指向他的邻居节点。最后一个节点的指针是空的。</li></ol><p><img src="img.png" alt="img.png" /> 6. 在哪里会用到链表？ - 需要实现某个抽象数据类型时会用到链表，比如要实现列表（lists），队列（Queue）和 栈（Stack），因为这些抽象数据类型需要频繁操作adding和remocving，而这两个操作对于链表来说是相当拿手的。 - 链表很容易对现实事物建模，如火车 - 创建循环列表时很有用，可以让链表的最后一个节点的指针指向第一个节点，训练列表对于建模重复事件循环很有用， - 实现哈希表通常使用链表处理冲突的问题。 7. 链表的几个关键元素： - head：指向链表的第一个节点的pointer， - tail：指向表示链表的最后一个节点的pointer， - pointer表示邻居节点的指针， - node：表示一个包含数据和指针的对象。在实现时，每个节点也可以被表示为structures，或者classes 8. 单链表和双链表的区别，单链表的每个节点只存储下一个邻居的pointer，双向链表的每个节点存储两个指针，分别是上一个邻居节点和下一个邻居节点。前者占用的内存是后者的1/2，缺点是不知道当前节点的前一个节点是什么，要找的话，只能从head开始遍历。如果要删除某个元素，双链表的时间复杂度是常数，单链表是线性的，因为他要重新遍历以找到上家，然后修改它的指针。 <img src="img_1.png" alt="单链表vs双链表-优缺点对比" /> 9. 链表中，同时存链表的第一个节点和最后一个节点，是为了更快速的添加和删除元素。 10. 链表中的复杂度分析： - 搜索某个值在链表中的位置：单链表和双链表的复杂度是O(n) - 在链表头部/尾部插入某个元素：单链表和双链表的复杂度是O(1) - 删除头部的元素：单链表和双链表的复杂度是O(1) - 删除尾部的元素：单链表的复杂度O(n)，双链表的复杂度是O(1) - 删除中间元素：单链表的时间复杂度为复杂度O(n)，双链表的复杂度是O(n)</p><h2 id="栈stack">栈（stack）</h2><ol type="1"><li><p>栈是一种线性的数据结构，栈的一端是固定的，跟现实世界中的stack一样，stack有两个主要的操作：入栈（push）和出栈（pop）。</p></li><li><p>栈中有一个top指针指向栈的顶端。以为对栈的操作主要是集中在顶端。</p></li><li><p>数据出栈（pop）和入栈（push）符合后进先出的顺序，也叫LIFO/</p><figure><img src="/Users/shihuanzhao/research_space/chiechie.github.io/source/_posts/data_structure/image-20210711073942878.png" alt="image-20210711073942878" /><figcaption aria-hidden="true">image-20210711073942878</figcaption></figure></li><li><p>可以使用数组或者链表来实现一个栈。</p></li><li><p>什么时候用到栈：括号匹配/撤销操作/汉诺塔/图遍历中的深度优先搜索。（DFS）</p></li></ol><h2 id="队列">队列</h2><ol type="1"><li><p>队列是一个线性的数据结构，有两个主要的操作，入队（enqueue/adding/offering）和出队（dequeue/polling）。入队就是添加数据到队尾添，出队就是删除队头的数据。</p></li><li><p>数据入队和出队，符合后进后出的顺序。（LILO）</p><figure><img src="/Users/shihuanzhao/research_space/chiechie.github.io/source/_posts/data_structure/image-20210711085139955.png" alt="image-20210711085139955" /><figcaption aria-hidden="true">image-20210711085139955</figcaption></figure></li><li><p>queue可以用于对排队场景建模；跟踪最新添加的k个数据；web server请求管理--谁先来就服务谁/图遍历中的广度优先搜索（BFS）。</p></li><li><p>可以用链表来实现队列。</p></li><li><p>使用队列实现BFS。 <img src="./image-20210711091627179.png" alt="image-20210711091627179" /></p></li></ol><h1 id="python中的数据结构">python中的数据结构</h1><ol type="1"><li><p>在python中，list就是一个动态的数组，append有时候效率很低。</p></li><li><p>在python中，collections.deque是一个double-ended queue，两端固定的队列，基于双向链表实现的插入效率更高一些。但是按照序号查找某个元素，效率不高。</p></li><li><p>在python中怎么实现一个栈？</p><ol type="1"><li>使用python内置的对象list，其自带的append和pop方法可以实现push和pop</li><li>使用collections.deque，其自带的append和pop方法可以实现push和pop</li></ol></li></ol><h1 id="参考">参考</h1><ol type="1"><li><a href="https://github.com/akzare/Algorithms">algorithm-python-github</a></li><li><a href="https://www.youtube.com/watch?v=gXgEDyodOJU">youtube</a></li><li><a href="https://www.youtube.com/watch?v=V_TulH374hw">Graph-theoretic Models</a></li><li><a href="https://stackoverflow.com/questions/13965757/what-is-the-difference-between-an-abstract-data-typeadt-and-a-data-structure">the difference-between-ADT和DS</a></li><li><a href="https://www.csie.ntu.edu.tw/~htlin/course/dsa21spring/">数据结构和算法</a></li><li><a href="https://www.youtube.com/watch?v=RBSGKlAvoiM&amp;t=102s">Data Structures Easy to Advanced Course</a></li><li>https://realpython.com/how-to-implement-python-stack/#implementing-a-python-stack</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;总结&quot;&gt;总结&lt;/h1&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;数组（array），链表（linked list），栈（stack），队列（queue）是最常用的线性的数据结构。&lt;/li&gt;
&lt;li&gt;数组是一片连续的存储空间中，链表在物理空间中不是连续地存在一起的。&lt;/</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="图数据" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"/>
    
    <category term="编程" scheme="https://chiechie.github.io/tags/%E7%BC%96%E7%A8%8B/"/>
    
    <category term="数据结构" scheme="https://chiechie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据结构0 总览</title>
    <link href="https://chiechie.github.io/2021/07/08/data_structure/ds0_summary/"/>
    <id>https://chiechie.github.io/2021/07/08/data_structure/ds0_summary/</id>
    <published>2021-07-08T01:17:29.000Z</published>
    <updated>2021-07-09T05:37:50.282Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>数据结构是在计算机中存储数据的方法</p><p>一般情况下，精心挑选的数据结构能产生有效的算法</p></blockquote><h2 id="why-数据结构">why 数据结构？</h2><ol type="1"><li>数据结构是对数据的一种组织形式，以方便后续数据被高效地使用。</li><li>数据结构是创建快速高效算法的必要ingredeints，数据结构可以帮助管理和组织数据；让代码看起来更干净。</li><li>数据结构/抽象数据类/程序的关系就好像，使用乐高搭建一个建筑，层层抽象。</li></ol><h3 id="抽象数据类型和数据结构">抽象数据类型和数据结构</h3><ol start="2" type="1"><li>抽象数据类型(Abstract Data Type, ADT)是对数据结构的一个抽象，它只提供接口（interface），数据结构必须遵循该接口（interface），但是这个接口不涉及任何关于实施或者编程语言相关的细节。</li><li>抽象数据类型的数据结构的关系，举个例子，从一个地方A到另一个地方B有很多种方法，可以骑自行车或者走路或者坐火车，前者是抽象数据类型，后者是该抽象数据类型对应的具体的数据结构。</li><li>一个抽象数据类型之定义了数据结构应该实现什么功能，应该有哪些方法，至于方法的实现细节抽象数据类型是不管的。</li><li>举几个抽象数据类型和数据结构的例子<ul><li>抽象数据类型列表（list）可以使用动态数组或者链表实现，这两个数据结构都能实现add，removing，indexing元素。</li><li>抽象数据类型队列（Queue）可以使用Linked list based Queue/Array based Queue/Stack based Queue。</li><li>抽象数据类型（Map）可以使用Tree Map/Hash Map/Hash Table实现</li></ul></li><li>ADT is a logical description and data structure is concrete.</li><li>ADT is the logical picture of the data and the operations to manipulate the component elements of the data.</li><li>Data structure is the actual representation of the data during the implementation and the algorithms to manipulate the data elements.</li><li>ADT is in the logical level and data structure is in the implementation level.</li></ol><h3 id="计算复杂度分析">计算复杂度分析</h3><ol type="1"><li><p>为了分析我们的数据结构的性能：执行该算法需要花费多少时间和占用多少内存？</p></li><li><p>O(*)表示在worst case的情况下，一个算法复杂度上界是多少，当输入数据变得很大的时候，有助于评估性能, 一般来说有这么几种时间复杂度:</p><ul><li>常数时间: O(1)</li><li>对数时间： O(log(n))</li><li>线性时间： O(n)</li><li>linearithmix时间: O(nlog(n))</li><li>二次时间： O(n^2)</li><li>三次时间: O(n^3)</li><li>指数时间：O（b^n）</li><li>因子时间: O(n!)</li></ul><p>n表示输入的大小复杂度从低到高。</p></li><li><p>因为O（*）只关注当数据变得足够大的时候，算法的表现，所以算法的实际运行时间中的常数项，低阶项都可以省掉，系数都可以扔掉哦。 <span class="math inline">\(O(n + c) = O(n)\)</span> <span class="math inline">\(O(cn ) = O(n)\)</span> <span class="math inline">\(O(n^2 +2*n) = O(n^2)\)</span></p></li><li><p>二分查找是用在一个有序数组上的查找算法，时间复杂度是对数</p></li><li><p>找一个集合的所有子集-时间复杂度是指数O(2^n)</p></li><li><p>找一个字符串的所有排列O(n!)</p></li><li><p>使用合并排序算法来排序，时间复杂度是 O(nlogn)</p></li></ol><h2 id="数据结构有哪几种">数据结构有哪几种？</h2><ol type="1"><li><p>数据结构分为线性数据结构和非线性数据结构</p></li><li><p>线性数据结构包括数组（array），链表（linked list），栈（stack），队列（queue） <img src="b3728c27302a8548fe9e8a87e619ca83.png" alt="线性数据结构" /></p></li><li><p>非线性数据结构包括树和图,树可以认为是有向图的special case <img src="e6d5a8d9a75587abe612dfef9abffc01.png" alt="非线性数据结构" /></p></li><li><p>图分有向图和无向图 <img src="18c651092d22c7204021d10a5a79b0ff.png" alt="有向图vs无向图" /></p></li><li><p>无向图的一个实例是fb的社交网络，边表示好友关系。 <img src="f3fc896014d62fb1ec1c96c93210f7ff.png" alt="社交网络" /></p></li><li><p>基于社交网络这个数据结构有什么应用呢？好友推荐, 推荐朋友的朋友,网络社会科学的小世界</p><blockquote><p>小世界网络的重要性质：“流行病学”、“合作”、“知识”</p></blockquote></li><li><p>有向图的一个实例是万维网： <img src="b9b97250ce6e998045dcbb0d5b379724.png" alt="www" /></p></li><li><p>图还可以分有权图和无权图。无权图可认为是权图的special case，权重都为1。</p></li><li><p>有权图的一个实例是高速公路网,边代表距离 <img src="5b81b50b2d2b048ed3188b71af85a02f.png" alt="公路网" /></p></li><li><p>树的一个实例是家谱，树种，任意一对节点，有且只有一条通路（不存在loop嘛）</p></li><li><p>因果图是一个有向有权图。</p></li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://github.com/williamfiset/Algorithms">algorithm-github</a></li><li><a href="https://www.youtube.com/watch?v=gXgEDyodOJU">youtube</a></li><li><a href="https://www.youtube.com/watch?v=V_TulH374hw">Graph-theoretic Models</a></li><li><a href="https://stackoverflow.com/questions/13965757/what-is-the-difference-between-an-abstract-data-typeadt-and-a-data-structure">the difference-between-ADT和DS</a></li><li><a href="https://www.csie.ntu.edu.tw/~htlin/course/dsa21spring/">数据结构和算法</a></li><li><a href="https://www.youtube.com/watch?v=RBSGKlAvoiM&amp;t=102s">Data Structures Easy to Advanced Course</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;数据结构是在计算机中存储数据的方法&lt;/p&gt;
&lt;p&gt;一般情况下，精心挑选的数据结构能产生有效的算法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;why-数据结构&quot;&gt;why 数据结构？&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;数据结构是对</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="图数据" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"/>
    
    <category term="编程" scheme="https://chiechie.github.io/tags/%E7%BC%96%E7%A8%8B/"/>
    
    <category term="数据结构" scheme="https://chiechie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>《Advances in Financial Machine Learning》读书笔记3 回测</title>
    <link href="https://chiechie.github.io/2021/07/07/AFML/AFML3/"/>
    <id>https://chiechie.github.io/2021/07/07/AFML/AFML3/</id>
    <published>2021-07-07T03:08:30.000Z</published>
    <updated>2021-07-12T07:22:28.060Z</updated>
    
    <content type="html"><![CDATA[<h2 id="chapter-10-仓位大小">chapter 10 仓位大小</h2><blockquote><p>即使股价预测很准，但是仓位配置的不当，还是会亏钱。</p></blockquote><ol type="1"><li><p>假定三期股价为 [1, 0.5, 1.25]，仓位1: [0.5, 1, 0] &amp; 仓位2: [1, 0.5, 0]，则前者挣钱而后者亏钱。这种情况下，更偏好这样的下注策略：建仓时保留一些现金，如果交易信号加强（股价变为0.5），就追加投资。</p></li><li><p>下图是交易信号(z)和最佳下注大小(m)之间的函数关系：</p><p>横轴的z表示交易信号强弱，纵轴m代表最佳下注大小：<span class="math inline">\(m = x (2Z [z] − 1), m\in{[-1,1]}\)</span></p><p>交易信号z是模型预测结果x的函数，预测结果x越极端，则分母越小，分子绝对值越大， z越接近<span class="math inline">\(+\infty\)</span>或者<span class="math inline">\(-\infty\)</span>，则交易信号越强, 但是可以看到仓位超过一定范围就饱和了。</p><p>总的来说，交易信号越强，下注越大，但是，当交易信号强度超过一定值，就可以不用加仓，最佳持仓水平m可以保持不变，这一点跟直觉是稳和的，价格猛涨多了一段时间之后，再往上走就很难了，这个时候不能追高。</p><figure><img src="./image-20210711134750210.png" alt="image-20210711134750210" /><figcaption aria-hidden="true">image-20210711134750210</figcaption></figure></li><li><p>如果每次预测时，如果交易信号的微小改变就要调仓，那换手率就太高了，作者建议可以对预测结果做滑动平均，或者将下注量离散化（如下图）<img src="./image-20210711140908880.png" alt="image-20210711140908880" /></p></li><li><p>动态BET SIZES 和 限价：当股票的市场价格<span class="math inline">\(p_t\)</span>和预测价格<span class="math inline">\(f_t\)</span>波动时，可用如下方法动态地确定下注量<span class="math inline">\(\hat{q}_{i, t}\)</span>： <span class="math display">\[\hat{q}_{i, t}=\operatorname{int}\left[\frac{x}{\sqrt{w+x^{2}}} \cdot Q\right]\]</span> <span class="math inline">\(x=f_{i}-p_{t}\)</span>为预测价与当前价的差价，如果x绝对值大于0，说明当前市场出现了套利空间，绝对值越大，越要押注，</p></li></ol><p>Q为最大持仓量, <em>𝜔</em>是一个用来控制sigmoid的宽度的系数， <em>𝜔</em>越小，策略就越保守。</p><h2 id="chapter-11-回测有风险">chapter 11 回测有风险</h2><p>回测是量化中最重要但也最容易被误用的工具，本章将介绍回测时容易犯的错误。</p><ol type="1"><li>一个物理实验室可以通过控制变量来探究精确的因果关系，而回测不是实验，它并不能证明一个策略好，只能证明一个策略不好。</li><li>要做好回测极其困难，我们至少会面临这些问题:<ul><li>幸存者偏差：只用现存的股票构建投资组合回测，殊不知过去多少公司倒闭退市了；</li><li>Look-ahead bias：回测时用到的数据在那一刻还没发布；</li><li>事后诸葛亮（各种事后分析都在编故事）；</li><li>手续费：要模拟手续费很难，唯一准确的方式是上实盘；</li><li>空头：实际交易中如何找借方、空头的成本、空头的额度都需要考虑；</li></ul></li><li>即使避免了以上问题，你的回测也可能是错的——在一个数据集上回测了成百上千次才得到的漂亮结果，大概率是假的。如第八章所述，特征重要性分析帮助我们理解 ML 发现的结果，它在回测之前进行，是一种“事前“归因。相反，回测并不能帮我们理解为什么一个策略会盈利。通过回测发现的有效“因子”如同上一期彩票的中奖号码，对下一轮抽奖无益。回测前做好数据结构化、标签、加权比回测本身更重要。</li><li>重复回测带来的过拟合可以认为是一种选择偏差，要避免这种偏差可能是量化中最根本的问题。</li><li>以下步骤可以帮我们减少这种偏差:<ul><li>在多种金融资产上回测：由于金融资产的多样性，如果你发现错误 只存在于债券，那该策略很可能是错的；</li><li>用 bagging 减少过拟合；在完成本书章节1-10 的研究之前别回测；</li><li>记录得到当前结果之前回测了多少次，从而推算出过拟合的可能性；</li><li>假如回测结果没能得到有效的策略，从头开始。千万不要在回测结果的基础上继续研究。 &gt; Backtesting while researching is like drinking and driving. &gt; Do not research under the influence of a backtest.</li></ul></li><li>如果用标准CV回测来选择策略，一些回测路径会重复出现，导致极易过拟合。所以一些随机性非常有必要，例如基于probability of backtest overfitting (PBO) 的回测。</li></ol><h2 id="chapter-12-使用交叉验证做回测">chapter 12 使用交叉验证做回测</h2><p>本章将介绍三种回测方法:向前游走;交叉验证，希望能得到更准确的回测结果。</p><ol type="1"><li><p>向前游走即在回测时，使用历史数据来检验策略的样本外表现，历史数据有两种用法：</p><ol type="1"><li><p>狭义上，模拟策略的历史表现（walk-forward，WF）；</p></li><li><p>广义上，模拟策略在特定市场环境（历史上不一定发生过）中的表现。</p><p>前一种方式更广为人知，但两种方式各有利弊，都应掌握。</p></li></ol></li><li><p>向前游走的优点：</p><ol type="1"><li>有清晰的历史意义，与模拟盘的结果一致；</li><li>测试集在训练集之后，只要正确 purging 后就不存在信息泄露（见第七章）。</li></ol></li><li><p>向前游走的缺点：</p><ol type="1"><li>只有一种情景测试（即历史重演），容易过拟合；</li><li>不足以代表未来的表现，因为回测结果可能受到特定数据的影响而产生偏差；</li><li>回测时数据利用率不高（“most of the information is used by only a small portion of the decisions”）</li></ol></li><li><p>交叉验证（cross-validation）得到一个新策略时，投资者往往想知道这个策略在“非常时期”，如08年金融危机、15年股灾中表现如何。可以将我们希望测试的时期划为测试集，其他时期划为训练集，例如将 2008年作为测试集，2009至今作为训练集。训练集在测试集之后的划分从历史角度来看并不准确（not historically accurate），但通过 CV回测的目的对策略做情景测试（scenarios），从而推断策略在未来的表现。</p></li></ol><blockquote><p>For each period of the backtest, we simulate the performance of a classifier that knew everything except for that period.</p></blockquote><ol start="5" type="1"><li>交叉验证的缺点：<ol type="1"><li>只有一条回测路径（尽管不是历史路径）；</li><li>没有明确的历史意义；</li><li>由于测试集可能位于训练集之前，容易发生信息泄露。</li></ol></li><li>混合purged交叉验证 (CPCV)克服了向前游走和交叉验证的缺点。假定将全数据集分为 N 份，其中 k 份作为测试集，其余作为训练集，则共有<span class="math inline">\(C_N^k\)</span>种划分方案。所有回测路径数 （如下图所示） <img src="./img_2.png" alt="img_2.png" /></li><li>Assignment of testing groups to each of the 5 paths，按照划分依次在训练集上训练、测试集上测试，最后可以计算<span class="math inline">\(\varphi[N, k]\)</span>条路径分别的技术指标（如夏普率），从而更全面地考察模型表现。向前游走和交叉验证，混合purged交叉验证得到的结果（如夏普率）方差小，从而能减少过拟合的可能。</li></ol><h2 id="chapter-13-在合成数据上做回测">chapter 13 在合成数据上做回测</h2><p>本章将探究如何合成数据并进行回测。</p><ol type="1"><li>使用历史数据生成一个合成数据：先从从真实数据估计得到的分布，然后从分布中采样得到合成数据。</li><li>使用合成数据去做回测的好处是，可以测试很多次，在unseen的情况下，因此可以减少得到一个过拟合策略的概率。</li><li>利用合成数据回测能减少过拟合，但合成金融数据是一个很大的课题，我们先考察一下交易规则（trading rules）。交易策略假定市场不是有效的，它们用基本面 / 技术面分析试图找到套利机会。交易策略千变万化，但交易规则大同小异：比如策略的信号强于阈值则买入，达到盈利或止损点则卖出，这里的阈值、盈利止损点就是交易规则。如果用合成数据回测来确定交易规则，则过拟合的风险将大大减小。</li><li>用离散OU过程（discrete O-U process）对资产价格建模，给定资产i当前价格和未来预测价格，其在 t 次交易后的盈亏<span class="math inline">\(\pi_{i,t}\)</span>服从正态分布。可以此为依据模拟价格走势进行实验，得到最佳交易规则，而无需用真实历史数据回测。</li><li>作者在原文中给出了详细步骤以及图文分析，这里试举一例：下图是采取不同盈利、止损点进行模拟数据回测得到的夏普率热力图。对于中性市场（上图），最好的策略是设定较宽的止损空间和较窄的盈利点；对于走高市场（下图），最好的策略是采取较宽的盈利点，而止损点的设定可以比较宽泛。</li><li>总而言之，通过探究引导价格波动的随机过程，而不是在真实历史数据上回测，我们得到的最佳交易规则不会在特定数据上过拟合。尽管这样得到的交易规则可能不是最优的，但也远好于过拟合的结果。本章仅以 OU 过程为例，也可以尝试其它建模方式。</li></ol><h2 id="chapter-14-回测统计">chapter 14 回测统计</h2><ol type="1"><li><p>回测的三种范式：</p><ul><li>历史模拟，向前游走</li><li>情景模拟：交叉验证</li><li>在模拟数据上做simulation</li></ul></li><li><p>回测统计量（Backtest statistics）应该帮助揭露策略的弊端（如可能的风险）、帮助投资者比较不同策略。</p></li><li><p>一般统计量（general characteristics）能告诉我们回测的大致特性：时间范围（回测起讫时间）、资产规模（Average AUM） ，策略的资金容量（Capacity）、杠杆率（Leverage）、平均持仓时间（Average holding period）、换手率（Annualized turnover），Maximum dollar position size/Ratio of longs/Frequency of bets</p></li><li><p>衡量策略表现（performance）的统计量包括：盈亏（PnL, Profit and Loss）、多头盈亏（PnL from long positions）、年化回报率（Annualized rate of return）、命中率（hit ratio）、命中回报率（Average return from hits）、失误回报率（Average return from misses）……</p></li><li><p>策略的回报率往往在一段时间内连续为正 / 负，称之为“周期”（Runs）。周期的存在增加了策略回撤的风险，所以需要一些统计量来衡量，包括：Returns Concentration（衡量回报的集中程度）、drawdown（回撤）、time under water。</p></li><li><p>某些策略错误地估计交易费用导致失败，这些需要考虑的统计量包括：Broker fees per turnover、Average slippage per turnover……</p></li><li><p>回测通常看以下指标：</p><ol type="1"><li>绝对收益：净损失/收益（Net Profit/Loss），投资收益率</li><li>剔除通货膨胀的收益：风险调整之后的收益率（risk-adjusted return），市场敞口，波动性</li></ol></li><li><p>一些考虑到风险的统计量包括：夏普率（Sharpe Ratio，SR）、概率夏普比率（Probabilistic SR， PSR）、DSR（Deflated SR）、信息率（Information ratio）……</p><blockquote><p>夏普率衡量的是一项投资在对其调整风险后，相对于无风险资产的表现。</p><p>夏普率 = (投资收益 - 无风险收益)/投资标准差(or波动率）。</p><p>夏普率代表投资者额外承受的每一单位风险所获得的额外收益。</p></blockquote><figure><img src="./image-20210712151844519.png" alt="image-20210712151844519" /><figcaption aria-hidden="true">image-20210712151844519</figcaption></figure><ul><li>Usually, any <strong>Sharpe ratio</strong> greater than 1.0 is considered acceptable to good by investors.</li><li>A <strong>ratio</strong> higher than 2.0 is rated as very good.</li><li>A <strong>ratio</strong> of 3.0 or higher is considered excellent.</li><li>A <strong>ratio</strong> under 1.0 is considered sub-optimal. &gt; Every backtest result must be reported in conjunction with all the trials involved in its production. Absent that information, it is impossible to assess the backtest’s “false discovery“ probability. —— MARCOS’ THIRD LAW OF BACKTESTING</li></ul></li><li><p>基金经理往往希望对模型的收益进行归因（performance attribution），可以参考多因子模型（Barra’s multi-factor method）。</p></li></ol><h2 id="chapter-15-了解策略风险">chapter 15 了解策略风险</h2><p>本章的目的是帮助你检验策略风险。</p><ol type="1"><li>几乎所有策略都有盈利点和止损点，所以我们可以对策略的收益建模，检验策略对一些参数的敏感程度。</li><li>假定一个策略每年进行 n次 IID 决策，每一次有p的概率盈利<span class="math inline">\(\pi\)</span>，1-p的概率盈利<span class="math inline">\(-\pi\)</span>，则该策略的年化夏普比率 。（盈亏额不对称的情形可做类似讨论）</li><li>如果交易频率不高（ 不大），则需要较大的 才能达到高夏普比率。但即使 略大于 0.5，只要 足够大，夏普比率也可以很大，这是高频交易的思路。另一方面， 很大时， 的小幅波动会带来 的较大改变，很可能 降低 1% 就会抹去所有盈利——我们称之为策略风险（Strategy risk）。</li><li>策略风险不同于资产组合风险（portfolio risk）：假定 为依照上面公式计算得到的盈亏平衡点，策略风险指 ；而资产组合风险是市场中存在的风险，由首席风险官监控。策略风险过大时，即使投资标的的风险不大，这个策略也有较大概率无法超过业绩标准。所以策略研发者需要想办法减小 ，比如调节 。</li></ol><h2 id="chapter-16-基于机器学习资产配置">chapter 16 基于机器学习资产配置</h2><p>本章提出了基于图模型的层次风险平价方法（Hierarchical Risk Parity / HRP），其在统计性能上优于传统的资产组合优化方法。</p><p>资产组合可谓金融中最历久弥新的问题了。60多年前，马科维茨提出了Critical Line Algorithm（CLA）用于不等式约束下的二次规划问题，尤其是资产组合优化问题。CLA 的缺点是鲁棒性不高，因为二次规划需要对资产间协方差矩阵取逆，当该矩阵条件数很大时会带来较大误差。</p><p>资产组合中相关资产（多重共线性）越多，资产间协方差矩阵的条件数越大，结果越不稳定，这便是 Markowitz’s curse。此外资产越多，用于估计协方差矩阵的数据也越多，这些历史久远的数据也会造成误差。</p><p>作者认为，二次规划试图构建全连接图（fully connected graph），其中每个节点可能替代其他节点。当有50支资产时，一个全连接图有1225条边，这种复杂结构造成二次规划结果的不稳定。很自然地，我们会希望减少不必要的边.</p><p>当我们决定投资摩根大通时，下一步我们更可能考虑增/减持高盛的股票而不是一家地产公司的股票，因为摩根和高盛同属金融企业。依照这种思路，我们可以利用 ML 中的聚类算法，根据资产的特性 / 资产间相关性将所有资产建构成树状图，如下图所示。这被称为层次风险平价方法（HRP）</p><p>实验中 HRP 构建的投资组合比 CLA 更分散，风险也更低，同时 HRP 的样本外夏普比率更高。考虑到 HRP 不需要计算逆矩阵，其适应性和鲁棒性也更强。</p><p>HRP 不仅可以用于在不同资产上配置资金，还可用于在不同策略上配置资金。</p><h2 id="参考">参考</h2><ol type="1"><li>《Advances in Financial Machine Learning》</li><li>https://blog.csdn.net/weixin_38753422/article/details/100179559</li><li>https://zhuanlan.zhihu.com/p/29208399</li><li>https://zhuanlan.zhihu.com/p/109934805</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;chapter-10-仓位大小&quot;&gt;chapter 10 仓位大小&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;即使股价预测很准，但是仓位配置的不当，还是会亏钱。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;假定三期股价为 [1, 0</summary>
      
    
    
    
    <category term="AFML" scheme="https://chiechie.github.io/categories/AFML/"/>
    
    
    <category term="量化" scheme="https://chiechie.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
    <category term="投资" scheme="https://chiechie.github.io/tags/%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>《Advances in Financial Machine Learning》读书笔记1 数据分析</title>
    <link href="https://chiechie.github.io/2021/07/06/AFML/AFML1/"/>
    <id>https://chiechie.github.io/2021/07/06/AFML/AFML1/</id>
    <published>2021-07-06T11:50:44.000Z</published>
    <updated>2021-07-12T00:57:59.690Z</updated>
    
    <content type="html"><![CDATA[<h2 id="chapter-2-金融数据结构">chapter 2 金融数据结构</h2><ol type="1"><li><p>金融数据经常分为4类，基本面数据，市场交易数据，分析数据，另类数据（Alternative） <img src="image.png" alt="img.png" /></p></li><li><p>基本面数据包括公司每个季度发布的会计报告，要注意发布时间和统计时间段的区别。</p></li><li><p>另类数据（Alternative）包括个人数据，商业过程数据，卫星，天气数据。</p></li><li><p>数据处理，为了让ml方法可用，需要将原始数据处理为表数据（bars）。两种处理方法，标准bar methods和信息驱动的方法。第二种在实践中用的多。</p></li><li><p>标准bars方法：将不等间隔处理成等间隔数据，很多数据厂商都是提供这种格式。标准bars方法包括Time Bars，Tick Bars，Volume Bars，Dollar Bars</p><blockquote><p>一个tick表示一次成交事件</p></blockquote></li><li><p>【Tick Bars】每隔多少笔交易采样一次，属于一种基于信息的采样方法，其理论依据是：固定时间内的价格变化服从方差无限大的Paretian分布；固定交易笔数内的价格变化服从高斯分布。</p><blockquote><p>Price changes over a fixed number of transactions may have a Gaussian distribution. Price changes over a fixed time period may follow a stable Paretian distribution, whose variance is infinite. Since the number of transactions in any time period is random, the above statements are not necessarily in disagreement --Mandelbrot and Taylor</p></blockquote></li><li><p>上面的假设很重要，因为很多统计方法的依赖于假设--样本来自IID高斯过程。</p></li><li><p>【Tick Bars】构造tick bars要留意异常点，很多交易所在看盘前和收盘后都有竞价（auction），这段时间，order book 积累 bids 和 offers单，并不撮合（match）。当竞价结束，有一笔数量很大的交易会公开，这一笔交易可等价于成千上万个ticks，虽然现实的是一个tick。</p></li><li><p>【Volume Bars】tick bars的缺陷在于，真实情况下，我们下的一笔单子会被拆分成多笔交易去成交。因此看到的tick比我们实际下的tick变多了。Volume bars可以解决这个问题，他是按照一定证券价值变动的时间段，进行抽样。举个例子，we could sample prices every time a futures contract exchanges 1,000 units, regardless of the number of ticks involved.</p></li><li><p>【Dollar Bars】每隔一段时间，市场上交易价值达到某个给定值（bar size），就进行抽样，the bar size could be adjusted dynamically as a function of the free-floating market capitalization of a company (in the case of stocks), or the outstanding amount of issued debt (in the case of fixed-income securities)</p></li><li><p>tick bars， volumn bars， dollar bars 三者对比： If you compute tick bars and volume bars on E-mini S&amp;P 500 futures for a given bar size, the number of bars per day will vary wildly over the years. That range and speed of variation will be reduced once you compute the number of dollar bars per day over the years, for a constant bar size. 结论是前面两者每天的变动范围和变动速度，要远高于dollar bars <img src="./img_1.png" alt="img_1.png" /></p></li><li><p>信息驱动的bars，目的在于，当有信息到达时，采样更频繁。信息驱动bars有几种方法：Tick Imbalance Bars，Volume/Dollar Imbalance Bars，Tick Runs Bars，Volume/Dollar Runs Bars</p></li><li><p>【Tick Imbalance Bars】背后的想法是只要tick数据超过我们的期望，就去采样。这样设置index，累计的交易信号超过某个阈值，没看懂</p></li><li><p>【Volume/Dollar Imbalance Bars】？</p></li><li><p>【Tick Runs Bars】？</p></li><li><p>【Volume/Dollar Runs Bars】？</p></li><li><p>处理多产品序列：The ETF Trick、PCA Weights，Single Future Roll <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pcaWeights</span>(<span class="params">cov,riskDist=<span class="literal">None</span>,riskTarget=<span class="number">1.</span></span>):</span></span><br><span class="line">    <span class="comment"># Following the riskAlloc distribution, match riskTarget</span></span><br><span class="line">    eVal,eVec = np.linalg.eigh(cov) <span class="comment"># must be Hermitian              </span></span><br><span class="line">    indices = eVal.argsort()[::-<span class="number">1</span>] <span class="comment"># arguments for sorting eVal desc</span></span><br><span class="line">    eVal,eVec=eVal[indices],eVec[:,indices]</span><br><span class="line">    <span class="keyword">if</span> riskDist <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        riskDist=np.zeros(cov.shape[<span class="number">0</span>])</span><br><span class="line">        riskDist[-<span class="number">1</span>]=<span class="number">1.</span> </span><br><span class="line">    loads=riskTarget*(riskDist/eVal)**<span class="number">.5</span> </span><br><span class="line">    wghts=np.dot(eVec,np.reshape(loads,(-<span class="number">1</span>,<span class="number">1</span>))) </span><br><span class="line">    <span class="comment">#ctr= (loads/riskTarget)**2*eVal # verify riskDist </span></span><br><span class="line"><span class="comment"># return wghts</span></span><br></pre></td></tr></table></figure></p></li><li><p>直接让ml预测涨跌很难， after certain catalytic conditions算法会更容易表现好。</p></li><li><p>对特征进行采样的方法-Event-Based Sampling，其中一种方法叫The CUSUM Filter，利用CUSUM可以构造交易策略（Fama and Blume [1966]的filter trading strategy），同事也可以用来采样：当累计收益<span class="math inline">\(S_t\)</span>超过某个阈值时，进行采样，并将<span class="math inline">\(S\)</span>置为0，</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTEvents</span>(<span class="params">gRaw,h</span>):</span> </span><br><span class="line">    <span class="comment"># gRaw： raw time series</span></span><br><span class="line">    <span class="comment"># h: thresh</span></span><br><span class="line">    tEvents,sPos,sNeg=[],<span class="number">0</span>,<span class="number">0</span> </span><br><span class="line">    diff=gRaw.diff()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> diff.index[<span class="number">1</span>:]:</span><br><span class="line">        sPos,sNeg=<span class="built_in">max</span>(<span class="number">0</span>,sPos+diff.loc[i]),<span class="built_in">min</span>(<span class="number">0</span>,sNeg+diff.loc[i]) </span><br><span class="line">        <span class="keyword">if</span> sNeg&lt;-h:</span><br><span class="line">            sNeg=<span class="number">0</span></span><br><span class="line">            tEvents.append(i) </span><br><span class="line">        <span class="keyword">elif</span> sPos&gt;h:</span><br><span class="line">            sPos=<span class="number">0</span></span><br><span class="line">            Events.append(i) </span><br><span class="line">    <span class="keyword">return</span> pd.DatetimeIndex(tEvents)</span><br></pre></td></tr></table></figure><ol start="19" type="1"><li><span class="math inline">\(S_t\)</span>可以是structural break statistics, entropy, or market microstructure measurement。比如，我们可以定义一个时间，之哟啊r SADF远离之前的取值足够远。</li><li>使用event-based的方法获得了一个子集之后，可以让ml算法来分析，这些特殊事件有没有蕴含一些有值得决策的信息。</li></ol><h2 id="chapter-3-标记">chapter 3 标记</h2><p>在监督学习中，需要输入label，那么在金融领域，如何定义label？ 固定时间范围方法不够准确（可用动态阈值来改进），同时未考虑价格变化的路径，更好的方法是三边界法；此外，元标签能结合各种先验知识，是基金公司做模型、裁员工必备工具。</p><h3 id="固定时间段方法">固定时间段方法</h3><ol type="1"><li>大部分论文都是采用的这个方法，即固定的一段时间收益率是否超过/低于某个取值。 <img src="./img_2.png" alt="img_2.png" /></li><li>虽然大部分人这么用，但是这个方法跟固定时间段采样有一样的毛病，就是固定时间段内的样本并不服从gaussian分布。第二个缺陷是，这个阈值是固定的，无视当前市场波动率的变化，可能会导致错失很多有价值的正样本。</li><li>有更优的标记方法：动态阈值（类似异常检测）和 volume /dollar bars（波动率更固定），</li><li>即使改进了fixed time 和 fixed thresh，还有一个很显现实的问题就是，要考虑到价格路径，如果在半路触发margin call，那么预测得再准也没有用。</li></ol><h3 id="三边界方法the-triple-barrier-method">三边界方法（THE TRIPLE-BARRIER METHOD）</h3><ol type="1"><li>简单说，固定一个窗口，价格先达到上沿就标记1，先达到下沿就标记-1，到窗口结束都被碰到就标记0。</li><li>具体说，首先设置2个水平障碍和1个垂直障碍。2个水平障碍是基于变动的日波动率算出来的，1个垂直障碍是说，离上一次position take，经过了bars的个数。</li><li>如果upper障碍最先触发，返回1；如果lower障碍最先触发，返回-1；如果垂直的障碍触发，返回-1/+1，或者0，具体情况具体分析.三重障碍方法是路径依赖的标记方法。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">applyPtSlOnT1</span>(<span class="params">close,events,ptSl,molecule</span>):</span></span><br><span class="line">    <span class="comment"># apply stop loss/profit taking, if it takes place before t1 (end of event)</span></span><br><span class="line">    events_=events.loc[molecule] </span><br><span class="line">    out=events_[[<span class="string">&#x27;t1&#x27;</span>]].copy(deep=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> ptSl[<span class="number">0</span>]&gt;<span class="number">0</span>:</span><br><span class="line">        pt=ptSl[<span class="number">0</span>]*events_[<span class="string">&#x27;trgt&#x27;</span>] </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pt=pd.Series(index=events.index) <span class="comment"># NaNs</span></span><br><span class="line">    <span class="keyword">if</span> ptSl[<span class="number">1</span>]&gt;<span class="number">0</span>:</span><br><span class="line">        sl=-ptSl[<span class="number">1</span>]*events_[<span class="string">&#x27;trgt&#x27;</span>] </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sl=pd.Series(index=events.index) <span class="comment"># NaNs</span></span><br><span class="line">    <span class="keyword">for</span> loc,t1 <span class="keyword">in</span> events_[<span class="string">&#x27;t1&#x27;</span>].fillna(close.index[-<span class="number">1</span>]).iteritems():</span><br><span class="line">        df0=close[loc:t1] <span class="comment"># path prices df0=(df0/close[loc]-1)*events_.at[loc,&#x27;side&#x27;] # path returns out.loc[loc,&#x27;sl&#x27;]=df0[df0&lt;sl[loc]].index.min() # earliest stop loss. out.loc[loc,&#x27;pt&#x27;]=df0[df0&gt;pt[loc]].index.min() # earliest profit taking.</span></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure> 3 根据需要，三边界方法也可以有其他合理变体：下边界+右边界：我们会在一定时间后平仓，除非触发止损点提前平仓；上边界+下边界：如果没有触发盈利点和止损点，则一直持有股票；（见下图） <img src="img_4.png" alt="img_4.png" /> 这部分讨论了三边界方法的代码实现，即如何给样本打标签使得 ML 算法可以同时学习到一笔交易的方向和规模</li></ol><h3 id="同时学习方向和规模learning-side-and-size">同时学习方向和规模（LEARNING SIDE AND SIZE）</h3><ol type="1"><li>这种标记可以让ml算法从side和size中学习到一些信息</li><li>如果没有side信息，我们没法区分profit-taking 障碍 和 stop-loss 障碍。</li></ol><h3 id="meta-labeling">META-LABELING</h3><p>假设你有个模型能决定交易方向，你只需要确定交易的规模（包括不交易，即规模为0）。这是金融从业者经常需要考虑的问题，我们确定要买或者卖，唯一的问题是这笔交易值得冒多大风险；同时，我们不需要 ML 模型学习交易方向，只需要它告诉我们合适的交易规模是多少。</p><p>假如有一个基于金融理论的模型，告诉我们交易方向，那我们的标签就变成了 [公式] （ML模型只需要决定是否执行这个操作），而不是 [公式] （ML模型同时决定交易方向和规模）</p><blockquote><p>元标签的含义:</p><p>金融中用ML的另一常见错误是同时学习仓位的方向和大小（据我所知很多论文仅对买/卖方向做决策，每笔交易的金额/股数是固定的）。具体而言，方向决策（买/卖）是最基本的决策，仓位大小决策（size decision）是风险管理决策，即我们的风险承受能力有多大，以及对于方向决策有多大信心。我们没必要用一个模型处理两种决策，更好的做法是分别构建两个模型：</p><ul><li>第一个模型来做方向决策，</li><li>第二个模型来预测第一个模型预测的准确度。很多ML模型表现出高精确度（precision）和低召回率（recall），这意味着这些模型过于保守，大量交易机会被错过。F1-score 综合考虑了精确度和召回率，是更好的衡量指标，元标签（META-LABELING）有助于构建高 F1-score 模型。首先（用专家知识）构建一个高召回率的基础模型，即对交易机会宁可错杀一千，不可放过一个。随后构建一个ML模型，用于决定我们是否应该执行基础模型给出的决策。元标签+ML有以下4个优势：1. 大家批评ML是黑箱，而元标签+ML则是在白箱（基础模型）的基础上构建的，具有更好的可解释性；2. 元标签+ML减少了过拟合的可能性，即ML模型仅对交易规模决策不对交易方向决策，避免一个ML模型对全部决策进行控制；3. 元标签+ML的处理方式允许更复杂的策略架构，例如：当基础模型判断应该多头，用ML模型来决定多头规模；当基础模型判断应该空头，用另一个ML模型来决定空头规模；4. 赢小输大会得不偿失，所以单独构建ML模型对规模决策是有必要的</li></ul></blockquote><blockquote><p>label就是交易信号，表示买或者卖, 两个label 如果是基于相同时间段的收益率 计算出来的，就说是concurrent的.</p></blockquote><h3 id="量化-基本面方法">量化 + 基本面方法</h3><p>THE QUANTAMENTAL WAY</p><p>很多对冲基金——包括一些老牌基金——正在拥抱量化方法。元标签正是这些公司需要的：假设你有了一系列有预测力的特征，你既可以同时预测交易方向和规模；也可以用元标签方法。元标签方法中确定方向的基础模型可以是 ML模型、计量公式、交易规则、基本面分析，也可以是人类基于直觉的预测结果，可见元标签方法的普适性。</p><p>举个例子，元标签方法可能会发现基金经理能及时预测市场风格转换，但无法在疲倦、压力下准确预测。由于基金经理必然会受生理心理等因素影响，元标签方法能评价基金经理的预测能力。综上所述，元标签方法为基金公司的量化之路指明了方向（做模型 &amp; 评价基金经理），它应该成为基金公司的基本工具。</p><h3 id="丢掉不必要的label">丢掉不必要的label</h3><p>it is preferable to drop extremely rare labels and focus on the more common outcomes. 当标签很多且类别不均衡（imbalance）时，一些ML模型表现不好。这种情况下，最好丢掉非常罕见的标签并专注于更常见的结果。这样做有另一个原因，即用bagging方法时罕见的标签可能无法采集到，这是 sklearn 的一个bug，短期难以解决，建议读者写自己的 class，扩展 sklearn的功能。</p><h2 id="chapter-4-样本权重">chapter 4 样本权重</h2><p>训练ML 模型需要抽取样本，本章我们会考虑抽样时如何给样本加权，以更好地训练模型。</p><ol type="1"><li><p>大部分ML算法都是基于IID假设，而金融时序不是IID的，所以大部分ml应用直接套用到金融场景会失败。</p></li><li><p>很多时候数据难免出现交叉（如三标签方法一段数据结束时间不确定），当两段数据出现交叉，标签序列就不再是IID了。这种场景经常出现在非time bars中。 <img src="img_3.png" alt="img_3.png" /></p></li><li><p>对此我们有三种解决方案：一是丢弃重复数据，这会造成信息损失，不推荐；二是根据独特性加权抽样——一段数据与其他数据交叉越少，独特性越高，应该给予更多权重；三是 Sequential Bootstrap，即序列有放回抽样，每抽出一个样本，相应地减少与该样本有重叠的样本被抽取的概率，这样抽取的样本比普通 Bootstrap 更接近 IID。</p></li><li><p>此外，绝对收益率（absolute return）大的样本应该给予更多权重，原因是对 ML 算法来说，绝对收益率小的样本不好预测，作为训练样本价值不大。</p><blockquote><p>The “neutral” case is unnecessary, as it can be implied by a “−1” or “1” prediction with low confidence.</p></blockquote></li><li><p>市场是常为新的，越新的数据与当前市场相关度越高，价值越大。</p><blockquote><p>Markets are adaptive systems (Lo [2017]). As markets evolve, older examples are less relevant than the newer ones.</p></blockquote></li><li><p>最后，我们还应该考虑类别权重。金融中不均衡数据集很常见，而且这些罕见的标签往往非常重要。在 sklearn 等科学计算包中可以设置为 class_weight='balanced' 。</p></li><li><p>label表示 买/卖 信号, 两个label 如果是基于相同时间段的收益率 计算出来的，就说是 concurrent的.</p></li><li><p>对样本使用bootstrap方法抽样，以期得到iid样本。</p></li><li><p>基于uniqueness和absolute return对样本赋予权重。绝对收益高的的labels应该被给予更高的权重；收益取值越unique的也要给予更高的权重</p></li><li><p>市场是演化着的，所以我们希望给新忘本更多的权重，给老样本更少的权重。</p></li><li><p>怎么量化这个事件衰减效应？设计一个时间衰减因子（所有元素加起来为1），用这个因子乘以样本权重，</p></li><li><p>使用机器学习做分类时，有的稀有事件（比如金融危机）出现次数很少，为了保证ml算法能重视这类事件，可以调整sample_weight</p></li><li><p>具体来说，在scikit learn中，设置class_weight='balanced'，或者在bagging trees中设置class_weight='balanced_subsample'，小心<a href="https://github.com/scikit-learn/scikit-learn/issues/4324">bug</a></p></li></ol><h2 id="chapter-5-分数差分">chapter 5 分数差分</h2><p>分数差分--Fractionally Differentiated Features</p><p>如何兼顾平稳性（adf）和 记忆性（跟price的相关性）？--分数差分</p><h3 id="stationarity-vs.-memory的两难问题">STATIONARITY VS. MEMORY的两难问题</h3><ol type="1"><li>金融序列大部分非平稳，且有很低信噪比，标准的平稳变换，例如差分变换，会丢失信息。</li><li>价格序列有记忆，但是差分后的序列没有记忆了。</li><li>接下来理论家们会从剩下的残差信号中使用各种fancy的工具去提取信息。</li><li>金融序列不平稳的原因是，它有很长的记忆.所以要使用传统的方法的话要做invariant processes，例如看价格的收益率或者取对数差，波动性变化</li><li>在信号处理中，我们是不希望所有的记忆都被抹除的，因为记忆是信号模型的basis。例如，均衡平稳模型需要一些记忆，来获取截止目前为止，结果偏离长期预测值多远，来预测。矛盾在于，收益是平稳的，但是没有记忆。价格有记忆，但是不是平稳的。 那么问题就来了：最小的差分阶数是什么？既能满足一个价格序列平稳，又能保留尽可能多的信息？</li><li>协整（cointergration）方法可以使用记忆来建模。</li><li>平稳性只是ml算法的必要不充分条件，但是通过差分变换的方法虽然获得了平稳性却丢失了记忆性，会导致ml基本上没有什么记忆能力。</li></ol><p>下面会介绍一些转换方法，在保留记忆的同时，又能实现平稳变换。</p><h3 id="分数差分方法">分数差分方法</h3><ol type="1"><li>如何解决平稳和记忆两难的问题？Hosking [1981]提出了分数差分的方法。 <img src="fd.png" alt="img.png" /></li><li>使用迭代法计算权重向量 <img src="fd1.png" alt="img.png" /> <img src="fd2.png" alt="img_1.png" /></li><li>在SP500上面做实验，当差分d=0.35时，跟原始价格序列的相关性仍然很高, 0.995，d=1时候，相关性只有0.03, 基本上丢失了记忆。从adf上看，d=0.35时, 序列的黏稠度也不高，adf约等于 –2.8623， 原始的adf是–0.3387,d=1对应的adf是–46.9114。</li><li>Expanding Window 和 固定宽度窗口分数差分方法(Fixed-Width Window Fracdiff)</li></ol><h2 id="参考">参考</h2><ol type="1"><li>《Advances in Financial Machine Learning》</li><li>https://blog.csdn.net/weixin_38753422/article/details/100179559</li><li>https://zhuanlan.zhihu.com/p/69231390</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;chapter-2-金融数据结构&quot;&gt;chapter 2 金融数据结构&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;金融数据经常分为4类，基本面数据，市场交易数据，分析数据，另类数据（Alternative） &lt;img src=&quot;image.png&quot; alt</summary>
      
    
    
    
    <category term="AFML" scheme="https://chiechie.github.io/categories/AFML/"/>
    
    
    <category term="量化" scheme="https://chiechie.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
    <category term="投资" scheme="https://chiechie.github.io/tags/%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>《Advances in Financial Machine Learning》读书笔记0 为什么金融领域的机器学习项目经常失败？</title>
    <link href="https://chiechie.github.io/2021/07/05/AFML/AFML0/"/>
    <id>https://chiechie.github.io/2021/07/05/AFML/AFML0/</id>
    <published>2021-07-05T01:49:02.000Z</published>
    <updated>2021-07-08T02:55:20.196Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概览">概览</h1><ol type="1"><li><p>市场上关于投资的书籍大致分为两类：一类是理论家写的，自己都没有实践过；一类是实践家写的，他们误用了数学工具。</p></li><li><p>金融市场上现在鱼龙混扎，小散受到到不良媒体的诱导会冲动投资，造成市场动荡。量化工具可以减少这种套利机会，肃清这种风气。</p></li><li><p>常见的陷阱： <img src="img1.png" alt="img.png" /></p><figure><img src="img.png" alt="img.png" /><figcaption aria-hidden="true">img.png</figcaption></figure></li><li><p>是否意味着有了ai就没有human投资者的空间了？不是，可以人+ai</p></li></ol><h1 id="pitfall-1-西西弗斯模式">Pitfall 1: 西西弗斯模式</h1><blockquote><p>Pitfall 1: 西西弗斯模式（THE SISYPHUS PARADIGM）</p><p>Solution 1: 元策略模式（THE META-STRATEGY PARADIGM）</p></blockquote><ol type="1"><li>自由基金经理（Discretionary portfolio managers～DPM）的投资理念比较玄学，不会遵循特定的理论，这样的一群人开会时往往漫无目的、各执一词。DPMs天然地不能组成一个队伍：让50个DPM一起工作，他们的观点会相互影响，结果是老板发了50份工资，只得到一个idea。他们也不是不能成功，关键是要让他们为同一个目标工作，但尽量别交流。</li><li>很多公司采用DPM模式做量化/ML 项目：让50个PhD分别去研究策略，结果要么得到严重过拟合的结果，要么得到烂大街&amp;低夏普率的多因子模型。即使有个别PhD研究出有效的策略，这种模式的投入产出比也极低。这便是所谓让每个员工日复一日搬石头上山的西西弗斯模式。</li><li>量化是一项系统工程，包括数据、高性能计算设备、软件开发、特征研究、模拟交易系统……如果交给一个人做，无异于让一个工人造整辆车——这周他是焊接工，下周他是电工，下下周他是油漆工，尝试---&gt;失败---&gt;尝试---&gt;失败，循环往复。</li><li>好的做法是将项目清晰地分成子任务，分别设定衡量质量的标准，每个quant在保持全局观的同时专注一个子任务，项目才能得以稳步推进。这是所谓元策略模式（THE META-STRATEGY PARADIGM）。</li></ol><h1 id="pitfall-2-根据回测结果做研究">Pitfall 2: 根据回测结果做研究</h1><blockquote><p>Pitfall 2: 根据回测结果做研究（RESEARCH THROUGH BACKTESTING）</p><p>Solution 2: 特征重要性分析（FEATURE IMPORTANCE ANALYSIS）</p></blockquote><ol type="1"><li>金融研究中很普遍的错误是在特定数据上尝试ML模型，不断调参直到得到一个比较好看的回测结果——这显然是过拟合,学术期刊往往充斥面向测试集调参。</li><li>考虑一个ML任务，我们可以构建一个分类器，在交叉检验集上评估其泛化误差。假定结果很好，一个自然的问题是：哪些特征对结果的贡献最大？“好的猎人不会对猎狗捕获的猎物照单全收”，回答了这个问题，我们可以增加对提高分类器预测力的特征，减少噪声特征。</li><li>需关注ml发现的跟特征相关的模式：什么特征最重要、这些特征的重要性会随时间改变么、这种改变能否被识别和预测。</li><li>总之，特征驱动的分析比回测结果驱动的分析更重要。</li></ol><h1 id="pitfall-3-按时间采样">Pitfall 3: 按时间采样</h1><blockquote><p>Pitfall 3: 按时间采样（CHRONOLOGICAL SAMPLING）</p><p>Solution 3: 交易量钟（THE VOLUME CLOCK）</p></blockquote><ol type="1"><li>Bars：表格数据的一行或者说一个样本，叫一个bar。</li><li>Time Bars:以固定的时间区间对数据进行取样（如每分钟一次）后得到的数据。</li><li>Time bars使用广泛，但是有两个不足：第一，市场交易信息的数量在时间上的分布并不是均匀的。开盘后的一小时内交易通常会比午休前的一小时活跃许多。因此，使用Time bars 会导致交易活跃的时间区间的欠采样，以及交易冷清的时间区间的过采样。第二，根据时间采样的序列通常呈现出较差的统计特征，包括序列相关、异方差等。</li><li>Tick bars 是指每隔固定的（如1000次）交易次数提取上述的变量信息。一些研究发现这一取样方法得到的数据更接近独立正态同分布 [Ane and Geman 2000]。 使用Tick Bars 还需注意异常值 (outliers) 的处理。一些交易所会在开盘和收盘时进行集中竞价，在竞价结束后以统一价格进行撮合。</li><li>Volume Bars &amp; Dollar Bars：Volume Bars 是指每隔固定的成交量提取上述的变量信息。Dollar Bars 则使用了成交额。 使用 Dollar Bars更有优势的。假设一只股票在一定时间区间内股价翻倍，期初10000元可以购买的股票将会是期末10000元可购买股票手数的两倍。在股价有巨大波动的情况下，Tick Bars以及Volume Bars每天的数量都会随之有较大的波动。除此之外，增发、配股、回购等事件也会导致Tick Bars以及Volume Bars每天数量的波动</li></ol><h1 id="pitfall-4-整数差分">Pitfall 4: 整数差分</h1><blockquote><p>Pitfall 4: 整数差分（INTEGER DIFFERENTIATION）</p><p>Solution #4: 非整数差分（FRACTIONAL DIFFERENTIATION）</p></blockquote><p>我们需要在数据平稳性和保留数据信息之间做取舍，非整数/分数差分就是一个较好的解决方案</p><h1 id="pitfall-5-固定时间范围标签">Pitfall 5: 固定时间范围标签</h1><blockquote><p>Pitfall 5: 固定时间范围标签（FIXED-TIME HORIZON LABELING）</p><p>Solution 5: 三边界方法（THE TRIPLE-BARRIER METHOD）</p></blockquote><ol type="1"><li>固定时间范围标签方法应用广泛, 但是有若干不足：time bars 的统计性质不好;常数阈值不顾波动性是不明智的;可能被强制平仓.</li><li>三边界方法（THE TRIPLE-BARRIER METHOD）考虑到平仓的触发条件，是更好的处理方式，其包括上下水平边界和右边的垂直边界。水平边界需要综合考虑盈利和止损，其边界宽度是价格波动性的函数（波动大边界宽，波动小边界窄）；垂直边界考虑到建仓后 bar 的流量，如果不采用 time bars，垂直边界的宽度就不是固定的（翻译太艰难了，附上原文）</li><li>如果未来价格走势先触及上边界，可以取1；先触及下边界，则取-2；先触及右边界，可以 0，或者根据盈利正负，取1或者-1 。</li></ol><h1 id="pitfall-6-同时学出方向和规模">Pitfall 6: 同时学出方向和规模</h1><blockquote><p>Pitfall 6: 同时学出方向和规模（LEARNING SIDE AND SIZE SIMULTANEOUSLY）</p><p>Solution #6: 元标签（META-LABELING）</p></blockquote><ol type="1"><li>金融中用ML的另一常见错误是同时学习仓位的方向和规模。</li><li>具体而言，方向决策（买/卖）是最基本的决策，规模决策（size decision）是风险管理决策，即我们的风险承受能力有多大，以及对于方向决策有多大信心。</li><li>我们没必要用一个模型处理两种决策，更好的做法是分别构建两个模型：第一个模型来做方向决策，第二个模型来预测第一个模型预测的准确度。</li><li>很多ML模型表现出高精确度（precision）和低召回率（recall）。这意味着这些模型过于保守，大量交易机会被错过。</li><li>F1-score 综合考虑了精确度和召回率，是更好的衡量指标，元标签（META-LABELING）有助于构建高 F1-score 模型。首先（用专家知识）构建一个高召回率的基础模型。随后构建一个ML模型，用于决定我们是否应该执行基础模型给出的决策。</li><li>元标签+ML有以下4个优势：<ol type="1"><li>元标签+ML则是在白箱（基础模型）的基础上构建的，具有更好的可解释性；</li><li>元标签+ML减少了过拟合的可能性，即ML模型仅对交易规模决策不对交易方向决策，避免一个ML模型对全部决策进行控制；</li><li>元标签+ML的处理方式允许更复杂的策略架构，例如：当基础模型判断应该多头，用ML模型来决定多头规模；当基础模型判断应该空头，用另一个ML模型来决定空头规模；</li><li>赢小输大会得不偿失，所以单独构建ML模型对规模决策是有必要的。</li></ol></li></ol><blockquote><p>achieving high accuracy on small bets and low accuracy on large bets will ruin you</p></blockquote><h1 id="pitfall-7-非iid样本加权">Pitfall 7: 非IID样本加权</h1><blockquote><p>Solution #7: （UNIQUENESS WEIGHTING AND SEQUENTIAL BOOTSTRAPPING）</p></blockquote><p>样本不是iid的，比如按照volumn bars，到达1000的成交量才采集一个样本。</p><p>如果实际数据中，交易发生拥堵，都集中在了t=10，那么： - t=1要到t=10才达到1000， - t=2其实也是到t=10达到1000， - t=3其实也是到t=10达到1000，</p><p>因此，t=10这个样本就被用了很多次（最多9次），当然time bars是没有这个问题的。</p><p>为了缓解这个样本重复出现的问题，作者定义了一个衰减因子即<span class="math inline">\(1/c_t\)</span>, <span class="math inline">\(c_t\)</span>表示t时刻的行情被用了多少次，所以t时刻的行情对应的return应该除以这个次数。</p><p>这些只对那种按成交量或者其他非等时间划分样本的方法有意义 (1) labels are decided by outcomes; (2) outcomes are decided over multiple observations; (3) because labels overlap in time, we cannot be certain about what observed features caused an effect.</p><h1 id="pitfall-8-交叉检验集泄露信息cross-validation-leakage">Pitfall 8: 交叉检验集泄露信息（CROSS-VALIDATION LEAKAGE）</h1><blockquote><p>Pitfall 8: 交叉检验集泄露信息（CROSS-VALIDATION LEAKAGE）</p><p>Solution #8: 清理和禁止（PURGING AND EMBARGOING）</p></blockquote><ol type="1"><li>金融中需要警惕在训练集 / CV 集中引入未来信息。</li><li>好的做法应该是在训练集和CV集之间设定一个间隔</li></ol><h1 id="pitfall-9-前向回测">Pitfall 9: 前向回测</h1><blockquote><p>Pitfall 9: 前向回测（WALK-FORWARD / HISTORICAL BACKTESTING）</p><p>Solution #9: CPCV（COMBINATORIAL PURGED CROSS-VALIDATION）</p></blockquote><ol type="1"><li>常用的回测方法是前向回测（Walk-forward Backtesting）：根据当前时刻以前的数据做决策。这种方式容易解读同时也很直观，但存在几点不足：<ol type="1"><li>前向回测只测试了单个场景，容易过拟合；</li><li>前向回测的结果未必能代表未来的表现。 2.作者提出了一种切分方法：将所有数据分为 N 份（注意避免信息泄露），从中任意取 k份作为测试集，剩下作为训练集，总共有很多种取法。这种方法最大的优势是允许我们得到某策略在不同时期的夏普率分布，而不是计算一个夏普率值。</li></ol></li></ol><h1 id="pitfall-10-回测过拟合">Pitfall 10: 回测过拟合</h1><blockquote><p>Pitfall 10: 回测过拟合（BACKTEST OVERFITTING）</p><p>Solution 10: 保守夏普率（THE DEFLATED SHARPE RATIO）</p></blockquote><ol type="1"><li>假设 <span class="math inline">\({y_i}\)</span> 独立同分布，可证明<span class="math inline">\(E\left[\max \left\{y_{i}\right\}_{i=1, \ldots, I}\right] \leq \sigma \cdot \sqrt{2 \log (I)}\)</span>。</li><li>若<span class="math inline">\(y_i\)</span> 代表一系列回测结果的夏普率，则只要回测次数足够多，或者每次回测结果方差足够大，从中都能选出任意高的结果，尽管有可能 <span class="math inline">\(E(y_i)=0\)</span> 。</li><li>这提醒我们要考虑到回测次数过多会造成过拟合，一种解决方案是保守夏普率（THE DEFLATED SHARPE RATIO，DSR），其思想是给定一系列对夏普率SR的估计值，通过统计检验的方法估计能否推翻零假设 SR=0 。</li></ol><h2 id="参考">参考</h2><ol type="1"><li>《Advances in Financial Machine Learning》</li><li>https://blog.csdn.net/weixin_38753422/article/details/100179559</li><li>https://zhuanlan.zhihu.com/p/69231390</li><li>https://zhuanlan.zhihu.com/p/29208399</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概览&quot;&gt;概览&lt;/h1&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;市场上关于投资的书籍大致分为两类：一类是理论家写的，自己都没有实践过；一类是实践家写的，他们误用了数学工具。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;金融市场上现在鱼龙混扎，小散受到到不良媒体的诱导会冲动投</summary>
      
    
    
    
    <category term="AFML" scheme="https://chiechie.github.io/categories/AFML/"/>
    
    
    <category term="量化" scheme="https://chiechie.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
    <category term="投资" scheme="https://chiechie.github.io/tags/%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>投资、投机与套利</title>
    <link href="https://chiechie.github.io/2021/07/04/reading_notes/investment/quant/"/>
    <id>https://chiechie.github.io/2021/07/04/reading_notes/investment/quant/</id>
    <published>2021-07-04T07:10:19.000Z</published>
    <updated>2021-07-09T00:29:06.181Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li>巴菲特希望通过分析公司基本面来做精准判断，通过每次投资的长期持有获得收益，而西蒙斯用机器算法来发现短期胜率，通过短期胜率超过50%加上大数定律（law of large numbers）来保证长时间的盈利</li><li>人在做投资决策的时候都会根据自己的经历和知识储备选取合适的策略。</li><li>从交易的方式来看，投资的策略有三种形式：投资（investment）、投机（speculation）和套利（arbitrage），三种形式都可以用量化的方法来提高胜率。</li><li>1949年，格雷厄姆在《聪明的投资者》里写到：“投资者与投机者最实际的区别在于他们对股市运动的态度上：投机者的兴趣主要在参与市场波动并从中谋取利润，投资者的兴趣主要在以适当的价格取得和持有适当的股票。”</li><li>所谓价值投资其实是“价值和价格之差”投资，然后低买高卖。他说投资的秘诀是“不要赔钱”。</li><li>在金融市场里，如果看不清30年这么长的时间线，分析交易量、交易价格和其它技术和基本面的数据，对股票未来短期内的变化，可以在日或周这样的时间线上找到机会。这样的投机行为并不关心股票未来的成长，但是只要在短期内做到足够好的判断，则可以成功地投机获利。</li><li>如果把时间线再缩短，就会有套利的机会。套利背后的逻辑是低买高卖</li><li>很多高频交易的策略，在市场的大量卖单和买单中找到规律，用一个较低的价格买回股票，然后找到下一个买家用高一点的价格卖出去。</li><li>从投资到投机再到套利，随着交易速度的提升，超额收益会越来越高，但是这样的速度提升也有缺点，那就是随着交易速度的提升，机会的窗口就比较小，盈利的容量会越来越小。如果把股市比作赌场，随着交易速度的提升，赢钱的概率会提高，但是能赢的钱的总量却是在下降的。</li><li>高频的量化投资有点像从沙子里捞金子，每捞一次在付出成本的同时都有一个概率找到金子，捞金子的收益可以从两方面得到，一是捞金子的成功率，这个可以通过优化算法加强预测准确率来得到；二是交易频率，可以通过增加单位时间的交易次数来达到。总收益大致和正确率与交易次数平方根的乘积是成正比的。这个原理叫主动管理基本定律The Fundamental Law of Active Management。</li><li>机器学习做交易策略的一个误区。大家一上来就在想办法预测股价，这个思路是最直接的，从数据分析的层面看，这并没有错，很多对算法很熟悉的人都非常厉害，可以迅速的找到一些算法（例如xgboost）来做非常好的样本内预测。但是一旦在实际股市中使用的时候就会发现预测准确度远远不如历史数据所做出来的。</li><li>问题不是机器学习的算法不够好，而是所有的预测模型都假设底层的市场逻辑没有变化，这样的假设是错误的，导致了过度拟合</li><li>如今的市场上，找一个会用厉害的算法做预测的码农并不难，难的是管理者和投资决策者需要知道如何从最底层理解金融市场的量化思维并发挥出算法的优势。</li></ol><h2 id="参考">参考</h2><ol type="1"><li>https://zhuanlan.zhihu.com/p/362383721</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;巴菲特希望通过分析公司基本面来做精准判断，通过每次投资的长期持有获得收益，而西蒙斯用机器算法来发现短期胜率，通过短期胜率超过50%加上大数定律（law of large numbers）来保证长时间的盈利&lt;/li&gt;
&lt;li&gt;人在做投资决策的时候</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="量化" scheme="https://chiechie.github.io/tags/%E9%87%8F%E5%8C%96/"/>
    
    <category term="投资" scheme="https://chiechie.github.io/tags/%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>《巫师唐望》读书笔记</title>
    <link href="https://chiechie.github.io/2021/07/03/reading_notes/zhexue/wushitangwang/"/>
    <id>https://chiechie.github.io/2021/07/03/reading_notes/zhexue/wushitangwang/</id>
    <published>2021-07-03T07:30:45.000Z</published>
    <updated>2021-07-05T06:53:28.420Z</updated>
    
    <content type="html"><![CDATA[<h2 id="停顿世界">停顿世界</h2><blockquote><p>stopping the world。</p></blockquote><ol type="1"><li>抹掉一切个人历史，免得我们受别人思想的牵绊。</li><li>"他们一旦知道你，你就被视为理所当然的，从那一刻开始，你就没有办法打破他们思想的束缚。我个人很喜欢那种（抹掉个人历史带来的）不为人知的终极自由。没有人能确切地了解我，像人们了解你一样"，唐望说。</li><li>“如果一个人没有个人历史，不论他说什么，都不会被当成谎言，而你的麻烦是你一定得向每个人说明每一件事，同时又希望保持行为的新鲜感。可是在说明所做的一切之后，你没法再兴奋，为了能好好活下去，你只好撒谎。</li><li>“从现在开始，你必须只让人知道你愿意让人知道的，但是不必说明你是怎样做到的。”</li><li>我们只有两条路；或者把一切都当成确定的、真实的：或者不这么做。如果走第一条路，最后会对自己以及世界感到厌倦至死。如果走第二条路，抹去个人历史，我们就在自己周围制造出一层雾，那是一种让人刺激而且神秘的状态，没有人知道兔子会从哪里冒出来，甚至连自己也不知道。</li><li>在没有一样事情是确定时，我们会一直保持警觉，会永远小心翼翼，不知道兔子藏在哪丛灌木后面，要远比假装知道一切来得刺激。</li><li>“你把自己看得太重了，”他慢条斯理地说，“在你心里，你把你自己看得太该死的重要。一定要改！你是如此该死的重要，使你觉得可以理直气壮地对每件事恼火。你是如此该死的重要，所以事情只要不如你的意，你可以掉头就走。你大概以为那样表示你有个性。胡扯！你是又软弱，又自命不凡！”.他指出，因为我加在身上这种夸大的重要感，使我这辈子一事无成。</li><li>“自我重要感是另一件必须丢弃的东西，就像个人历史。”</li><li>我喜欢他这种如谜般的谈话，神秘而带挑战性。不过，我无法判断这些是深奥难懂还是一派胡言。</li><li>我们现在所关心的是丢掉自我重要感。只要你还是感觉你是世界上最重要的事物，就不能真正欣赏周围的世界，就好像一匹戴着眼罩的马只能看到一个远离一切事物的自己</li><li>“死亡是我们永恒的伴侣，”唐望以最严肃的语气说，“它永远在我们的左边，一臂之遥。在你监视白鹰时，它也在监视你，它在你耳边低语，于是你感觉到了它的寒意，就像今天一样。死亡永远在监视你，直到有一天它轻轻碰触你。”</li><li>“你这个男孩，偷偷地潜行追踪猎物，也知道耐心等待，就像死亡的等待。你非常清楚死亡就在我们的左边，就像你在白鹰的左边那样。”</li><li>“如果我们知道死亡正在潜猎我们，又怎能感觉自己如此重要呢？”他问。</li><li>“当你不耐烦时，”他继续说，“你应该转向左边，向死亡寻求忠告。如果死亡对你打个手势，或你瞥见了它，或者你只要感觉它在那儿守望你，你就可以抛弃许多令人心烦的琐事。”</li><li>“死亡是我们仅有的明智忠告者。当你觉得一切都不顺利，一切就要完蛋的时候，转身问问死亡事实是否如此。你的死亡会告诉你，你错了；除了它的触摸之外，一切都无关紧要。它会告诉你：‘我还没有碰你呢！’”</li><li>不知为什么，他的笑声不再像过去那样无礼而令人讨厌。我不认为笑的声调、大小、笑意和过去有什么不同，不同的是我的心情。从死亡随时会降临的观点看，我的恐惧与恼火都失去了意义。</li></ol><p>猎人知道他会一次又一次地把猎物引进陷阱里，因此他不忧虑。他忧虑的话，就会被得到，不知不觉地被得到。一旦你开始忧虑，你就会因为绝望而抓住任何东西；一旦你抓住东西不放，就会为之耗尽你的力量，或耗尽你所抓住的人或东西。</p><p>把你的注意力集中在你和死亡的联系上，没有反悔、悲伤或忧虑。集中心思去想，你已经没有时间了，然后让你的行动自然发生，让你的一举一动都成为你在世上的最后一战。只有在这种情况下，你的行动才有正当的力量。否则，你穷尽一生所为，也不过是个胆怯的人而已</p><h2 id="参考">参考</h2><ol type="1"><li><a href="https://weread.qq.com/web/reader/7c332a60717d350b7c30d04">唐王三部曲-微信读书</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;停顿世界&quot;&gt;停顿世界&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;stopping the world。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;抹掉一切个人历史，免得我们受别人思想的牵绊。&lt;/li&gt;
&lt;li&gt;&quot;他们一旦知道你，你就被</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="哲学" scheme="https://chiechie.github.io/tags/%E5%93%B2%E5%AD%A6/"/>
    
    <category term="自我中心主义" scheme="https://chiechie.github.io/tags/%E8%87%AA%E6%88%91%E4%B8%AD%E5%BF%83%E4%B8%BB%E4%B9%89/"/>
    
  </entry>
  
  <entry>
    <title>图的最大流和最小割算法</title>
    <link href="https://chiechie.github.io/2021/07/02/data_structure/graph/graph-basic/min-cut/"/>
    <id>https://chiechie.github.io/2021/07/02/data_structure/graph/graph-basic/min-cut/</id>
    <published>2021-07-02T00:48:14.000Z</published>
    <updated>2021-07-03T06:16:12.561Z</updated>
    
    <content type="html"><![CDATA[<h2 id="最大流">最大流</h2><h3 id="最大流问题">最大流问题</h3><p>在一个网络中，求从起点、source到目标点，经过的最大的流量，每条边的权重等于该管道的最大流量，求整个路径的最大流量。</p><p><img src="img_3.png" /></p><p>残差 = 容量 - 真实流量 <img src="img_4.png" /></p><h3 id="最大流算法">最大流算法</h3><p>最简单的方法，但是未必能找到最大流</p><figure><img src="img_5.png" alt="img_5.png" /><figcaption aria-hidden="true">img_5.png</figcaption></figure><p>通过多次迭代，先找可达路径，计算残差图，移走空闲量=0的边，进入第二次循环。</p><h2 id="最小割">最小割</h2><h3 id="最小割问题">最小割问题</h3><p>最小割要解决的问题和最大流是一样的</p><p>输入：方向有权图 目标：割的容量最小 输出：某个S-T cut，</p><blockquote><p>最大流最小割定理（Max-Flow Min-Cut Theorem）</p><p>在一个网络流量中，从s到t的最大流量等于，最小s-t cut的容量。</p><p>--L. R. Ford and D. R. Fulkerson. Flows in Networks. Princeton University Press, (1962 .)</p></blockquote><p><img src="img_1.png" /></p><h3 id="寻找最小割的方法">寻找最小割的方法</h3><ol type="1"><li>使用最大流算法获得residual graph， 移走其中反向的边</li><li>在residual graph中，从起点s出发，找到所有能达到的节点，并记为集合S，把其他所有节点记做T（s到不了的节点）。</li><li>将{S, T}记做最小割。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.youtube.com/watch?v=6DFWUgV5Osc&amp;t=774s">图的最大流和</a></li><li><a href="https://www.youtube.com/watch?v=Ev_lFSIzNh4&amp;t=128s">图的最小割算法-youtube</a></li><li><a href="https://github.com/wangshusen/AdvancedAlgorithms">图的最小割算法-slide</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;最大流&quot;&gt;最大流&lt;/h2&gt;
&lt;h3 id=&quot;最大流问题&quot;&gt;最大流问题&lt;/h3&gt;
&lt;p&gt;在一个网络中，求从起点、source到目标点，经过的最大的流量，每条边的权重等于该管道的最大流量，求整个路径的最大流量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;img_3.png&quot; </summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="图算法" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E7%AE%97%E6%B3%95/"/>
    
    <category term="图" scheme="https://chiechie.github.io/tags/%E5%9B%BE/"/>
    
    <category term="最小割" scheme="https://chiechie.github.io/tags/%E6%9C%80%E5%B0%8F%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>基于Ray的融合计算框架</title>
    <link href="https://chiechie.github.io/2021/06/30/reading_notes/computer/fenbushi/"/>
    <id>https://chiechie.github.io/2021/06/30/reading_notes/computer/fenbushi/</id>
    <published>2021-06-30T06:39:39.000Z</published>
    <updated>2021-07-01T11:56:15.297Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>看到阿里在2021发表的一个演讲--《新一代的计算基础设施-对于Ray的融合计算》，做一些笔记</p></blockquote><ol type="1"><li>举一个复杂的业务的例子，用户支付流程：</li></ol><ul><li>先选择支付场景--线上/线下/，</li><li>选择支付方式--</li><li>选择交易网络--银联/第三方</li><li>选择银行</li></ul><figure><img src="./img_1.png" alt="img_1.png" /><figcaption aria-hidden="true">img_1.png</figcaption></figure><p>整个流程需要秒级响应。</p><ol start="2" type="1"><li>要是大家都开始用统一的计算平台，AIOps也可以用同一套方案了</li></ol><figure><img src="./img_2.png" alt="img_2.png" /><figcaption aria-hidden="true">img_2.png</figcaption></figure><ol start="3" type="1"><li>业务需求之一是，复杂的业务逻辑计算存在单机性能瓶颈，需要支持分布式无范式的分布式开发。</li><li>现在的基础设计在单一用途的组件上，已经做的比较成熟了，专门做流式计算的计算引擎有flink，专门做深度学习的计算引擎有tensorflow，专门做批处理的有spark。</li><li>但是，考虑到一个复杂的应用场景，完成1个任务需要用到多种计算模式，统一计算平台 能够让这几种计算模式实现状态共享，中间结果共享。在这种场景下，统一计算平台，相较于多个独立的计算组件 效率更高。</li><li>要不要搞统一计算平台，取决于需求的复杂性，如果是一个很pure很纯粹的任务，搞这个的意义就不大。但是可以作为预研嘛，万一以后业务变得越来越复杂，也有准备。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.bilibili.com/video/BV1gh411Y7qf?t=1">Ray Forward Meetup 2021-面向金融决策场景的在线计算系统-bilibili</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;看到阿里在2021发表的一个演讲--《新一代的计算基础设施-对于Ray的融合计算》，做一些笔记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;举一个复杂的业务的例子，用户支付流程：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="大数据" scheme="https://chiechie.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="读书笔记" scheme="https://chiechie.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机原理" scheme="https://chiechie.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="分布式计算" scheme="https://chiechie.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>零知识量证明</title>
    <link href="https://chiechie.github.io/2021/06/29/reading_notes/qukuailian/zkp/"/>
    <id>https://chiechie.github.io/2021/06/29/reading_notes/qukuailian/zkp/</id>
    <published>2021-06-29T15:54:57.000Z</published>
    <updated>2021-07-05T03:28:18.664Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念">基本概念</h2><ol type="1"><li><p>零知识证明(ZKP)的定义为：证明者（prover）能够在不向验证者（verifier）提供任何有用的信息的情况下，使验证者（verifier）相信某个论断是正确的。</p></li><li><p>零知识证明具有三个重要的性质：</p><ul><li>完备性（Completeness）：只要证明者拥有相应的知识，那么就能通过验证者的验证，即证明者有足够大的概率使验证者确信。---验证者的问题是可解的。</li><li>可靠性（Soundness）：如果证明者没有相应的知识，则无法通过验证者的验证，即证明者欺骗验证者的概率可以忽略。---验证者的问题是有难度的。</li><li>零知识性（Zero-Knowledge）：证明者在交互过程中仅向验证者透露是否拥有相应知识的陈述，不会泄露任何关于知识的额外信息。---验证者是无不要知道解答过程的，它只在另外一个问题空间验证。</li></ul></li><li><p>零知识证明是一种基于概率的验证方式，验证者（verifier）向证明者（prover)提出多个随机问题，如果证明者都能给出正确回答，则说明证明者大概率拥有他所声称的“知识”。零知识证明并不是数学意义上的证明，因为它存在小概率的误差，欺骗的证明者有可能通过虚假的陈诉骗过验证者，但是，可通过技术手段将误差降低到可以忽略的值。</p></li></ol><h2 id="应用场景">应用场景</h2><p>零知识证明在区块链上的两大应用场景：隐私和扩容。</p><p>隐私：在隐私场景中，我们可以借助零知识证明的“不泄露信息”特性，在不泄漏交易的细节（接收方，发送方，交易余额）的情况下证明区块链上的资产转移是有效的。</p><p>扩容：在扩容场景中，我们不太需要关注零知识证明技术的“不泄露信息”这个特性，我们的关注重点是它的“证明论断有效”这个特性，由于链上资源是有限的，所以我们需要把大量的计算迁移到链下进行，因此需要有一种技术能够证明这些在链下发生的动作是可信的，零知识证明正好可以帮助我们做链下可信计算的背书。</p><p>目前，使用零知识证明技术的应用有隐私币(例如zcash)，以太坊上的混币合约，链下扩容技术zkRollup。</p><h3 id="隐私">隐私</h3><h4 id="为什么需要隐私币">为什么需要隐私币？</h4><ol type="1"><li><p>举个生活中的例子：游客向庙里功德箱中仍香火钱，所有的游客仍的都是同一个年份的一元硬币，这时有一个第三方在一旁观察，他可以知道谁在什么时间扔进去多少个硬币。但是当小沙弥从功德箱中取钱的时候，他无法分别取出的硬币是由谁扔进去的。这里功德箱就起到一个混币的功能.同理，为了保证区块链上交易的隐私性，也可以进行混币。</p></li><li><p>混币的目的是切断加密货币交易中发送方与接受方的联系，发送方利用混币系统将自己的钱与其他人的钱进行混合，接受方(Verifier)利用零知识证明来证明有某一个混币的所有权，从而进行转账交易。</p></li><li><p>需要隐私币的第一个理由：隐私币能实现匿名且不可追踪。</p><ul><li>目前公链（比特币）的匿名只起到假名的作用，例如现实生活中的人可以生成任意多的公私钥对，用这些公钥在链上发送或接受每笔交易，这些公钥就充当他们的假名。如果外界不知道你和公钥的关系，他们就无法把你和你的交易历史关联起来，只要有人能把你跟公钥联系起来，就可以顺藤摸瓜找到你过去的交易历史。目前不没有办法阻止第三方将我们和我们的公钥联系起来。</li></ul></li><li><p>需要隐私币的第二个理由：由于隐私币无法查看货币的交易记录，所以减少了货币不可互换的问题。</p><ul><li>流通性使货币具有了内在可互换性,但是加密货币具有极度透明性，我们可以追踪到与某一特定货币的所有相关历史交易，这样一来，人们一旦发现某个货币是“污点”货币（俗称“黑钱”）就可以拒绝接受这种货币。如果这种情况大规模发生，加密货币将不再是可互换的因为“干净”的货币比“污点”货币具有更大价值。</li><li>不可互换的货币会给用户带来额外的负担，用户为了避免不小心买入“污点”货币，那么用户就会被迫检查他们购买的每笔货币的交易历史。</li></ul></li><li><p>下图是使用零知识证明的一般过程，在circuit中会执行一些约束，这些约束是与要解决的问题是相关的。Private input的值只有Prover自己知道,Public input的值是Prover与Verifier共享的。该过程可以总结为，Prover在不揭露Private input 的情况下向Verifier证明自己知道一个值能满足（x+3=5)。</p><figure><img src="./img.png" alt="基于circuit的零知识证明" /><figcaption aria-hidden="true">基于circuit的零知识证明</figcaption></figure></li></ol><h4 id="zk-snark的流程图">zk-SNARK的流程图</h4><p>下图是zk-SNARK的流程图，zk-SNARK不能直接用于解决计算问题，必须先把问题转换成正确的“形式”（即"quadratic arithmetic problem"，QAP)，在转换为QAP的同时，可以用Private input和Public input创建一个对应的解，称为QAP的witness。只有Prover用这个witness来生成proof。</p><figure><img src="./img_1.png" alt="zk-Snark流程图" /><figcaption aria-hidden="true">zk-Snark流程图</figcaption></figure><p>整个过程如下：</p><ul><li>首先得有一个计算问题，这个问题一般是NP问题</li><li>然后将计算问题做一个等价转换变成QAP，步骤如下：<ul><li>将计算问题拍平（flatten）变成circuit</li><li>把circuit转化成 R1CS(rank-1 constraint system，一阶约束系统)。R1CS 是一个由三向量组 (a,b,c) 组成的序列，R1CS 有个解向量s（就是witness），s 必须满足符号表示向量的内积运算 a.s * b.s - c.s = 0</li><li>将R1CS转化成QAP形式，这两者的区别是QAP使用多项式来代替点积运算，他们所实现的逻辑完全相同。</li></ul></li><li>trusted setup会生成两个值PK，VK，truseted setup的目的是实现零交互验证，它生成的PK，VK相当于是一个“上帝”,由它来帮我们验证Prover。</li><li>Prover用PK生成一个Proof交给Verifier</li><li>Verifier拿到这个Proof会用VK做校验，这一步发生在链上，由链上的节点或智能合约来做校验。 &gt; PK就像一个query word，VK是该query word对应的answer，CRS相当于是一个生成了一个&lt;question，answer&gt;，拿Prover的答案跟正确的已知的答案进行对比，从而验证Prover是qualified</li></ul><h3 id="扩容">扩容</h3><ol type="1"><li>17年出现了一款非常火爆的Dapp应用叫加密猫，加密猫曾造成以太坊主网大规模的拥堵，造成拥堵的原因是以太坊当时的TPS只有15，这意味着以太坊每秒只能处理15笔交易，如此低的TPS严重限制了区块链应用的大规模落地，所以有人开始研究区块链扩容的问题，目的就是为了提高链上的TPS。</li><li>但区块链扩容受到Vitalik提出的不可能三角的限制(区块链系统设计无法兼顾可扩展性，去中心化和安全性).但我们必须知道，一切事物都有自己的边界，公链不应该做所有的事情，公链应该做它该做的事情：“公链是以最高效率达成共识的工具，能够以最低成本来构建信任”。</li><li>作为共识的工具，信任的引擎，公链不应该为了可扩展性放弃去中心化与安全性。那么公链的TPS这么低，该怎么使用呢？可以将大量的工作放到链下去解决，仅仅将最重要的数据提交到区块链主链上，让所有节点都能够验证这些链下的工作都是准确可靠的.</li><li>社会的发展带来的是更精细化的分工，区块链的技术发展也是如此，在底层区块链（Layer1）上构建一个扩展层（Layer2)，Layer1来保证安全和去中心化，绝对可靠、可信；它能做到全球共识，并作为“加密法院”，通过智能合约设计的规则进行仲裁，以经济激励的形式将信任传递到Layer2上，而Layer2追求极致的性能，它只能做到局部共识，但是能够满足各类商业场景的需求。</li></ol><h2 id="发展历史">发展历史</h2><ol type="1"><li>1985 年，零知识证明Zero-Knowledge Proof - 由 S.Goldwasser、 S.Micali 及 C.Rackoff 首次提出。</li><li>2010年，Groth实现了首个基于椭圆曲线双线性映射全能的，常数大小的非交互式零知识证明协议。后来这个协议经过不断优化，最终成为区块链著名的零知识证明协议SNARKs。</li><li>2013年，Pinocchio协议实现了分钟级别证明，毫秒级别验证，证明大小不到300字节，将零知识证明从理论带到了应用。后来Zcash使用的SNARKs正是基于Pinocchio的改进版。</li><li>2014 年，名为Zerocash的加密货币则使用了一种特殊的零知识证明工具zk-SNARKs （ Zero-Knowledge Succinct Non-interactive Arguments of Knowledge ) 实现了对交易金额、交易双方的完全隐藏，更注重于隐私，以及对交易透明的可控性。</li><li>2017 年， Zerocash 团队提出将 zk-SNARKs 与智能合约相互结合的方案，使交易能在众目睽睽下隐身，打造保护隐私的智能合约。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://zhuanlan.zhihu.com/p/152065162">零知识证明介绍-zhihu</a></li><li><a href="https://www.notboring.co/p/zero-knowledge">Zero Knowledge, from not boring</a></li><li><a href="https://mp.weixin.qq.com/s/_IrI8SJLo1Ht51nJfI4V_Q">十分钟开发一个混币-原理篇</a></li><li><a href="https://mp.weixin.qq.com/s/8OkwqNXIkUz2PBURoghRJQ">十分钟开发零知识证明之混币</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本概念&quot;&gt;基本概念&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;零知识证明(ZKP)的定义为：证明者（prover）能够在不向验证者（verifier）提供任何有用的信息的情况下，使验证者（verifier）相信某个论断是正确的。&lt;/p&gt;&lt;/li&gt;
&lt;l</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="加密货币" scheme="https://chiechie.github.io/tags/%E5%8A%A0%E5%AF%86%E8%B4%A7%E5%B8%81/"/>
    
    <category term="零知识量证明" scheme="https://chiechie.github.io/tags/%E9%9B%B6%E7%9F%A5%E8%AF%86%E9%87%8F%E8%AF%81%E6%98%8E/"/>
    
  </entry>
  
  <entry>
    <title>几个思维模型</title>
    <link href="https://chiechie.github.io/2021/06/28/reading_notes/reality/10-mental-model/"/>
    <id>https://chiechie.github.io/2021/06/28/reading_notes/reality/10-mental-model/</id>
    <published>2021-06-28T04:50:00.000Z</published>
    <updated>2021-06-28T05:47:15.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>基于剃刀原则的几个思维模型，from Sahil Bloom的twitter</p></blockquote><h2 id="the-steve-jobs-quality-razor">The Steve Jobs Quality Razor</h2><p>When building, take pride in carrying the quality all the way through.Would you be proud for your work to be seen from every angle and perspective? If not, keep working.</p><h2 id="the-eli5-razor">The ELI5 Razor</h2><ul><li>Complexity and jargon are often used to mask a lack of true understanding.</li><li>If you can’t explain it to a 5-year-old, you don’t really understand it.</li><li>If someone uses a lot of complexity and jargon to explain something to you, they probably don’t understand it.</li></ul><h2 id="mungers-rule-of-opinions">Munger’s Rule of Opinions</h2><p>“I never allow myself to have an opinion on anything that I don’t know the other side’s argument better than they do.” - Charlie Munger</p><p>Opinions aren’t free. You have to work to earn the right to have them.</p><h2 id="the-bezos-regret-minimization-framework">The Bezos Regret Minimization Framework</h2><p>The goal is to minimize the number of regrets in life.</p><p>When faced with a difficult decision: (1) Project yourself into the future (2) Look back on the decision (3) Ask "Will I regret not doing this?" (4) Take action</p><h2 id="buffetts-rule-of-holes">Buffett’s Rule of Holes</h2><p>“The most important thing to do if you find yourself in a hole is to stop digging." - Warren Buffett</p><p>When things aren’t working, change course and try something different.</p><p>When you find yourself at the bottom of a hole, stop digging and climb out of it.</p><h2 id="pgs-crazy-idea-razor">PG's Crazy Idea Razor</h2><p>If someone proposes a crazy idea, ask:</p><ol type="1"><li>Are they a domain expert?</li><li>Do I know them to be reasonable?</li></ol><p>If yes on (1) and (2), you should take the idea seriously, as it may be an asymmetric bet on the future.</p><h2 id="the-boasters-razor">The Boaster’s Razor</h2><p>Truly successful people rarely feel the need to boast about their success.</p><p>If someone regularly boasts about their income, wealth, or success, it’s fair to assume the reality is a fraction of what they claim.</p><h2 id="the-circle-of-competence">The Circle of Competence</h2><p>Be ruthless in identifying your circle of competence (and its boundaries).</p><p>When faced with a big decision, ask yourself whether you are qualified to handle it given your circle.</p><p>If yes, proceed. If no, outsource it to someone who is.</p><h2 id="the-duck-test">The Duck Test</h2><p>If it looks like a duck, swims like a duck, and quacks like a duck, it’s probably a duck.</p><p>You can determine a lot about a person by regularly observing their habitual characteristics.</p><h2 id="buffetts-juicy-pitch">Buffett’s Juicy Pitch</h2><p>“You don't have to swing at everything - you can wait for your pitch." - Warren Buffett</p><p>Life doesn’t reward you for the number of swings you take.</p><p>Slow down and focus on identifying the juiciest pitch.</p><p>When it comes, swing hard and don’t miss it.</p><h2 id="occams-razor">Occam’s Razor</h2><p>The simplest explanation is often the best one.</p><p>Simple assumptions &gt; complex assumptions.</p><p>Simple is beautiful.</p><h2 id="the-buffett-reputation-razor">The Buffett Reputation Razor</h2><p>“It takes 20 years to build a reputation and five minutes to ruin it. If you think about that, you'll do things differently.” - Warren Buffett</p><p>Remember that quote and act accordingly.</p><p>Your character is your fate.</p><h2 id="hanlons-razor">Hanlon’s Razor</h2><p>Never attribute to malice that which can be adequately explained by stupidity.</p><p>In assessing someone's actions, we should not assume negative intent if there is a viable alternative explanation, such as different beliefs, incompetence, or ignorance.</p><p>汉隆的剃刀</p><p>永远不要把可以用愚蠢充分解释的事情归咎于恶意。</p><p>在评估某人的行为时，如果有可行的替代解释，例如不同的信念、无能或无知，我们不应假设消极意图。</p><h2 id="nntalebs-the-look-the-part-razor">nntaleb's The “Look the Part” Razor</h2><p>If forced to choose between two options of seemingly equal merit, choose the one that doesn’t look the part.</p><p>The one who doesn’t look the part has had to overcome much more to achieve its status than the one who fit in perfectly.</p><p>如果被迫在看似同等价值的两个选项之间做出选择，请选择一个看起来不合适的选项。</p><p>与完美契合的人相比，看起来不合群的人必须克服更多才能获得地位。</p><h2 id="newtons-flaming-laser-sword">Newton’s Flaming Laser Sword</h2><p>If something cannot be settled by experiment or observation, it is not worth debating.</p><p>This will save you from wasting a lot of time on pointless arguments.</p><p>牛顿的火焰激光剑</p><p>如果一些事情不能通过实验或观察来解决，那就不值得争论了。</p><p>这将使您免于在无意义的争论上浪费大量时间。</p><h2 id="machiavellis-razor">Machiavelli’s Razor</h2><p>Never attribute to malice that which can be adequately explained by self-interest.</p><p>In assessing someone's actions, we should not assume negative intent if there is a viable alternative explanation that they are acting on rooted self-interest.</p><p>马基雅维利的剃刀</p><p>永远不要将可以用自身利益充分解释的事情归咎于恶意。</p><p>在评估某人的行为时，如果有一个可行的替代解释表明他们是根据根深蒂固的自身利益行事，我们不应该假设消极意图。</p><h2 id="hitchens-razor">Hitchens’ Razor</h2><p>What can be asserted without evidence can also be dismissed without evidence.</p><p>The burden of proof regarding a claim lies with the one who makes the claim. If unmet, no argument is required to dismiss it.</p><h2 id="sagans-standard">Sagan’s Standard</h2><p>“Extraordinary claims require extraordinary evidence.”</p><p>The more crazy and outrageous the claim, the more crazy and outrageous the body of evidence must be in order to prove it.</p><p>萨根的标准</p><p>“非凡的主张需要非凡的证据。”</p><p>声称越疯狂和越离谱，为了证明它，证据主体就必须越疯狂和离谱。</p><h2 id="the-eisenhower-decision-matrix">The Eisenhower Decision Matrix</h2><p>When faced with a task, ask: “Is this urgent? Is this important?”</p><p>An "urgent" task is one that requires immediate attention. An "important" task is one that promotes or furthers your long-term goals.</p><p>Place it on a 2x2 matrix and act accordingly.</p><h2 id="the-steve-jobs-settling-razor">The Steve Jobs Settling Razor</h2><p>“The only way to do great work is to love what you do. If you haven’t found it yet, keep looking. Don’t settle.” - Steve Jobs</p><p>It’s Monday morning. Did you wake up with energy or with dread?</p><p>Your answer will tell you if you’re settling.</p><h2 id="the-career-razor">The Career Razor</h2><p>When deciding on a new job, choose the one that will challenge you the most (intellectually, physically, or emotionally).</p><p>Challenge and discomfort forces growth.</p><p>(P.S. Check out the job board below for challenging new roles!)</p><h2 id="decisions">Decisions</h2><p>• If you can’t decide, the answer is no.</p><p>• If two equally difficult paths, choose the one more painful in the short term (pain avoidance is creating an illusion of equality).</p><p>• Choose the path that leaves you more equanimous in the long term."</p><h2 id="参考">参考</h2><ol type="1"><li>https://pallet.xyz/list/thebloomboard/jobs</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;基于剃刀原则的几个思维模型，from Sahil Bloom的twitter&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;the-steve-jobs-quality-razor&quot;&gt;The Steve Jobs Quality Razor&lt;</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="决策" scheme="https://chiechie.github.io/tags/%E5%86%B3%E7%AD%96/"/>
    
  </entry>
  
  <entry>
    <title>进程,线程和协程</title>
    <link href="https://chiechie.github.io/2021/06/27/reading_notes/computer/thread-and-process/"/>
    <id>https://chiechie.github.io/2021/06/27/reading_notes/computer/thread-and-process/</id>
    <published>2021-06-27T04:16:07.000Z</published>
    <updated>2021-07-02T00:47:36.273Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本定义">基本定义</h2><h3 id="进程process">进程（process）</h3><p>进程（process）有时候也称做任务，是指一个程序运行的实例。</p><p>我们想要计算机要做一项任务（task），我们会写一段代码（python/java等）。</p><p>编译器将它翻译成二进制代码--机器的语言。</p><p>但是此时不执行这段断码的话，就还是一段静态程序。</p><p>当执行起来的时候，就变成了一个进程。</p><h4 id="进程的3种状态">进程的3种状态</h4><p>从程序员的角度，可以认为进程总是处于下面三种状态之一：</p><ul><li><p>运行。进程要么在CPU上执行，要么在等待被执行且最终会被内核调度</p></li><li><p>停止。进程的执行被挂起（suspend），且不会被调度。当收到SIGSTOP/SIGTSTP/SIDTTIN/SIGTTOU信号时，进程就停止，直收到SIGCONT信号，这时，进程再次运行</p></li><li><p>终止。进程永远的停止了。进程会因为三种原因终止:</p><ol type="1"><li>收到一个终止进程的信号</li><li>从主程序返回；</li><li>调用exit函数。</li></ol></li></ul><h4 id="创建子进程">创建子进程</h4><ol type="1"><li>子进程得到与父进程用户级虚拟地址空间相同的（但是独立的）一份拷贝，包括文本、数据和bss段、堆以及用户栈。</li><li>子进程还获得与父进程任何打开文件描述符相同的拷贝。</li><li>父进程和新创建的子进程之间最大的区别在于它们有不同PID。</li><li>fork函数调用一次，返回两次，父进程中，fork返回子进程的PID，子进程中fork返回0。</li></ol><h4 id="进程的地址空间">进程的地址空间</h4><ol type="1"><li><p>进程提供给应用程序关键的抽象：</p><ul><li>一个独立的逻辑控制流，它提供一个假象，好像我们的程序独占地使用处理器</li><li>一个私有的地址空间，它提供一个假象，好像我们的程序独占地使用存储器系统</li></ul></li><li><p>每个程序都能看到一片完整连续的地址空间，这些空间并没有直接关联到物理内存，只是操作系统提供的对内存一种抽象，在程序的运行时，会将虚拟地址映射到物理地址。</p></li><li><p>进程的地址空间是分段的，存在所谓的数据段，代码段，bbs段，堆，栈等等。每个段都有特定的作用，下面这张图介绍了进程地址空间中的划分。 <img src="img_3.png" /></p></li><li><p>对于32位的机器来说，虚拟的地址空间大小就是4G，可能实际的物理内存大小才1G到2G，意味着程序可以使用比物理内存更大的空间。</p><ol type="1"><li>从0xc000000000到0xFFFFFFFF共1G的大小是内核地址空间，余下的低地址3G是用户地址空间。</li><li>Code VMA: 即程序的代码段，CPU执行的机器指令部分。通常，这一段是可以共享的，即多线程共享进程的代码段。并且，此段是只读的，不能修改。</li><li>Data VMA: 即程序的数据段，包含ELF文件在中的data段和bss段。</li><li>堆和栈: new或者malloc分配的空间在「堆」上，需要程序猿维护，如果没有主动释放堆上的空间，进程运行结束后会自动释放。「栈」上的是函数栈临时的变量，还有程序的局部变量，自动释放。</li><li>共享库和mmap内容映射区：位于栈和堆之间，例如程序使用的printf，函数共享库printf.o固定在某个物理内存位置上，让许多进程映射共享。mmap是一个系统函数，可以把磁盘文件的一部分直接映射到内存，这样文件中的位置直接就有对应的内存地址。</li><li>命令行参数: 程序的命令行参数</li><li>环境变量：类似于Linux下的PATH，HOME等环境变量，子进程会继承父进程的环境变量。</li></ol></li></ol><h3 id="线程threads">线程（threads）</h3><p>一个进程中的执行的单位。</p><p>线程（thread）：能并行运行，并且与他们的父进程（创建他们的进程）共享同一地址空间（一段内存区域）和其他资源的轻量级的进程</p><h3 id="协程coroutines">协程（CoRoutines）</h3><p>Co：即corperation，Routines即函数。</p><p>协程（CoRoutines）：即用来实现functions 即corperate with each other。</p><p><img src="img_2.png" /></p><p>不同的编程语言有不同的实现协程的方式，在python和js里面，用的较多的就是yield</p><p>用python实现一个协程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_coroutine_body</span>(<span class="params">*args</span>):</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Do some funky stuff</span></span><br><span class="line">        *args = <span class="keyword">yield</span> value_im_returning</span><br><span class="line">        <span class="comment"># Do some more funky stuff</span></span><br><span class="line"></span><br><span class="line">my_coro = make_coroutine(my_coroutine_body)</span><br><span class="line"></span><br><span class="line">x = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">   <span class="comment"># The coroutine does some funky stuff to x, and returns a new value.</span></span><br><span class="line">   x = my_coro(x)</span><br><span class="line">   <span class="built_in">print</span> x</span><br></pre></td></tr></table></figure><h3 id="生成器generator">生成器(generator)</h3><p>A generator is essentially a cut down (asymmetric) coroutine.</p><p>The difference between a coroutine and generator is that a coroutine can accept arguments after it's been initially called, whereas a generator can't.</p><h2 id="应用-vs-线程-vs-进程">应用 vs 线程 vs 进程</h2><p>一个应用，比如chrome，可能会启动多个进程（多个网页）, 一个进有多个线程。</p><p>进程和线程的区别：</p><ul><li>进程（火车）间不会相互影响，一个线程（车厢）挂掉将导致整个进程（火车）挂掉</li><li>线程（车厢）在进程（火车）下行进</li><li>一个进程（火车）可以包含多个线程（车厢）</li><li>不同进程（火车）间数据很难共享，同一进程（火车）下不同线程（车厢）间数据很易共享</li></ul><p>线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据， 进程之间的通信需要以通信的方式（IPC)进行</p><ul><li>进程要比线程消耗更多的计算机资源</li><li>进程间不会相互影响，一个线程挂掉将导致整个进程挂掉</li><li>进程可以拓展到多机，线程最多适合多核</li><li>进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。－"互斥锁"</li><li>进程使用的内存地址可以限定使用量－“信号量”</li></ul><h2 id="硬件多线程vs软件多线程">硬件多线程vs软件多线程</h2><p>CPU架构演进路线：</p><p>多cpu---&gt;超线程--&gt;多core</p><p>https://stackoverflow.com/questions/680684/multi-cpu-multi-core-and-hyper-thread</p><p>其中的超线程（hyper thread）指的硬件多线程，如下图，相当于给一个core，虚拟化为2个core，可以更方便压榨计算机性能</p><h2 id="多进程-or-多线程">多进程 or 多线程？</h2><p>多进程更稳定，但是多线程能达到更高的计算效率</p><figure><img src="img_1.png" alt="左边是单线程，右边是多线程" /><figcaption aria-hidden="true">左边是单线程，右边是多线程</figcaption></figure><p>多线程的优势：</p><ul><li>响应性：比如启动一个网页（启动一个浏览器进程），可以同时并行做个事情，如浏览/下载/问答（并行启动多个线程）。</li><li>资源共享：一个进程上的所有线程共享同一份内存，这样能够让机器的使用效率更高，可以做更多的复杂的事情。---赋能/增效</li><li>更经济： 多进程浪费资源，因为创建1个进程需要分配很多内存和资源，相比之下，创建和切换线程的成本小的多。另外，完成一个复杂的任务，多线程共用一份底层资源，多进程就需要把资源复制几份。又浪费了一遍。--降本</li><li>充分压榨多处理器的架构：</li></ul><blockquote><p>大中台类似多线程，烟囱式开发类似多进程</p></blockquote><h2 id="实践">实践</h2><h3 id="练习1-模拟单线程cpp的进程管理">练习1-模拟单线程CPP的进程管理</h3><p><a href="https://leetcode-cn.com/problems/single-threaded-cpu/">leetcode</a>的题目，</p><p>需求：实现一个任务管理/编排的机制，即，输入一堆任务，每个任务的计划执行时间/执行时长都有，现在有一台单线程CPU，如何安排这些任务的执行顺序？</p><p>分析： 设想应用场景，医院的排队系统/有一堆任务要排期。</p><p>一遍在执行已有的任务，一边有源源不断的接到新的任务，</p><p>每执行完一个任务，check一下距离上次检查，有多少新任务进来了，加到任务池里面，从里面选出最容易的。</p><p>设计1个数据结构：</p><p>1个是优先队列，存放候选任务，并且按照执行时间长短从小到到排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tasks = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">n = <span class="built_in">len</span>(tasks)</span><br><span class="line">timestamp = <span class="number">1</span></span><br><span class="line">candidate_list = []</span><br><span class="line">new_task = []</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="keyword">while</span> (j &lt; n) <span class="keyword">and</span> (tasks[j][<span class="number">0</span>] &lt;= timestamp):</span><br><span class="line">        heapq.heappush(candidate_list, (tasks[j][<span class="number">1</span>], j))</span><br><span class="line">        j+=<span class="number">1</span></span><br><span class="line">        print(j, n)</span><br><span class="line">    print(candidate_list)</span><br><span class="line">    process, index = heapq.heappop(candidate_list)</span><br><span class="line">    print(candidate_list)</span><br><span class="line">    new_task.append(index)</span><br><span class="line">    timestamp += process</span><br><span class="line">new_task</span><br></pre></td></tr></table></figure><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.zhihu.com/question/25532384/answer/411179772">线程和进程的区别是什么？-zhihu</a></li><li><a href="https://www.youtube.com/watch?v=usyg5vbni34">Threading in Python - Advanced Python 16 - Programming Tutorial-youtube</a></li><li><a href="https://www.junmajinlong.com/os/multi_cpu/">计算机原理系列-blog</a></li><li><a href="https://www.junmajinlong.com/os/cpu_cache/">关于CPU上的高速缓存</a></li><li><a href="https://www.youtube.com/watch?v=tqay-vzqSN0">What are CoRoutines in Programming?YOUTUBE</a></li><li><a href="https://stackoverflow.com/questions/715758/coroutine-vs-continuation-vs-generator">生成器和协程</a></li><li><a href="https://buptjz.github.io/2014/04/23/processAndThreads">进程、线程及其内存模型</a></li><li>深入理解操作系统</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本定义&quot;&gt;基本定义&lt;/h2&gt;
&lt;h3 id=&quot;进程process&quot;&gt;进程（process）&lt;/h3&gt;
&lt;p&gt;进程（process）有时候也称做任务，是指一个程序运行的实例。&lt;/p&gt;
&lt;p&gt;我们想要计算机要做一项任务（task），我们会写一段代码（python/j</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="计算机原理" scheme="https://chiechie.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="线程" scheme="https://chiechie.github.io/tags/%E7%BA%BF%E7%A8%8B/"/>
    
    <category term="进程" scheme="https://chiechie.github.io/tags/%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>计算机的存储单元</title>
    <link href="https://chiechie.github.io/2021/06/26/reading_notes/computer/cache-memory/"/>
    <id>https://chiechie.github.io/2021/06/26/reading_notes/computer/cache-memory/</id>
    <published>2021-06-26T01:47:32.000Z</published>
    <updated>2021-06-28T05:56:10.034Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概览">概览</h2><ol type="1"><li><p>计算机上的存储单元的处理速度从快到慢依次是： 寄存器&gt; L1&gt;L2&gt;L3&gt;内存&gt;固态硬盘&gt; 机械硬盘</p><figure><img src="./img_1.png" alt="存储单元" /><figcaption aria-hidden="true">存储单元</figcaption></figure></li><li><p>早期的计算机只有寄存器和内存，但是寄存器的处理速度远高于内存，所以大部分时间是寄存器在等内存，所以CPU是处于空转状态。 经过改进</p></li><li><p>除了寄存器，后面的计算机在CPU中又逐步加入了高速缓存--L1/L2/L3缓存，相当于让内存提前做功课，把数据提前取出来，在3个缓存中候着，等寄存器有空了就取来用。笨鸟先飞嘛。</p></li><li><p>L1/L2/L3缓存，每层速度递减、容量递增。L1缓存速度接近寄存器速度，大约1ns时延。</p></li><li><p>多核CPU的L3对诶个core是共享的，L2和L1是每个core私有的。</p></li></ol><figure><img src="./img_2.png" alt="img_2.png" /><figcaption aria-hidden="true">img_2.png</figcaption></figure><ol start="6" type="1"><li>CPU读取数据时，要从内存读取到L3，再读取到L2再读取到L1，同样写到内存时也会经过这些层次。</li></ol><h2 id="参考">参考</h2><ol type="1"><li>https://www.junmajinlong.com/os/cpu_cache/</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概览&quot;&gt;概览&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;计算机上的存储单元的处理速度从快到慢依次是： 寄存器&amp;gt; L1&amp;gt;L2&amp;gt;L3&amp;gt;内存&amp;gt;固态硬盘&amp;gt; 机械硬盘&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;./img_1</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="计算机原理" scheme="https://chiechie.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="内存" scheme="https://chiechie.github.io/tags/%E5%86%85%E5%AD%98/"/>
    
    <category term="缓存" scheme="https://chiechie.github.io/tags/%E7%BC%93%E5%AD%98/"/>
    
    <category term="寄存器" scheme="https://chiechie.github.io/tags/%E5%AF%84%E5%AD%98%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>一些聚类算法</title>
    <link href="https://chiechie.github.io/2021/06/23/AI/machine_learning/clustering/"/>
    <id>https://chiechie.github.io/2021/06/23/AI/machine_learning/clustering/</id>
    <published>2021-06-23T03:20:42.000Z</published>
    <updated>2021-07-12T11:09:37.181Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>除了kmeans，聚类算法中还有一类更简单直观的方法，就是层次聚类。</p></blockquote><h2 id="总结">总结</h2><ol type="1"><li><p>聚类是一种非监督的数据驱动的分析方法，当给定一堆样本的特征时，希望聚类算法能从特征中发现不同 的样本的聚集的模式，并且可以将发现的知识推广到新的数据上。</p></li><li><p>我们常常对表数据使用kmeans来实现聚类，内部流程分为两步，第一步是是将多维的表数据（维度代表特征），转为2D的相似图（一个节点代表一个样本）。第二步，是对这个图进行聚类。</p></li><li><p>kmeans在实际中应用较多，但是并没有理论保证其收敛。</p><blockquote><p>...I don't think there is a nice theory of convergence or rate of convergence, But is a very popular algorimth,..., But that's one sort of hack that works quite well--Gilbert Strang</p></blockquote></li><li><p>对一个图，找到其中的一些cluster，kmeans是其中一种方法，还有谱聚类（spectral clustring）。</p></li><li><p>除此之外，还可以使用终极大招--使用数值方法去求解优化问题（比如BP，就是alternative methods的一个特例）。</p></li><li><p>按照理论严密从高到低，实现聚类的算法可以分为三类：谱聚类，基于运筹优化的聚类，一些直觉类的方法。直觉类的方法包括kmeans，层次聚类</p></li><li><p>2个基本概念： <span class="math display">\[\operatorname{variability}(c)=\sum_{e \in c} \operatorname{distance}(\operatorname{mean}(c), e)^{2}\]</span></p></li></ol><p><span class="math display">\[\operatorname{dissimilarity}(C)=\sum_{c \in C} \operatorname{variability}(c)\]</span></p><ul><li><p>variability表示一个cluster的紧实程度（肌肉or胖子），有点点像衡量发散程度的方差，</p></li><li><p>variability计算一个cluster内部，每个样本点离中心点的距离（eg欧式距离）</p></li><li><p>dissimilarity表示一系列cluster的紧实程度，除以clusters的个数就是方差。</p></li></ul><ol start="9" type="1"><li>聚类就是求解一个优化问题： 最小化dissimilarity， 同时满足约束：至少有几个cluster（kmeans）or。cluster之间的距离超过给定的阈值（层次聚类）</li><li>层次聚类是确定性的，kmeans是随机的，聚类结果依赖于K个中心点的初始值。</li><li>目前常用的聚类算法大概分为三类：层次聚类/kmeans聚类</li></ol><ul><li>层次聚类： nearest neigbour</li><li>基于分区的聚类（partitional）：Kmeans系列/FCM/图理论</li><li>基于密度：DBSCAN</li><li>其他：ACODF</li></ul><figure><img src="./img.png" alt="img.png" /><figcaption aria-hidden="true">img.png</figcaption></figure><h2 id="kmeans">kmeans</h2><p>怎么选择k？</p><ol type="1"><li>使用不同的k，看聚类效果，需要借助label， 看每个cluster的impurity，值越少，越符合预期。</li><li>在一小数据集上使用层次聚类，然后在全量数据上使用kmeans。</li></ol><p>怎么避免初始中心点的随机性造成的算法效果不稳定？</p><ol type="1"><li>设定初始聚类中心的时候，尽可能散布在整个空间。</li><li>多跑几组聚类，选择聚类结果的dissimilarity最小的实验</li></ol><p>注意样本的特征范围需要统一，否则某一个特征就决定距离</p><h2 id="层次聚类">层次聚类</h2><p>层次聚类算法,又称为树聚类算法,它使用数据的联接规则,透过一种层次架构方式,反复将数据进行分裂或聚合,以形成一个层次序列的聚类问题解.</p><ol type="1"><li><p>层次聚类算法中的层次聚合算法为例进行介绍.</p></li><li><p>如何判断两个cluster之间的距离？average-linkage/single-linkage/complete-linkage</p><ul><li>average-linkage计算两个cluster各自数据点的两两距离的平均值。</li><li>single-linkage,选择两个cluster中距离最短的一对数据点的距离作为类的距离</li><li>complete-linkage选择两个cluster中距离最长的一对数据点的距离作为类的距离。</li></ul></li><li><p>层次聚合算法的时间复杂度为<span class="math inline">\(O(n^2)\)</span>,适合于小数据集的分类.</p></li></ol><p>该算法由树状结构的底部开始逐层向上进行聚合,假定样本集S={o1,o2,...,on}共有n个样本.</p><ol type="1"><li>初始化：每个样本 <span class="math inline">\(o_i\)</span> 为一个类; 共形成 n 个类:<span class="math inline">\(o_1,o_2,...,o_n\)</span></li><li>找最近的<strong>两个类</strong>：从现有所有类中找出相似度最大的<strong>两个</strong>类<span class="math inline">\(o_r\)</span> 和 <span class="math inline">\(o_k\)</span> <span class="math display">\[distance(o_r,o_k) = min_{\forall{o_u,o_v \in S,o_u \neq o_v}}distance(o_u,o_v)\]</span></li><li>将类<span class="math inline">\(o_r\)</span>和<span class="math inline">\(o_k\)</span>合并成一个新类<span class="math inline">\(o_{rk}\)</span>，现有cluster个数减1</li><li>若所有的样本都属于同一个类,则终止;否则,返回步骤2.</li></ol><p>层次聚类最大的优点，它一次性地得到了整个聚类的过程。如果想要改变cluster个数，不需要重新训练聚类模型，取聚类树的中间某一层就好了。相比之下kmeans是要重新弄训练的。</p><p>层次聚类的缺点是计算量比较大，因为要每次都要计算多个cluster内所有数据点的两两距离，复杂度是O(n^2*d)，d表示聚类树的深度。</p><p>还有一个缺点，超参数比较难设置，跟kmeans一样。最终到底聚成多少类，需人工给定一个distance threshold，这个阈值跟sample有关，跟距离函数的定义也有关，并且聚类结果对这个参数比较敏感。</p><h2 id="谱聚类">谱聚类</h2><ol type="1"><li>基于图数据的聚是非监督算法中的一种，目的是对一个图的点和变去聚类，而不是在特征空间中聚类。典型应用就是web图或者社交网络进行数据挖掘。</li></ol><blockquote><p><strong>Graph</strong>-<strong>based clustering</strong> comprises a family of unsupervised classification algorithms that are designed to <strong>cluster</strong> the vertices and edges of a <strong>graph</strong> instead of objects in a feature space. A typical application field of these methods is the Data Mining of online social networks or the Web <strong>graph</strong> [1].Feb 8, 2020</p></blockquote><ol start="2" type="1"><li><p>矩阵的spectrum就是矩阵的特征根。Spectral clustering就是使用矩阵的特征根聚类。</p></li><li><p>图的拉普拉斯矩阵定义为 <span class="math inline">\(L=D-A\)</span>.</p></li></ol><p>怎么根据L来找到目标的clusters（or centriod）？ 对L进行谱分解（特征根分解），并且找到L的fieder向量。</p><p>fieder向量中的元素有正的，也有负的， 正的位置对应的样本分配到cluster1，负元素对应的样本分配到cluster2</p><p>谱聚类的流程是：</p><p>输入：n个样本, 类别k</p><ol type="1"><li>根据样本两两之间的相似度，构建有权无向图，以及邻接矩阵W。</li><li>计算出拉普拉斯矩阵L，对L做谱分解（相当于降维）：计算拉普拉斯矩阵L的最小的k个特征值对应的特征向量u1, u2,...,uk,将这些向量组成n*k维的矩阵U</li><li>将U中的每一行作为一个样本，共n个样本，使用k-means对这n个样本进行聚类</li></ol><p>得到簇划分C(c1,c2,...ck).</p><p>可以将谱聚类应用到，对样本的相似图做聚类。</p><h2 id="聚类应用-图像分割">聚类应用-图像分割</h2><p>图像分割是图聚类的其中一个应用场景。</p><p>层次聚类可看成框架，而基于图的图像分割是在层次聚类上加了骨头，使他更适用于图像分割的领域，而使用者可以在骨头上继续加肉来达到不同的分割效果。</p><p>图像中的每一个像素点是一个item，图像分割的任务即对所有items聚类，属于一个cluster的所有像素就构成了一个区域， 最终的分割结构就是若干个区域（即clusters）组成的。由此我们过渡到本文的第二部分，</p><p>基于图的图像分割算法，主要流程如下</p><p>输入一个图<span class="math inline">\(G=(V,E)\)</span>，有n个点和m个边。输出是一个分割V，分割成<span class="math inline">\(S=(C_1,...,C_2).\)</span></p><ol type="1"><li><p>对边E进行排序，生成非递减的序列<span class="math inline">\(\pi = (o_1,...,o_m)\)</span></p></li><li><p>从初始分割<span class="math inline">\(S^0\)</span>开始，每一个点<span class="math inline">\(v_i\)</span>自己就是一个区域</p></li><li><p>对于每一个边<span class="math inline">\(q = 1,...,m\)</span>重复步骤3，通过<span class="math inline">\(S^{q-1}\)</span>构建<span class="math inline">\(S^q\)</span>，使用如下的方式</p><ul><li>令<span class="math inline">\(v_i\)</span>和<span class="math inline">\(v_j\)</span>表示按顺序排列的第q条边的两个点，比如<span class="math inline">\(o_q = (v_i,v_j)\)</span>。</li><li>如果<span class="math inline">\(v_i\)</span>和<span class="math inline">\(v_j\)</span>在<span class="math inline">\(S^{q-1}\)</span>中连个不同的区域下，并且<span class="math inline">\(w(o_q)\)</span>比两个区域的内部差异都小，那么合并这连个区域，否则什么也不做。</li><li>用公式来表达就是：<ul><li>令<span class="math inline">\(C_{i}^{q-1}是S^{q-1}\)</span>的一个区域，它包含点<span class="math inline">\(v_i\)</span>；令<span class="math inline">\(C_{j}^{q-1}是S^{q-1}\)</span>的一个区域，它包含点<span class="math inline">\(v_j\)</span>。</li><li>如果<span class="math inline">\(C_{i}^{q-1} \neq C_{j}^{q-1}\)</span> 并且<span class="math inline">\(w(o_q) \leq MInt(C_i^{q-1},C_j^{q-1})\)</span>，那么通过合并<span class="math inline">\(C_{i}^{q-1}\)</span>和<span class="math inline">\(C_{j}^{q-1}\)</span>得到了<span class="math inline">\(S^q\)</span>；</li><li>否则的话<span class="math inline">\(S^q = S^{q-1}\)</span>，返回<span class="math inline">\(S = S^m\)</span></li></ul></li></ul></li></ol><p>图像分割结果如下</p><figure><img src="./img1.png" alt="图像分割" /><figcaption aria-hidden="true">图像分割</figcaption></figure><h2 id="参考">参考</h2><ol type="1"><li><a href="https://patentimages.storage.googleapis.com/34/c0/df/3417293b1602a5/CN105468677A.pdf">一种基于图结构的日志聚类算法</a></li><li><a href="https://zhuanlan.zhihu.com/p/78382376">常用聚类算法综述</a></li><li><a href="https://buptjz.github.io/2014/04/21/cluster">从层次聚类到Graph-based图像分割</a></li><li><a href="http://www.jos.org.cn/1000-9825/19/48.pdf">聚类算法</a></li><li><a href="https://www.youtube.com/watch?v=cxTmmasBiC8">35. Finding Clusters in Graphs-mit-youtube</a></li><li>https://www.youtube.com/watch?v=esmzYhuFnds</li><li>https://en.wikipedia.org/wiki/Spectral_clustering</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;除了kmeans，聚类算法中还有一类更简单直观的方法，就是层次聚类。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;聚类是一种非监督的数据驱动的分析方法，当给定一堆样本的特征时</summary>
      
    
    
    
    <category term="AI" scheme="https://chiechie.github.io/categories/AI/"/>
    
    
    <category term="图数据" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"/>
    
    <category term="聚类" scheme="https://chiechie.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
    <category term="日志聚类" scheme="https://chiechie.github.io/tags/%E6%97%A5%E5%BF%97%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>图数据基础</title>
    <link href="https://chiechie.github.io/2021/06/22/data_structure/graph/graph-basic/"/>
    <id>https://chiechie.github.io/2021/06/22/data_structure/graph/graph-basic/</id>
    <published>2021-06-22T03:42:17.000Z</published>
    <updated>2021-07-02T10:29:07.111Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总结">总结</h2><ol type="1"><li>图不仅仅通过边存储静态的信息，还能基于边，inference更多的信息。</li></ol><h2 id="真实世界中的网络">真实世界中的网络</h2><ol type="1"><li>计算机网络：互联网</li><li>交通网络（铁路网，公路网）</li><li>金融网络</li><li>下水道网络</li><li>政治网络</li><li>犯罪网络</li></ol><h2 id="图的定义">图的定义</h2><p>传统的空间都是定义在欧几里得空间（Euclidean Space）的，在该空间定义的距离被称为欧式距离。 音频，图像，视频等都是定义在欧式空间下的欧几里得结构化数据，然而对于社交网络等数据，使用欧几里得空间进行定义并不合适，所以将这一类的数据所处的空间称为非欧几里得空间。</p><p>非欧空间下最有代表的结构就是图（Graph）结构， 每一个图都有对应的4个矩阵：incidence matrix,A/degree matrix/adjacency matrix/拉普拉斯矩阵（laplacian matrix， L）</p><p>incidence matrix:<span class="math inline">\(A \ in R^{m *n}\)</span>, m 是边的个数，n是节点的个数 每一行代表一条边的起点（也叫parent）和终点（也叫children）</p><figure><img src="img_2.png" alt="img_2.png" /><figcaption aria-hidden="true">img_2.png</figcaption></figure><h3 id="邻接矩阵adjacency-matrix">邻接矩阵（Adjacency matrix）</h3><p>邻接矩阵是一个NxN的矩阵，对于有权图，其值为权重或0，对于无权图，其值为0和1，该矩阵定义如下： <span class="math display">\[A \in R^{N \times N}, A_{i j}=\left\{\begin{array}{ll}a_{i j} \neq 0 &amp; e_{i j} \in E \\ 0 &amp; \text { othersize }\end{array}\right.\]</span></p><h3 id="度矩阵degree-matrix">度矩阵（Degree matrix）</h3><p>度矩阵D是一个对角矩阵，其定义为：</p><p><span class="math display">\[D \in R^{N \times N}, D_{i i}=\sum_{j} A_{i j}\]</span></p><h3 id="邻域neighborhood">邻域（Neighborhood）</h3><p>邻域表示与某个顶点有连接的点集，其定义为：<span class="math display">\[N\left(v_{i}\right)=\left\{v_{j} \mid e_{i j} \in E\right\}\]</span></p><h3 id="谱spectral">谱（Spectral）</h3><h3 id="拉普拉斯矩阵lapalcian-matrix">拉普拉斯矩阵（Lapalcian matrix）</h3><h4 id="谱分解spectral-factorization">谱分解(Spectral Factorization)</h4><p>什么是spectrum？ 矩阵的spectrum就是矩阵的特征根。 只有方阵才有谱概念，方阵作为线性算子，其所有特征值的集合称为方阵的谱。方阵的谱半径为其最大的特征值，谱分解就是特征分解。</p><p>谱分解(Spectral Factorization)又叫特征值分解，实际上就是对n维方阵做特征分解. 只有含有n个线性无关的特征向量的n维方阵才可以进行特征分解.</p><blockquote><p>spectral定理用公式表达：对于一个对称矩阵S，其特征根<span class="math inline">\(\Lambda\)</span>都是实数, 特征向量都正交<span class="math inline">\(Q\)</span></p><blockquote><p><span class="math display">\[S = Q\LambdaQ^T\]</span></p></blockquote></blockquote><p>graph laplacian 矩阵：connection of 线性代数和图论</p><p>拉普拉斯矩阵（laplacian matrix），<span class="math inline">\(L \ in R^{n *n}\)</span> <span class="math display">\[L = A^T A= D-B\]</span></p><p>degree matrix：<span class="math inline">\(D \ in R^{n*n}\)</span>，代表了每个节点有多少degree adjaceny matrix：<span class="math inline">\(B \ in R^{n*n}\)</span>，代表了任意两个点是否有link</p><p>拉普拉斯矩阵是半正定矩阵,其特征根从小到达排序 1. 第一个特征根为0，<span class="math inline">\(\lambda_1 = 0\)</span>, 即DIM(null space) = 1，对应的特种向量为常熟向量 2. 第二个特征根（最小的正的特征根）叫fiedler， 对应的特征向量叫fiedler vector</p><p>graph的laplacian矩阵和laplace方程（有限差分方法）的关系：</p><p>laplace方程也叫微分方程，其微分形式 跟一个网格图的laplacian矩阵（n*n）就是离散形式。</p><ol start="3" type="1"><li>实对称矩阵,有n个线性无关的特征向量;</li><li>其特征向量可以进行正交单位化;</li><li>所有的特征值非负;</li></ol><p><span class="math display">\[L=U\left(\begin{array}{ccc}\lambda_{1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_{n}\end{array}\right) U^{-1}=U\left(\begin{array}{ccc}\lambda_{1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_{n}\end{array}\right) U^{T}\]</span></p><p>有时，使用的都是正规拉普拉斯矩阵（Symmetric Normalized Laplacian matrix）: <span class="math display">\[L^{s y s}=D^{-1 / 2} L D^{-1 / 2}=I-D^{-1 / 2} A D^{-1 / 2}\]</span></p><h2 id="图的算法">图的算法</h2><p>基于这个网络，可以做一些什么分析呢？聚类分析，</p><p>即，找到一些cluster，内部的距离很小，之间的距离很大，这个可以抽象成求最大割或者最小流问题 通常还有一些其他的基于图的问题：</p><p>比如，给定一个图，任意两个元素之间是否存在一个link？如果存在，最快捷的路径是什么？</p><p>还有一个经典的问题--'图分割'（graph partition）</p><h3 id="s-t-cut">S-T Cut</h3><p>S-T Cut 就是把一个图切成了两个子图，S和T</p><figure><img src="img.png" alt="img.png" /><figcaption aria-hidden="true">img.png</figcaption></figure><p>S-T cut的容量，就是链接S,C的变（图中红色的边）的权重求和，</p><figure><img src="img_1.png" alt="img_1.png" /><figcaption aria-hidden="true">img_1.png</figcaption></figure><h3 id="最小割">最小割</h3><p>最小割（min -cut），就是容量最小的那个S-T cut</p><figure><img src="img_2.png" alt="img_2.png" /><figcaption aria-hidden="true">img_2.png</figcaption></figure><p>最小割是指去掉图中的一些边，在使得图从连通图变为不连通图，并且尽可能保证去除的边对应的权重很小。</p><p>最小割可能并不唯一</p><p>对于相似性图来说，最小割就是要去除一些很弱的相似性，把数据点从一个连通的大图切割成多个独立的连通小图。</p><h2 id="参考">参考</h2><ol type="1"><li>https://zhuanlan.zhihu.com/p/84271169</li><li><a href="https://www.youtube.com/watch?v=V_TulH374hw">Graph-theoretic Models</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;图不仅仅通过边存储静态的信息，还能基于边，inference更多的信息。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;真实世界中的网络&quot;&gt;真实世界中的网络&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;计算机网</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="图算法" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E7%AE%97%E6%B3%95/"/>
    
    <category term="图" scheme="https://chiechie.github.io/tags/%E5%9B%BE/"/>
    
    <category term="拓扑数据" scheme="https://chiechie.github.io/tags/%E6%8B%93%E6%89%91%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>细说红楼梦</title>
    <link href="https://chiechie.github.io/2021/06/21/reading_notes/zhexue/hongloumengxishuo/"/>
    <id>https://chiechie.github.io/2021/06/21/reading_notes/zhexue/hongloumengxishuo/</id>
    <published>2021-06-21T02:02:37.000Z</published>
    <updated>2021-06-22T06:21:25.475Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一回">第一回</h2><p>空空道人有批注：“因空见色，由色生情，传情入色，自色悟空，遂易名为情僧……”</p><p>“因空见色”，是说本来就白茫茫一片，什么都没有，（六祖慧能：本来无一物），因为我们的幻觉，看到好多好多的现象，“由色生情”，情多了就会陷进去，陷进了更深的色的幻觉，“传情入色”，要经过多少彻悟之后，再从里面出来，“自色悟空”，再回到白茫茫一片真干净。</p><p>甄士隐经了这许多人生起伏，有一天突然又看见跛足道士来了，口里唱一首《好了歌》：</p><blockquote><p>世人都晓神仙好，惟有功名忘不了！古今将相在何方？荒冢一堆草没了。世人都晓神仙好，只有金银忘不了！终朝只恨聚无多，及到多时眼闭了。世人都晓神仙好，只有姣妻忘不了！君生日日说恩情，君死又随人去了。世人都晓神仙好，只有儿孙忘不了！痴心父母古来多，孝顺儿孙谁见了？</p></blockquote><p>甄士隐说：“你唱什么？我只听见‘好’‘了’、‘好’‘了’两个字。道人说：</p><blockquote><p>好便是了，了便是好。若不了，便不好；若要好，须是了。</p></blockquote><p>跛足道人讲的是道家的哲学，对儒家社会秩序，有很大的颠覆性。儒家修身齐家治国平天下这一套道理，要建立的是稳定的社会秩序（social order），鼓励人入世，求功名、利禄、妻子、儿女，儒家宗法社会下面，大概就是这些。</p><p>跛足道人给了很大的一个警告，好就是了，了就是好，等于给恋恋在红尘中的人临头棒喝、醍醐灌顶。</p><p>人大概都经过几个阶段：年轻的时候，大家都是入世哲学，儒家那一套，要求功名利禄。到了中年，大概受了些挫折，于是道家来了，点你一下，有所醒悟。到了最后，要超脱人生境界的时候，佛家就来了。</p><p>所以过去的中国人，从儒道释，大致都经过这么三个阶段，有意思的是这三个阶段不冲突。在同一个人身上，这三样哲学都有。所以中国人既出世又入世的态度，常常造成整个文化的一种紧张，也就是说，我们的人生态度在这之间常常有一种徘徊迟疑，我想，这就是文学的起因。</p><p>文学写什么呢？写一个人求道提升，讲他的目标求道，讲这一生多么地艰难，往往很多人没有求到，在半路已经失败。不管是爱情也好，理想也好，各种的失败，我想，文学写的就是这些。《红楼梦》写的也是这些。</p><p>甄士隐对《好了歌》的注解：</p><blockquote><p>陋室空堂，当年笏满床；衰草枯杨，曾为歌舞场。蛛丝儿结满雕梁，绿纱今又糊在蓬窗上。说什么脂正浓、粉正香，如何两鬓又成霜？昨日黄土陇头送白骨，今宵红灯帐底卧鸳鸯。金满箱，银满箱，展眼乞丐人皆谤。正叹他人命不长，那知自己归来丧！训有方，保不定日后作强梁。择膏粱，谁承望流落在烟花巷！因嫌纱帽小，致使锁枷扛；昨怜破袄寒，今嫌紫蟒长：乱烘烘你方唱罢我登场，反认他乡是故乡。甚荒唐，到头来都是为他人作嫁衣裳！</p></blockquote><p>人生不就是个大舞台，一个人唱完下来，第二个人上去唱，唱完又一鞠躬下台，又换个人上去唱，“乱烘烘你方唱罢我登场”，然后“反认他乡是故乡”。</p><p>佛家说，我们以为这是我们自己的故乡，其实也靠不住，一下子一把火就整个烧掉了。道家说，要醒悟这一点，才找到你真正理想的地方。甚荒唐！到头来都是为他人做嫁衣裳。讲起来，整个一生是白忙一场，都为他人做嫁衣裳，所有事情都是为他人做的。</p><p>甄士隐、贾雨村，一个代表出世，一个代表入世。后来甄士隐变成道士，贾雨村经过好多官宦历程的折腾，到了书结束的时候，这两人又碰到一起。甄士隐想要度化贾雨村，贾雨村还是迷恋红尘。各走各的路，两个分歧，自己去看，自己去选择。</p><h2 id="第二章">第二章</h2><p>贾政，自觉是遵从儒家理想的一个人。他非常正直，也想用儒家那一套思想道德来持家，但是太过守法，太过拘束了。中国社会能够生存下来，光靠儒家思想、书生之见是不够的，还需要别种哲学，譬如很要紧的法家，很实在的、很现实的顶在后边。儒家的很多理想不一定都能实现，碰到了现实问题，常常不能解决。不过儒家也很重要，它是一种道德力量（moral force），没有它也不行，但光是有它也不行，所以，还要配合别的东西。</p><p>贾雨村听冷子兴讲了半天贾宝玉这个怪人，也发表了一篇言论：</p><p>“天地生人，除大仁大恶两种，馀者皆无大异。若大仁者，则应运而生，大恶者，则应劫而生。运生世治，劫生世危。尧、舜、禹、汤、文、武、周、召、孔、孟、董、韩、周、程、张、朱，皆应运而生者。蚩尤、共工、桀、纣、始皇、王莽、曹操、桓温、安禄山、秦桧等，皆应劫而生者。大仁者，修治天下；大恶者，挠乱天下。清明灵秀，天地之正气，仁者之所秉也；残忍乖僻，天地之邪气，恶者之所秉也。今当运隆祚永之朝，太平无为之世，清明灵秀之气所秉者，上至朝廷，下及草野，比比皆是。所馀之秀气，漫无所归，遂为甘露、为和风，洽然溉及四海。彼残忍乖僻之邪气，不能荡溢于光天化日之中，遂凝结充塞于深沟大壑之内，偶因风荡，或被云摧，略有摇动感发之意，一丝半缕误而泄出者，偶值灵秀之气适过，正不容邪，邪复妒正，两不相下，亦如风水雷电，地中既遇，既不能消，又不能让，必至搏击掀发后始尽。故其气亦必赋人，发泄一尽始散。使男女偶秉此气而生者，在上则不能成仁人君子，下亦不能为大凶大恶。置之于万万人中，其聪俊灵秀之气，则在万万人之上；其乖僻邪谬不近人情之态，又在万万人之下。若生于公侯富贵之家，则为情痴情种；若生于诗书清贫之族，则为逸士高人；纵再偶生于薄祚寒门，断不能为走卒健仆，甘遭庸人驱制驾驭，必为奇优名娼。如前代之许由、陶潜、阮籍、嵇康、刘伶、王谢二族、顾虎头、陈后主、唐明皇、宋徽宗、刘庭芝、温飞卿、米南宫、石曼卿、柳耆卿、秦少游，近日之倪云林、唐伯虎、祝枝山，再如李龟年、黄幡绰、敬新磨、卓文君、红拂、薛涛、崔莺、朝云之流，此皆易地则同之人也。”</p><h2 id="第四章">第四章</h2><p>再看看贾雨村这个人，他也是有高度象征性的一个人，象征这个世界上每一个为求功名利禄不择手段往上爬的凡俗之人。</p><p>贾雨村刚刚做官，还不太清楚。葫芦僧就讲，贾、王、史、薛这四大家族是互相牵连、互相庇护的，你一动薛家，其他家族就暗中使力，根本没法去办他们。的确是！后来薛蟠又打死人，贾家去讲讲情也就过了。</p><p>为官为政的这些人，像贾雨村者也是很典型的，一旦发迹，若从前是贫贱出身，他不愿意人家知道他的过去，哪个不长心眼儿的去碰他的过去的话，砍掉！因为他要维持现在的形象，掩掉过去。后来贾家没落的时候，贾雨村果然对有恩的贾家加踹一脚，这正是官场反复无情的写照。</p><h2 id="第五章">第五章</h2><p>无常，我们看起来好像是个悲剧，在佛家看，人所有的一切就是如此。佛教很理性地看待人生，看待一切的事物，没有永远存在的东西，因为时间会破坏一切、会毁灭一切，有时间的转动，春夏秋冬的转动，就会有无常现象。人生无常，什么都无常。所以，“生关死劫谁能躲？闻说道，西方宝树唤婆娑，上结着长生果。”这是惜春的醒悟！佛教的传说，佛陀释迦牟尼圆寂的时候，是在两棵宝树中间，那两棵宝树叫作“娑罗”。佛教对“婆娑”两字另有所解，长生果，佛家讲修成正果，要悟道了以后，从而解脱。最后惜春出家了，她是找到解脱最彻底的一个人物。</p><p>王熙凤，机关算尽太聪明，她最精明，涉入红尘也最深，《好了歌》讲：“世人都晓神仙好，只有金银忘不了！”等到聚多之时缘尽了。凤姐放高利贷，到处攒钱，聚敛多时，一下子抄家抄得精光，一场空欢喜！她在贾府最得宠，掌大权，抄家的原因之一是她放高利贷给抄出来了，一世的颜面丢得精光。“反算了卿卿性命”，后来王熙凤的下场也很悲惨。</p><p>对照一下，惜春走一条路，王熙凤走另外一条路，一个出世，一个入世，两种不同的道路。甄士隐跟贾雨村，也是出世和入世的辩证。曹雪芹从未批判哪方，他只是写出各人选的道路。儒家的入世在《红楼梦》的结局也很重要，贾府衰败，王熙凤死了，也还有接班人，谁呢？薛宝钗，她把贾府重新扛起来，这就是人生。</p><p>中国人的人生，常常有三种哲学的循环。年轻的时候有所求，中年醒悟，晚年一切看开。有这三种哲学，才是完整的中国文化，三者相克相生，互用互补。 《红楼梦》曲最后收尾的曲子：</p><blockquote><p>为官的，家业凋零；富贵的，金银散尽；有恩的，死里逃生；无情的，分明报应。欠命的，命已还；欠泪的，泪已尽。冤冤相报实非轻，分离聚合皆前定。欲知命短问前生，老来富贵也真侥幸。看破的，遁入空门；痴迷的，枉送了性命。好一似食尽鸟投林，落了片白茫茫大地真干净！</p></blockquote><h2 id="第16回">第16回</h2><p>儒家跟道家人生观，宇宙、社会的看法，互相冲突。《红楼梦》的悲剧，并不是一个突发性的意外，而是人生必然的过程，</p><h2 id="第22回">第22回</h2><p>宝玉出的灯谜：南面而坐，北面而朝，“象忧亦忧，象喜亦喜”。 佛家有一句话说镜花水月，一切都是幻象。宝玉看到的一切，由色入空，一切都是幻象。</p><h2 id="参考">参考</h2><ol type="1"><li>https://weread.qq.com/web/reader/3c432fc05d0f283c488450e</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;第一回&quot;&gt;第一回&lt;/h2&gt;
&lt;p&gt;空空道人有批注：“因空见色，由色生情，传情入色，自色悟空，遂易名为情僧……”&lt;/p&gt;
&lt;p&gt;“因空见色”，是说本来就白茫茫一片，什么都没有，（六祖慧能：本来无一物），因为我们的幻觉，看到好多好多的现象，“由色生情”，情多了就会陷进</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="道家" scheme="https://chiechie.github.io/tags/%E9%81%93%E5%AE%B6/"/>
    
    <category term="佛家" scheme="https://chiechie.github.io/tags/%E4%BD%9B%E5%AE%B6/"/>
    
    <category term="人生哲学" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E7%94%9F%E5%93%B2%E5%AD%A6/"/>
    
    <category term="儒家" scheme="https://chiechie.github.io/tags/%E5%84%92%E5%AE%B6/"/>
    
  </entry>
  
  <entry>
    <title>概率图简介</title>
    <link href="https://chiechie.github.io/2021/06/19/data_structure/graph/gailvtu/"/>
    <id>https://chiechie.github.io/2021/06/19/data_structure/graph/gailvtu/</id>
    <published>2021-06-19T08:52:21.000Z</published>
    <updated>2021-07-12T00:27:44.902Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念">基本概念</h2><p>概率图模型(Probabilistic Graphical Model，PGM)，简称图模型(Graphical Model，GM)，首先是一种概率模型，其次，它用图结构描述多个随机变量之间的依赖关系，它是研究高维空间中的概率模型的一种有用工具。</p><p>概率图模型有三个基本问题: 表示问题，学习问题和推断问题</p><p><img src="./e3036b4e-18d3-4e73-b996-eafe4a4c08d1-3380623.png" /></p><h3 id="表示问题">1. 表示问题</h3><p>表示问题，即对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。</p><h4 id="有向图模型directed-graphical-model">有向图模型(Directed Graphical model)</h4><p>有向图模型(Directed Graphical model)，也称为贝叶斯网络(Bayesian Network)，或信念网络(Belief Network，BN)，是指用有向图来表示概率分布的图模型。</p><figure><img src="./75c9420a-058f-4005-a4b2-4609e7987c1b-3380623.png" alt="贝叶斯网络" /><figcaption aria-hidden="true">贝叶斯网络</figcaption></figure><p>很多机器学习模型可以用有向图模型来描述，比如</p><ul><li>朴素贝叶斯分类器</li><li>隐马尔可夫模型</li><li>深度信念网络(sigmoid 信念网络)</li></ul><h4 id="无向图模型">无向图模型</h4><p>无向图模型也称为马尔可夫随机场(Markov Random Field，MRF)或 马尔可夫网络(Markov Network)，是一类用无向图来描述一组具有局部马尔可夫性质的随机向量 X 的联合概率分布的模型。</p><figure><img src="./85a18510-051c-4339-a808-4d3759095432-3380623.png" alt="无向图模型" /><figcaption aria-hidden="true">无向图模型</figcaption></figure><ul><li><p>如果(G,X)满足局部马尔可夫性质， 即一个变量 Xk 在给定它的邻居的情况下独立于所有其它变量，</p></li><li><p>无向图模型的概率分解</p><ul><li>因子分解 :无向图中的的联合概率可以分解为一系列定义在最大团上的非负函数的乘积形式。</li><li>吉布斯分布 :无向图模型和吉布斯分布是一致的。吉布斯分布一定满足马尔可夫随机场的条件独立性质，并且马尔可夫随机场的概率分布一定可以表示成吉布斯分布。</li></ul></li><li><p>常见的无向图模型:很多经典的机器学习模型可以使用无向图模型来描述，比如</p><ul><li>对数线性模型(Log-Linear Model)或最大熵模型(Maximum Entropy Model)</li><li>条件随机场(Conditional Random Field，CRF)</li><li>玻尔兹曼机</li><li>受限玻尔兹曼机等</li></ul></li><li><p>有向图和无向图之间的转换:</p><ul><li>无向图模型可以表示有向图模型无法表示的一些依赖关系，比如循环依赖;</li><li>无向图模型不能表示有向图模型能够表示的某些关系，比如因果关系。 ### 2. 推断问题</li></ul></li></ul><ol type="1"><li><p>推断问题，即求条件概率分布，根据贝叶斯公式可以转化为求两个边际概率分布的商</p></li><li><p>如果我们能求一个更一般的问题：任意变量的边际概率分布问题，那么上面推断问题就迎刃而解了。</p></li><li><p>求变量的边际概率分布的方法大致分为精确推断和近似推断两类。</p></li><li><p>精确推断的方法包括变量消除法（Variable Elimination Algorithm）和信念传播（Belief Propagation）方法</p></li><li><p>图模型中有些变量的局部条件分布可能非常复杂，或其积分无法计算。需要使用数值方法来近似，比如变分法和采样法.</p></li></ol><h4 id="变量消除法">变量消除法</h4><ol type="1"><li>变量消除法即将边际概率分布转换为更基础的条件概率以及概率求和的形式，然后从内到外逐次求和</li><li>变量消除法是动态规划的思想，随着图模型 规模的增长，变量消除法的收益越大。</li><li>变量消除法的一个缺点是在计算多个边际分布时存在很多重复的计算。比 如在上面的图模型中，计算边际概率 p(x4 ) 和 p(x3 ) 时很多局部的求和计算是一 样的。</li></ol><figure><img src="./image-20210710162559549.png" alt="image-20210710162559549" /><figcaption aria-hidden="true">image-20210710162559549</figcaption></figure><h4 id="信念传播">信念传播</h4><p>信念传播算法，也称为和积(Sum-Product)算法 或消息传递(Message Passing)算法，是将变量消除法中的和积(Sum-Product) 操作看作是消息，并保存起来，这样可以节省大量的计算资源。</p><p>链式结构上的的信念传播算法</p><p><img src="https://img.mubu.com/document_image/bd0b3e40-76a5-42e2-8716-799247eddc32-3380623.jpg" /></p><p>树结构上的信念传播算法：</p><ul><li>从叶子节点到根节点依次计算并传递 消息;</li><li>从根节点开始到叶子节点，依次计算并传递消息;</li><li>在每个节点上 计算所有接收消息的乘积(如果是无向图还需要归一化)，就得到了所有变量的 边际概率。</li></ul><h4 id="变分法">变分法</h4><ol type="1"><li><p>变分法(Variational Method)是引入一个变分分布(通常是比较简单的分布，比如高斯分布)来近似复杂的局部条件概率，然后通过迭代的方法计算后验分布。</p></li><li><p>变分法首先更新变分分布的参数,来最小化变分分布和真实分布的差异(比如交叉熵或 KL 距离)，然后根据变分分布来进行推断。</p></li></ol><h4 id="采样法">采样法</h4><ol type="1"><li><p>采样法(Sampling Method)也叫蒙特卡罗方法(Monte Carlo Method)或 统计模拟方法，是 20 世纪 40 年代中期提出的一种通过随机采样的方法来近似估计一些计算问题的数值解。</p></li><li><p>采样法通过模拟的方式来采集符合某个分布 p(<strong>x</strong>) 的一些样本，并通过这些样本来估计和这个分布有关的运算，比如期望等。</p></li><li><p>如果p比较简单，可以直接使用直接采样方法。如果p比较复杂或者不知道其精确值，就要采用间接的采样策略，如拒绝采样、重要性采样、马尔可夫链蒙特卡罗 采样等。这些方法一般是先根据一个比较容易采样的分布进行采样，然后通过一 些策略来间接得到符合 p(x) 分布的样本。</p></li><li><p>拒绝采样(Rejection Sampling):也叫接受-拒绝采样(Acceptance-RejectionSampling)：假设原始分布 p(x) 难以直接采样，可引入一个容易采样的分布 q(x)， 一般称为提议分布(Proposal Distribution)，然后以某个标准来拒绝一部分的样本使得最终采集的样本服从分布p(x)。</p></li></ol><p><img src="./f6ae17af-597a-4445-abbf-2c2b0a5bce67-3380623.png" /></p><ol start="4" type="1"><li><p>重要性采样(Importance Sampling):通过引入重要性权重，将分布p(x)下f(x)的期望变为在分布q(x)下f(x)w(x)的期望.如果采样的目的是计算分布 p(x)下函数f(x)的期望，那么实际上抽取的样本不需要严格服从分布p(x)。也可以通过另一个分布，即提议分布q(x)，直接采样并估计。</p></li><li><p>马尔可夫链蒙特卡罗(Markov Chain Monte Carlo，MCMC): 在高维空间中拒绝采样和重要性采样效率低，</p></li></ol><ul><li>​ Metropolis-Hastings算法，以及两个特例Metropolis算法和吉布斯采样(Gibbs Sampling)<ul><li>Metropolis算法，假设MH算法中的提议分布是对称的</li><li>吉布斯采样(Gibbs Sampling)，用全条件概率(Full Conditional Probability)作为提议分布来依次对每个维度 进行采样，并设置接受率为A = 1。</li></ul></li></ul><blockquote><p>蒙特卡罗的基本思想可以归结为，根据一个已知概率密度函数为 p(x) 的 分布来计算函数 f (x) 的期望</p></blockquote><h3 id="学习问题">3. 学习问题</h3><p>概率图模型的学习包括：图结构的学习和参数的学习</p><p>参数的学习，即参数估计问题，可分为含隐变量的参数估计和不含因变量的参数估计：</p><ul><li><p>不含隐变量的参数估计：</p><ul><li>如果图模型中不包含隐变量，即所有变量都是可观测的，那么网络参数一般可以直接通过最大似然来进行估计。</li><li>有向图模型：所有变量x的联合概率分布可以分解为每个随机变量<span class="math inline">\(x_k\)</span>的局部条件概率 <span class="math inline">\(p(xk |xπk , \theta_k)\)</span> 的连乘形式，其中<span class="math inline">\(\theta_k\)</span>为第 k 个变量的局部 条件概率的参数。</li><li>无向图模型： 所有变量x的联合概率分布可以分解为定义在最 大团上的势能函数的连乘形式。</li></ul></li><li><p>含隐变量的参数估计：</p><ul><li>如果图模型中包含隐变量，即有部分变量是不可观测的，就需要用 EM 算法</li><li>EM 算法的应用例子:高斯混合模型。高斯混合模型(Gaussian Mixture Model，GMM)是由多个高斯分布组成的模型，其密度函数为多个高 斯密度函数的加权组合。 <img src="./3476923c-5528-4dd4-aa4d-ae10183bf377-3380623.png" /> <img src="https://img.mubu.com/document_image/28d53640-0eb6-4a17-81c6-ffa8e4dbb289-3380623.jpg" /></li></ul></li></ul><h2 id="概率图模型与机器学习">概率图模型与机器学习</h2><ul><li>很多机器学习模型都可以归结为概率模型，即建模输入和输 出之间的条件概率分布。</li><li>图模型提供了一种新的角度来解释机器学习模 型，并且这种角度有很多优点，比如了解不同机器学习模型之间的联系，方便设计新模型等。</li><li>概率图模型中最基本的假设是条件独立性。图形化表示直观地描述了随机 变量之间的条件独立性，有利于将复杂的概率模型分解为简单模型的组合，并 更好地理解概率模型的表示、推断、学习等方法。</li></ul><h2 id="概率图模型与神经网络">概率图模型与神经网络</h2><p>概率图模型和神经网络有着类似的网络结构，但两者不同。</p><ul><li>节点<ul><li>概率图模型的节点是随机变量，其图结构的主要功能是用来描述变量 之间的依赖关系，一般是稀疏连接。使用图模型的好处是可以有效进行统计推 断。</li><li>神经网络中的节点是神经元，是一个计算节点。如果将神经网络中每个神经元看做是一个binary随机变量，那神经网络就变成一个sigmoid信念网络。</li></ul></li><li>变量的含义<ul><li>图模型中的每个变量一般有着明确的解释，<strong>变量之间依赖关系一般是人工来定义</strong>。</li><li>神经网络中的单个神经元则没有直观的解释</li></ul></li><li>生成模型与判别模型<ul><li>神经网络是判别模型，直接用来分类</li><li>图模型不但可以是判别模型，也可以是生成模型。生成模型不但可以用来生成样本，也可以通过贝叶斯公式用来做分类。</li></ul></li><li>学习方法<ul><li>神经网络参数学习的目标为交叉熵或平方误差等损失函数。</li><li>图模型的参数学习的目标函数为似然函数或条件似然函数，若包含隐变量则通常通过EM算法来求解。</li></ul></li></ul><p>神经网络和概率图模型的结合：</p><ul><li><p>用神经网络强大的表示能力来建模图模型中的</p><ul><li>推断问题：比如变分自编码器</li><li>生成问题：比如生成对抗网络</li><li>势能函数：比如 LSTM+CRF模型[Lample et al., 2016, Ma and Hovy, 2016]</li></ul></li><li><p>用图模型的算法来解决复杂结构神经网络中的学习和推断问题</p><ul><li>图结构神经网络(Graph Neural Network)</li></ul></li></ul><h2 id="参考">参考</h2><ol type="1"><li>《神经网络与深度学习》</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本概念&quot;&gt;基本概念&lt;/h2&gt;
&lt;p&gt;概率图模型(Probabilistic Graphical Model，PGM)，简称图模型(Graphical Model，GM)，首先是一种概率模型，其次，它用图结构描述多个随机变量之间的依赖关系，它是研究高维空间中的概率</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="贝叶斯网络" scheme="https://chiechie.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
    <category term="概率图" scheme="https://chiechie.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE/"/>
    
    <category term="贝叶斯优化" scheme="https://chiechie.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
</feed>
