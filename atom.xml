<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chiechie&#39;s Mini World</title>
  
  <subtitle>Set your course by the stars, not by the lights of passing ships. —— Omar Bradley</subtitle>
  <link href="https://chiechie.github.io/atom.xml" rel="self"/>
  
  <link href="https://chiechie.github.io/"/>
  <updated>2021-06-30T07:30:46.231Z</updated>
  <id>https://chiechie.github.io/</id>
  
  <author>
    <name>Chiechie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于Ray的融合计算框架</title>
    <link href="https://chiechie.github.io/2021/06/30/reading_notes/computer/fenbushi/"/>
    <id>https://chiechie.github.io/2021/06/30/reading_notes/computer/fenbushi/</id>
    <published>2021-06-30T06:39:39.000Z</published>
    <updated>2021-06-30T07:30:46.231Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>看到阿里在2021发表的一个视频，《新一代的计算基础设施-对于Ray的融合计算》，做一些笔记</p></blockquote><ol type="1"><li>举一个复杂的业务的例子，用户支付流程，：</li></ol><ul><li>先选择支付场景--线上/线下/，</li><li>选择支付方式--</li><li>选择交易网络--银联/第三方</li><li>选择银行</li></ul><p><img src="./img_2.png" alt="img_2.png" /> 整个流程需要秒级响应。</p><ol start="2" type="1"><li>要是大家都开始用统一的计算平台，AIOps也可以用一套了</li></ol><figure><img src="./img_1.png" alt="img_1.png" /><figcaption aria-hidden="true">img_1.png</figcaption></figure><ol start="3" type="1"><li>业务需求之一是，复杂的业务逻辑计算存在单机性能瓶颈，需要支持分布式无范式的分布式开发。</li><li>现在的基础设计在单一用途的组件上，已经做的比较成熟了，专门做流式计算的计算引擎有flink，专门做深度学习的计算引擎有tensorflow，专门做批处理的有spark。</li><li>但是，考虑到一个复杂的应用场景，完成1个任务需要用到多种计算模式，统一计算平台 能够让这几种计算模式实现状态共享，中间结果共享。在这种场景下，统一计算平台，相较于多个独立的计算组件 效率更高。</li><li>要不要搞统一计算平台，取决于需求的复杂性，如果是一个很 pure 很纯粹 的任务，搞这个 的意义就不大。但是可以作为预研嘛，万一以后业务变得越来越复杂，也提前有技术储备。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.bilibili.com/video/BV1gh411Y7qf?t=1">Ray Forward Meetup 2021-面向金融决策场景的在线计算系统-bilibili</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;看到阿里在2021发表的一个视频，《新一代的计算基础设施-对于Ray的融合计算》，做一些笔记&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;举一个复杂的业务的例子，用户支付流程，：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="大数据" scheme="https://chiechie.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="读书笔记" scheme="https://chiechie.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机原理" scheme="https://chiechie.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="分布式计算" scheme="https://chiechie.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>零知识量证明</title>
    <link href="https://chiechie.github.io/2021/06/29/zkp/"/>
    <id>https://chiechie.github.io/2021/06/29/zkp/</id>
    <published>2021-06-29T15:54:57.000Z</published>
    <updated>2021-07-01T01:49:35.756Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念">基本概念</h2><ol type="1"><li><p>零知识证明(ZKP)的定义为：证明者（prover）能够在不向验证者（verifier）提供任何有用的信息的情况下，使验证者（verifier）相信某个论断是正确的。</p></li><li><p>零知识证明具有三个重要的性质：</p><ul><li>完备性（Completeness）：只要证明者拥有相应的知识，那么就能通过验证者的验证，即证明者有足够大的概率使验证者确信。---验证者的问题是可解的。</li><li>可靠性（Soundness）：如果证明者没有相应的知识，则无法通过验证者的验证，即证明者欺骗验证者的概率可以忽略。---验证者的问题是有难度的。</li><li>零知识性（Zero-Knowledge）：证明者在交互过程中仅向验证者透露是否拥有相应知识的陈述，不会泄露任何关于知识的额外信息。---验证者是无不要知道解答过程的，它只在另外一个问题空间验证。</li></ul></li><li><p>使用零知识证明技术的应用有：隐私币--zcash就是的隐私币，以太坊上的混币合约，链下扩容技术zkRollup。</p></li><li><p>零知识证明是一种基于概率的验证方式，验证者（verifier）向证明者（prover)提出多个随机问题，如果证明者都能给出正确回答，则说明证明者大概率拥有他所声称的“知识”。零知识证明并不是数学意义上的证明，因为它存在小概率的误差，欺骗的证明者有可能通过虚假的陈诉骗过验证者。换句话说，零知识证明是概率证明而不是确定性证明，但是也存在技术能将误差降低到可以忽略的值。</p></li></ol><h2 id="应用场景">应用场景</h2><p>零知识证明在区块链上的两大应用场景：隐私和扩容。</p><p>隐私：在隐私场景中，我们可以借助零知识证明的“不泄露信息”特性，在不泄漏交易的细节（接收方，发送方，交易余额）的情况下证明区块链上的资产转移是有效的。</p><p>扩容：在扩容场景中，我们不太需要关注零知识证明技术的“不泄露信息”这个特性，我们的关注重点是它的“证明论断有效”这个特性，由于链上资源是有限的，所以我们需要把大量的计算迁移到链下进行，因此需要有一种技术能够证明这些在链下发生的动作是可信的，零知识证明正好可以帮助我们做链下可信计算的背书。</p><h3 id="隐私场景">隐私场景</h3><h4 id="为什么需要隐私币">为什么需要隐私币？</h4><ol type="1"><li><p>举个生活中的例子：游客向庙里功德箱中仍香火钱，所有的游客仍的都是同一个年份的一元硬币，这时有一个第三方在一旁观察，他可以知道谁在什么时间扔进去多少个硬币。但是当小沙弥从功德箱中取钱的时候，他无法分别取出的硬币是由谁扔进去的。这里功德箱就起到一个混币的功能.同理，为了保证区块链上交易的隐私性，也可以进行混币。</p></li><li><p>混币的目的是切断加密货币交易中发送方与接受方的联系，发送方利用混币系统将自己的钱与其他人的钱进行混合，接受方(Verifier)利用零知识证明来证明有某一个混币的所有权，从而进行转账交易。</p></li><li><p>需要隐私币的第一个理由：隐私币可以能实现完全匿名且不可追踪。</p><ul><li>目前公链（比特币）的匿名只起到假名的作用，例如现实生活中的人可以生成任意多的公私钥对，用这些公钥在链上发送或接受每笔交易，这些公钥就充当他们的假名。如果外界不知道你和公钥的关系，他们就无法把你和你的交易历史关联起来，只要有人能把你跟公钥联系起来，就可以顺藤摸瓜找到你过去的交易历史。目前不没有办法阻止第三方将我们和我们的公钥联系起来。</li></ul></li><li><p>需要隐私币的第二个理由：由于隐私币无法查看货币的交易记录，所以减少了货币不可互换性的问题。</p><ul><li>流通性使货币具有了内在可互换性,但是加密货币具有极度透明性，我们可以追踪到与某一特定货币的所有相关历史交易，这样一来，人们一旦发现某个货币是“污点”货币（俗称“黑钱”）就可以拒绝接受这种货币。如果这种情况大规模发生，加密货币将不再是可互换的因为“干净”的货币比“污点”货币具有更大价值。</li><li>不可互换的货币会给用户带来额外的负担，用户为了避免不小心买入“污点”货币，那么用户就会被迫检查他们购买的每笔货币的交易历史。</li></ul></li><li><p>下图是使用零知识证明的一般过程，在circuit中会执行一些约束，这些约束是与要解决的问题是相关的。Private input是不对外揭露的，只有prover自己知道这个值。public input是prover与Verifier之间共享的一个值。所以上面的过程可以总结为，prover 在不揭露Private input 的情况下向Verifier证明自己知道一个值能满足（x+3=5)。</p><figure><img src="img.png" alt="基于circuit的零知识证明" /><figcaption aria-hidden="true">基于circuit的零知识证明</figcaption></figure></li></ol><h4 id="zk-snark的流程图">zk-SNARK的流程图</h4><p>下图是，zk-SNARK不能直接用于解决任何计算问题，我们必须先把问题转换成正确的“形式”来处理，这种形式叫做 "quadratic arithmetic problem"(QAP)，在进行QAP 转换的同时，我们可以用Private input ，public input创建一个对应的解，称为QAP的witness。只有prover用这个witness来生成proof。</p><figure><img src="img_1.png" alt="zk-Snark流程图" /><figcaption aria-hidden="true">zk-Snark流程图</figcaption></figure><p>整个过程如下：</p><ul><li>首先得有一个计算问题，这个问题一般是NP问题</li><li>然后将计算问题做一个等价转换变成QAP，步骤如下：<ul><li>将计算问题拍平变成circuit</li><li>把circuit转化成 R1CS(rank-1 constraint system，一阶约束系统)。R1CS 是一个由三向量组 (a,b,c) 组成的序列，R1CS 有个解向量 s，s 必须满足符号表示向量的内积运算 a.s * b.s - c.s = 0，这里的解向量s就是witness</li><li>将R1CS转化成QAP形式，这两者的区别是QAP使用多项式来代替点积运算，他们所实现的逻辑完全相同。</li></ul></li><li>接下的是比较重要的一步trusted setup，trusted setup会生成两个值PK，VK，truseted setup的目的是实现零交互验证，它生成的PK，VK相当于是一个“上帝”由它来帮我们做一些挑战，来验证prover。</li></ul><p>prover会用PK已经witness生成一个proof交给Verifier</p><p>Verifier拿到这个proof会用VK做一些校验，这一步发生在链上，有链上的节点或智能合约来做校验。</p><h3 id="扩容">扩容</h3><ol type="1"><li>17年出现了一款非常火爆的Dapp应用叫加密猫，加密猫曾造成以太坊主网大规模的拥堵，造成拥堵的原因是以太坊当时的TPS只有15，这意味着以太坊每秒只能处理15笔交易，如此低的TPS严重限制了区块链应用的大规模落地，所以有人开始研究区块链扩容的问题，目的就是为了提高链上的TPS。</li><li>但区块链扩容受到Vitalik提出的不可能三角的限制(区块链系统设计无法同时兼顾可扩展性，去中心化和安全性).但我们必须知道，一切事物都有自己的边界，公链不应该做所有的事情，公链应该做它该做的事情：“公链是以最高效率达成共识的工具，能够以最低成本来构建信任”。</li><li>作为共识的工具，信任的引擎，公链不应该为了可扩展性放弃去中心化与安全性。那么公链的TPS这么低，该怎么使用呢？可以将大量的工作放到链下去解决，仅仅将最重要的数据提交到区块链主链上，让所有节点都能够验证这些链下的工作都是准确可靠的.</li><li>社会的发展带来的是更精细化的分工，区块链的技术发展也是如此，在底层区块链（Layer1）上构建一个扩展层（Layer2)，Layer1来保证安全和去中心化，绝对可靠、可信；它能做到全球共识，并作为“加密法院”，通过智能合约设计的规则进行仲裁，以经济激励的形式将信任传递到Layer2上，而Layer2追求极致的性能，它只能做到局部共识，但是能够满足各类商业场景的需求。</li></ol><h2 id="发展历史">发展历史</h2><ol type="1"><li>1985 年，零知识证明Zero-Knowledge Proof - 由 S.Goldwasser、 S.Micali 及 C.Rackoff 首次提出。</li><li>2010年，Groth实现了首个基于椭圆曲线双线性映射全能的，常数大小的非交互式零知识证明协议。后来这个协议经过不断优化，最终成为区块链著名的零知识证明协议SNARKs。</li><li>2013年，Pinocchio协议实现了分钟级别证明，毫秒级别验证，证明大小不到300字节，将零知识证明从理论带到了应用。后来Zcash使用的SNARKs正是基于Pinocchio的改进版。</li><li>2014 年，名为Zerocash的加密货币则使用了一种特殊的零知识证明工具zk-SNARKs （ Zero-Knowledge Succinct Non-interactive Arguments of Knowledge ) 实现了对交易金额、交易双方的完全隐藏，更注重于隐私，以及对交易透明的可控性。</li><li>2017 年， Zerocash 团队提出将 zk-SNARKs 与智能合约相互结合的方案，使交易能在众目睽睽下隐身，打造保护隐私的智能合约。</li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://zhuanlan.zhihu.com/p/152065162">零知识证明介绍-zhihu</a></li><li><a href="https://www.notboring.co/p/zero-knowledge">Zero Knowledge, from not boring</a></li><li><a href="https://mp.weixin.qq.com/s/_IrI8SJLo1Ht51nJfI4V_Q">十分钟开发一个混币-原理篇</a></li><li><a href="https://mp.weixin.qq.com/s/8OkwqNXIkUz2PBURoghRJQ">十分钟开发零知识证明之混币</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本概念&quot;&gt;基本概念&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;零知识证明(ZKP)的定义为：证明者（prover）能够在不向验证者（verifier）提供任何有用的信息的情况下，使验证者（verifier）相信某个论断是正确的。&lt;/p&gt;&lt;/li&gt;
&lt;l</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="加密货币" scheme="https://chiechie.github.io/tags/%E5%8A%A0%E5%AF%86%E8%B4%A7%E5%B8%81/"/>
    
    <category term="零知识量证明" scheme="https://chiechie.github.io/tags/%E9%9B%B6%E7%9F%A5%E8%AF%86%E9%87%8F%E8%AF%81%E6%98%8E/"/>
    
  </entry>
  
  <entry>
    <title>几个思维模型</title>
    <link href="https://chiechie.github.io/2021/06/28/reading_notes/reality/10-mental-model/"/>
    <id>https://chiechie.github.io/2021/06/28/reading_notes/reality/10-mental-model/</id>
    <published>2021-06-28T04:50:00.000Z</published>
    <updated>2021-06-28T05:47:15.061Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>基于剃刀原则的几个思维模型，from Sahil Bloom的twitter</p></blockquote><h2 id="the-steve-jobs-quality-razor">The Steve Jobs Quality Razor</h2><p>When building, take pride in carrying the quality all the way through.Would you be proud for your work to be seen from every angle and perspective? If not, keep working.</p><h2 id="the-eli5-razor">The ELI5 Razor</h2><ul><li>Complexity and jargon are often used to mask a lack of true understanding.</li><li>If you can’t explain it to a 5-year-old, you don’t really understand it.</li><li>If someone uses a lot of complexity and jargon to explain something to you, they probably don’t understand it.</li></ul><h2 id="mungers-rule-of-opinions">Munger’s Rule of Opinions</h2><p>“I never allow myself to have an opinion on anything that I don’t know the other side’s argument better than they do.” - Charlie Munger</p><p>Opinions aren’t free. You have to work to earn the right to have them.</p><h2 id="the-bezos-regret-minimization-framework">The Bezos Regret Minimization Framework</h2><p>The goal is to minimize the number of regrets in life.</p><p>When faced with a difficult decision: (1) Project yourself into the future (2) Look back on the decision (3) Ask "Will I regret not doing this?" (4) Take action</p><h2 id="buffetts-rule-of-holes">Buffett’s Rule of Holes</h2><p>“The most important thing to do if you find yourself in a hole is to stop digging." - Warren Buffett</p><p>When things aren’t working, change course and try something different.</p><p>When you find yourself at the bottom of a hole, stop digging and climb out of it.</p><h2 id="pgs-crazy-idea-razor">PG's Crazy Idea Razor</h2><p>If someone proposes a crazy idea, ask:</p><ol type="1"><li>Are they a domain expert?</li><li>Do I know them to be reasonable?</li></ol><p>If yes on (1) and (2), you should take the idea seriously, as it may be an asymmetric bet on the future.</p><h2 id="the-boasters-razor">The Boaster’s Razor</h2><p>Truly successful people rarely feel the need to boast about their success.</p><p>If someone regularly boasts about their income, wealth, or success, it’s fair to assume the reality is a fraction of what they claim.</p><h2 id="the-circle-of-competence">The Circle of Competence</h2><p>Be ruthless in identifying your circle of competence (and its boundaries).</p><p>When faced with a big decision, ask yourself whether you are qualified to handle it given your circle.</p><p>If yes, proceed. If no, outsource it to someone who is.</p><h2 id="the-duck-test">The Duck Test</h2><p>If it looks like a duck, swims like a duck, and quacks like a duck, it’s probably a duck.</p><p>You can determine a lot about a person by regularly observing their habitual characteristics.</p><h2 id="buffetts-juicy-pitch">Buffett’s Juicy Pitch</h2><p>“You don't have to swing at everything - you can wait for your pitch." - Warren Buffett</p><p>Life doesn’t reward you for the number of swings you take.</p><p>Slow down and focus on identifying the juiciest pitch.</p><p>When it comes, swing hard and don’t miss it.</p><h2 id="occams-razor">Occam’s Razor</h2><p>The simplest explanation is often the best one.</p><p>Simple assumptions &gt; complex assumptions.</p><p>Simple is beautiful.</p><h2 id="the-buffett-reputation-razor">The Buffett Reputation Razor</h2><p>“It takes 20 years to build a reputation and five minutes to ruin it. If you think about that, you'll do things differently.” - Warren Buffett</p><p>Remember that quote and act accordingly.</p><p>Your character is your fate.</p><h2 id="hanlons-razor">Hanlon’s Razor</h2><p>Never attribute to malice that which can be adequately explained by stupidity.</p><p>In assessing someone's actions, we should not assume negative intent if there is a viable alternative explanation, such as different beliefs, incompetence, or ignorance.</p><p>汉隆的剃刀</p><p>永远不要把可以用愚蠢充分解释的事情归咎于恶意。</p><p>在评估某人的行为时，如果有可行的替代解释，例如不同的信念、无能或无知，我们不应假设消极意图。</p><h2 id="nntalebs-the-look-the-part-razor">nntaleb's The “Look the Part” Razor</h2><p>If forced to choose between two options of seemingly equal merit, choose the one that doesn’t look the part.</p><p>The one who doesn’t look the part has had to overcome much more to achieve its status than the one who fit in perfectly.</p><p>如果被迫在看似同等价值的两个选项之间做出选择，请选择一个看起来不合适的选项。</p><p>与完美契合的人相比，看起来不合群的人必须克服更多才能获得地位。</p><h2 id="newtons-flaming-laser-sword">Newton’s Flaming Laser Sword</h2><p>If something cannot be settled by experiment or observation, it is not worth debating.</p><p>This will save you from wasting a lot of time on pointless arguments.</p><p>牛顿的火焰激光剑</p><p>如果一些事情不能通过实验或观察来解决，那就不值得争论了。</p><p>这将使您免于在无意义的争论上浪费大量时间。</p><h2 id="machiavellis-razor">Machiavelli’s Razor</h2><p>Never attribute to malice that which can be adequately explained by self-interest.</p><p>In assessing someone's actions, we should not assume negative intent if there is a viable alternative explanation that they are acting on rooted self-interest.</p><p>马基雅维利的剃刀</p><p>永远不要将可以用自身利益充分解释的事情归咎于恶意。</p><p>在评估某人的行为时，如果有一个可行的替代解释表明他们是根据根深蒂固的自身利益行事，我们不应该假设消极意图。</p><h2 id="hitchens-razor">Hitchens’ Razor</h2><p>What can be asserted without evidence can also be dismissed without evidence.</p><p>The burden of proof regarding a claim lies with the one who makes the claim. If unmet, no argument is required to dismiss it.</p><h2 id="sagans-standard">Sagan’s Standard</h2><p>“Extraordinary claims require extraordinary evidence.”</p><p>The more crazy and outrageous the claim, the more crazy and outrageous the body of evidence must be in order to prove it.</p><p>萨根的标准</p><p>“非凡的主张需要非凡的证据。”</p><p>声称越疯狂和越离谱，为了证明它，证据主体就必须越疯狂和离谱。</p><h2 id="the-eisenhower-decision-matrix">The Eisenhower Decision Matrix</h2><p>When faced with a task, ask: “Is this urgent? Is this important?”</p><p>An "urgent" task is one that requires immediate attention. An "important" task is one that promotes or furthers your long-term goals.</p><p>Place it on a 2x2 matrix and act accordingly.</p><h2 id="the-steve-jobs-settling-razor">The Steve Jobs Settling Razor</h2><p>“The only way to do great work is to love what you do. If you haven’t found it yet, keep looking. Don’t settle.” - Steve Jobs</p><p>It’s Monday morning. Did you wake up with energy or with dread?</p><p>Your answer will tell you if you’re settling.</p><h2 id="the-career-razor">The Career Razor</h2><p>When deciding on a new job, choose the one that will challenge you the most (intellectually, physically, or emotionally).</p><p>Challenge and discomfort forces growth.</p><p>(P.S. Check out the job board below for challenging new roles!)</p><h2 id="decisions">Decisions</h2><p>• If you can’t decide, the answer is no.</p><p>• If two equally difficult paths, choose the one more painful in the short term (pain avoidance is creating an illusion of equality).</p><p>• Choose the path that leaves you more equanimous in the long term."</p><h2 id="参考">参考</h2><ol type="1"><li>https://pallet.xyz/list/thebloomboard/jobs</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;基于剃刀原则的几个思维模型，from Sahil Bloom的twitter&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;the-steve-jobs-quality-razor&quot;&gt;The Steve Jobs Quality Razor&lt;</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="决策" scheme="https://chiechie.github.io/tags/%E5%86%B3%E7%AD%96/"/>
    
  </entry>
  
  <entry>
    <title>进程和线程</title>
    <link href="https://chiechie.github.io/2021/06/27/reading_notes/computer/thread-and-process/"/>
    <id>https://chiechie.github.io/2021/06/27/reading_notes/computer/thread-and-process/</id>
    <published>2021-06-27T04:16:07.000Z</published>
    <updated>2021-06-28T05:56:10.038Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本定义">基本定义</h2><h3 id="进程process">进程（process）</h3><p>我们想要计算机要做一项任务（task），我们会写一段代码（python/java等）。</p><p>编译器将它翻译成二进制代码--机器的语言。</p><p>但是此时不执行这段断码的话，就还是一段静态程序。</p><p>当执行起来的时候，就变成了一个进程。</p><p>进程（process）有时候也称做任务，是指一个程序运行的实例。</p><h3 id="线程threads">线程（threads）</h3><p>一个进程中的执行的单位。</p><p>线程（thread）：能并行运行，并且与他们的父进程（创建他们的进程）共享同一地址空间（一段内存区域）和其他资源的轻量级的进程</p><h2 id="应用-vs-线程-vs-进程">应用 vs 线程 vs 进程</h2><p>一个应用，比如chrome，可能会启动多个进程（多个网页）, 一个进有多个线程。</p><p>进程和线程的区别：</p><p>• 进程（火车）间不会相互影响，一个线程（车厢）挂掉将导致整个进程（火车）挂掉 • 线程（车厢）在进程（火车）下行进 • 一个进程（火车）可以包含多个线程（车厢） • 不同进程（火车）间数据很难共享，同一进程（火车）下不同线程（车厢）间数据很易共享 线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据， 进程之间的通信需要以通信的方式（IPC)进行 • 进程要比线程消耗更多的计算机资源 • 进程间不会相互影响，一个线程挂掉将导致整个进程挂掉 • 进程可以拓展到多机，线程最多适合多核 • 进程使用的内存地址可以上锁，即一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。－"互斥锁" • 进程使用的内存地址可以限定使用量－“信号量”</p><h2 id="硬件多线程vs软件多线程">硬件多线程vs软件多线程</h2><p>CPU架构演进路线： 多cpu---&gt;超线程--&gt;多core</p><p>https://stackoverflow.com/questions/680684/multi-cpu-multi-core-and-hyper-thread</p><p>其中的超线程（hyper thread）指的硬件多线程，如下图，相当于给一个core，虚拟化为2个core，可以更方便压榨计算机性能</p><h2 id="多进程-or-多线程">多进程 or 多线程？</h2><p>多进程更稳定，但是多线程能达到更高的计算效率</p><figure><img src="img_1.png" alt="左边是单线程，右边是多线程" /><figcaption aria-hidden="true">左边是单线程，右边是多线程</figcaption></figure><p>多线程的优势：</p><ul><li>响应性：比如启动一个网页（启动一个浏览器进程），可以同时并行做个事情，如浏览/下载/问答（并行启动多个线程）。</li><li>资源共享：一个进程上的所有线程共享同一份内存，这样能够让机器的使用效率更高，可以做更多的复杂的事情。---赋能/增效</li><li>更经济： 多进程浪费资源，因为创建1个进程需要分配很多内存和资源，相比之下，创建和切换线程的成本小的多。另外，完成一个复杂的任务，多线程共用一份底层资源，多进程就需要把资源复制几份。又浪费了一遍。--降本</li><li>充分压榨多处理器的架构：</li></ul><blockquote><p>大中台类似多线程，烟囱式开发类似多进程</p></blockquote><h2 id="实践">实践</h2><h3 id="练习1-模拟单线程cpp的进程管理">练习1-模拟单线程CPP的进程管理</h3><p><a href="https://leetcode-cn.com/problems/single-threaded-cpu/">leetcode</a>的题目，</p><p>需求：实现一个任务管理/编排的机制，即，输入一堆任务，每个任务的计划执行时间/执行时长都有，现在有一台单线程CPU，如何安排这些任务的执行顺序？</p><p>分析： 设想应用场景，医院的排队系统/有一堆任务要排期。</p><p>一遍在执行已有的任务，一边有源源不断的接到新的任务，</p><p>每执行完一个任务，check一下距离上次检查，有多少新任务进来了，加到任务池里面，从里面选出最容易的。</p><p>设计1个数据结构：</p><p>1个是优先队列，存放候选任务，并且按照执行时间长短从小到到排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tasks = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">4</span>,<span class="number">1</span>]]</span><br><span class="line">n = <span class="built_in">len</span>(tasks)</span><br><span class="line">timestamp = <span class="number">1</span></span><br><span class="line">candidate_list = []</span><br><span class="line">new_task = []</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="keyword">while</span> (j &lt; n) <span class="keyword">and</span> (tasks[j][<span class="number">0</span>] &lt;= timestamp):</span><br><span class="line">        heapq.heappush(candidate_list, (tasks[j][<span class="number">1</span>], j))</span><br><span class="line">        j+=<span class="number">1</span></span><br><span class="line">        print(j, n)</span><br><span class="line">    print(candidate_list)</span><br><span class="line">    process, index = heapq.heappop(candidate_list)</span><br><span class="line">    print(candidate_list)</span><br><span class="line">    new_task.append(index)</span><br><span class="line">    timestamp += process</span><br><span class="line">new_task</span><br></pre></td></tr></table></figure><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.zhihu.com/question/25532384/answer/411179772">biaodianfu-zhihu</a></li><li><a href="https://www.youtube.com/watch?v=usyg5vbni34">thred</a></li><li><a href="https://www.junmajinlong.com/os/multi_cpu/">计算机原理系列-blog</a></li><li><a href="https://www.junmajinlong.com/os/cpu_cache/">关于CPU上的高速缓存</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本定义&quot;&gt;基本定义&lt;/h2&gt;
&lt;h3 id=&quot;进程process&quot;&gt;进程（process）&lt;/h3&gt;
&lt;p&gt;我们想要计算机要做一项任务（task），我们会写一段代码（python/java等）。&lt;/p&gt;
&lt;p&gt;编译器将它翻译成二进制代码--机器的语言。&lt;/p&gt;
</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="计算机原理" scheme="https://chiechie.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="线程" scheme="https://chiechie.github.io/tags/%E7%BA%BF%E7%A8%8B/"/>
    
    <category term="进程" scheme="https://chiechie.github.io/tags/%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>计算机的存储单元</title>
    <link href="https://chiechie.github.io/2021/06/26/reading_notes/computer/cache-memory/"/>
    <id>https://chiechie.github.io/2021/06/26/reading_notes/computer/cache-memory/</id>
    <published>2021-06-26T01:47:32.000Z</published>
    <updated>2021-06-28T05:56:10.034Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概览">概览</h2><ol type="1"><li><p>计算机上的存储单元的处理速度从快到慢依次是： 寄存器&gt; L1&gt;L2&gt;L3&gt;内存&gt;固态硬盘&gt; 机械硬盘</p><figure><img src="./img_1.png" alt="存储单元" /><figcaption aria-hidden="true">存储单元</figcaption></figure></li><li><p>早期的计算机只有寄存器和内存，但是寄存器的处理速度远高于内存，所以大部分时间是寄存器在等内存，所以CPU是处于空转状态。 经过改进</p></li><li><p>除了寄存器，后面的计算机在CPU中又逐步加入了高速缓存--L1/L2/L3缓存，相当于让内存提前做功课，把数据提前取出来，在3个缓存中候着，等寄存器有空了就取来用。笨鸟先飞嘛。</p></li><li><p>L1/L2/L3缓存，每层速度递减、容量递增。L1缓存速度接近寄存器速度，大约1ns时延。</p></li><li><p>多核CPU的L3对诶个core是共享的，L2和L1是每个core私有的。</p></li></ol><figure><img src="./img_2.png" alt="img_2.png" /><figcaption aria-hidden="true">img_2.png</figcaption></figure><ol start="6" type="1"><li>CPU读取数据时，要从内存读取到L3，再读取到L2再读取到L1，同样写到内存时也会经过这些层次。</li></ol><h2 id="参考">参考</h2><ol type="1"><li>https://www.junmajinlong.com/os/cpu_cache/</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;概览&quot;&gt;概览&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;计算机上的存储单元的处理速度从快到慢依次是： 寄存器&amp;gt; L1&amp;gt;L2&amp;gt;L3&amp;gt;内存&amp;gt;固态硬盘&amp;gt; 机械硬盘&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;./img_1</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="计算机原理" scheme="https://chiechie.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="内存" scheme="https://chiechie.github.io/tags/%E5%86%85%E5%AD%98/"/>
    
    <category term="缓存" scheme="https://chiechie.github.io/tags/%E7%BC%93%E5%AD%98/"/>
    
    <category term="寄存器" scheme="https://chiechie.github.io/tags/%E5%AF%84%E5%AD%98%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>数据结构基础</title>
    <link href="https://chiechie.github.io/2021/06/25/data_structure/data-structure-base/"/>
    <id>https://chiechie.github.io/2021/06/25/data_structure/data-structure-base/</id>
    <published>2021-06-25T01:17:29.000Z</published>
    <updated>2021-06-28T02:08:34.886Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li><p>数据结构分为线性和非线性</p></li><li><p>线性包括数组列表栈队列</p><figure><img src="b3728c27302a8548fe9e8a87e619ca83.png" alt="线性数据结构" /><figcaption aria-hidden="true">线性数据结构</figcaption></figure></li><li><p>非线性包括树和图,树可以认为是图的special case <img src="e6d5a8d9a75587abe612dfef9abffc01.png" alt="非线性数据结构" /></p></li><li><p>图分有向图和无向图</p><figure><img src="18c651092d22c7204021d10a5a79b0ff.png" alt="有向图vs无向图" /><figcaption aria-hidden="true">有向图vs无向图</figcaption></figure></li><li><p>无向图的一个实例是fb的社交网络，边表示好友关系。</p><figure><img src="f3fc896014d62fb1ec1c96c93210f7ff.png" alt="社交网络" /><figcaption aria-hidden="true">社交网络</figcaption></figure></li><li><p>基于社交网络这个数据结构有什么应用呢？好友推荐, 推荐朋友的朋友,网络社会科学的小世界</p><blockquote><p>小世界网络的重要性质：“流行病学”、“合作”、“知识”</p></blockquote><figure><img src="d5fe57a166d6f2ee93457d0ea4b54cef0.png" alt="社交网路" /><figcaption aria-hidden="true">社交网路</figcaption></figure></li><li><p>有向图的一个实例是万维网,好有一个文章影响因子 <img src="b9b97250ce6e998045dcbb0d5b379724.png" alt="www" /></p></li><li><p>图还可以分有权图和无权图，无权图可认为是权图的special case，权重都为1。</p></li><li><p>有权图的一个实例是高速公路网,边代表距离 <img src="5b81b50b2d2b048ed3188b71af85a02f.png" alt="公路网" /></p></li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.youtube.com/watch?v=gXgEDyodOJU">youtube</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;数据结构分为线性和非线性&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;线性包括数组列表栈队列&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;b3728c27302a8548fe9e8a87e619ca83.png&quot; alt=&quot;线性数据结构&quot; /&gt;&lt;f</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="编程" scheme="https://chiechie.github.io/tags/%E7%BC%96%E7%A8%8B/"/>
    
    <category term="数据结构" scheme="https://chiechie.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="图数据" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>细说红楼梦</title>
    <link href="https://chiechie.github.io/2021/06/21/reading_notes/zhexue/hongloumengxishuo/"/>
    <id>https://chiechie.github.io/2021/06/21/reading_notes/zhexue/hongloumengxishuo/</id>
    <published>2021-06-21T02:02:37.000Z</published>
    <updated>2021-06-22T06:21:25.475Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第一回">第一回</h2><p>空空道人有批注：“因空见色，由色生情，传情入色，自色悟空，遂易名为情僧……”</p><p>“因空见色”，是说本来就白茫茫一片，什么都没有，（六祖慧能：本来无一物），因为我们的幻觉，看到好多好多的现象，“由色生情”，情多了就会陷进去，陷进了更深的色的幻觉，“传情入色”，要经过多少彻悟之后，再从里面出来，“自色悟空”，再回到白茫茫一片真干净。</p><p>甄士隐经了这许多人生起伏，有一天突然又看见跛足道士来了，口里唱一首《好了歌》：</p><blockquote><p>世人都晓神仙好，惟有功名忘不了！古今将相在何方？荒冢一堆草没了。世人都晓神仙好，只有金银忘不了！终朝只恨聚无多，及到多时眼闭了。世人都晓神仙好，只有姣妻忘不了！君生日日说恩情，君死又随人去了。世人都晓神仙好，只有儿孙忘不了！痴心父母古来多，孝顺儿孙谁见了？</p></blockquote><p>甄士隐说：“你唱什么？我只听见‘好’‘了’、‘好’‘了’两个字。道人说：</p><blockquote><p>好便是了，了便是好。若不了，便不好；若要好，须是了。</p></blockquote><p>跛足道人讲的是道家的哲学，对儒家社会秩序，有很大的颠覆性。儒家修身齐家治国平天下这一套道理，要建立的是稳定的社会秩序（social order），鼓励人入世，求功名、利禄、妻子、儿女，儒家宗法社会下面，大概就是这些。</p><p>跛足道人给了很大的一个警告，好就是了，了就是好，等于给恋恋在红尘中的人临头棒喝、醍醐灌顶。</p><p>人大概都经过几个阶段：年轻的时候，大家都是入世哲学，儒家那一套，要求功名利禄。到了中年，大概受了些挫折，于是道家来了，点你一下，有所醒悟。到了最后，要超脱人生境界的时候，佛家就来了。</p><p>所以过去的中国人，从儒道释，大致都经过这么三个阶段，有意思的是这三个阶段不冲突。在同一个人身上，这三样哲学都有。所以中国人既出世又入世的态度，常常造成整个文化的一种紧张，也就是说，我们的人生态度在这之间常常有一种徘徊迟疑，我想，这就是文学的起因。</p><p>文学写什么呢？写一个人求道提升，讲他的目标求道，讲这一生多么地艰难，往往很多人没有求到，在半路已经失败。不管是爱情也好，理想也好，各种的失败，我想，文学写的就是这些。《红楼梦》写的也是这些。</p><p>甄士隐对《好了歌》的注解：</p><blockquote><p>陋室空堂，当年笏满床；衰草枯杨，曾为歌舞场。蛛丝儿结满雕梁，绿纱今又糊在蓬窗上。说什么脂正浓、粉正香，如何两鬓又成霜？昨日黄土陇头送白骨，今宵红灯帐底卧鸳鸯。金满箱，银满箱，展眼乞丐人皆谤。正叹他人命不长，那知自己归来丧！训有方，保不定日后作强梁。择膏粱，谁承望流落在烟花巷！因嫌纱帽小，致使锁枷扛；昨怜破袄寒，今嫌紫蟒长：乱烘烘你方唱罢我登场，反认他乡是故乡。甚荒唐，到头来都是为他人作嫁衣裳！</p></blockquote><p>人生不就是个大舞台，一个人唱完下来，第二个人上去唱，唱完又一鞠躬下台，又换个人上去唱，“乱烘烘你方唱罢我登场”，然后“反认他乡是故乡”。</p><p>佛家说，我们以为这是我们自己的故乡，其实也靠不住，一下子一把火就整个烧掉了。道家说，要醒悟这一点，才找到你真正理想的地方。甚荒唐！到头来都是为他人做嫁衣裳。讲起来，整个一生是白忙一场，都为他人做嫁衣裳，所有事情都是为他人做的。</p><p>甄士隐、贾雨村，一个代表出世，一个代表入世。后来甄士隐变成道士，贾雨村经过好多官宦历程的折腾，到了书结束的时候，这两人又碰到一起。甄士隐想要度化贾雨村，贾雨村还是迷恋红尘。各走各的路，两个分歧，自己去看，自己去选择。</p><h2 id="第二章">第二章</h2><p>贾政，自觉是遵从儒家理想的一个人。他非常正直，也想用儒家那一套思想道德来持家，但是太过守法，太过拘束了。中国社会能够生存下来，光靠儒家思想、书生之见是不够的，还需要别种哲学，譬如很要紧的法家，很实在的、很现实的顶在后边。儒家的很多理想不一定都能实现，碰到了现实问题，常常不能解决。不过儒家也很重要，它是一种道德力量（moral force），没有它也不行，但光是有它也不行，所以，还要配合别的东西。</p><p>贾雨村听冷子兴讲了半天贾宝玉这个怪人，也发表了一篇言论：</p><p>“天地生人，除大仁大恶两种，馀者皆无大异。若大仁者，则应运而生，大恶者，则应劫而生。运生世治，劫生世危。尧、舜、禹、汤、文、武、周、召、孔、孟、董、韩、周、程、张、朱，皆应运而生者。蚩尤、共工、桀、纣、始皇、王莽、曹操、桓温、安禄山、秦桧等，皆应劫而生者。大仁者，修治天下；大恶者，挠乱天下。清明灵秀，天地之正气，仁者之所秉也；残忍乖僻，天地之邪气，恶者之所秉也。今当运隆祚永之朝，太平无为之世，清明灵秀之气所秉者，上至朝廷，下及草野，比比皆是。所馀之秀气，漫无所归，遂为甘露、为和风，洽然溉及四海。彼残忍乖僻之邪气，不能荡溢于光天化日之中，遂凝结充塞于深沟大壑之内，偶因风荡，或被云摧，略有摇动感发之意，一丝半缕误而泄出者，偶值灵秀之气适过，正不容邪，邪复妒正，两不相下，亦如风水雷电，地中既遇，既不能消，又不能让，必至搏击掀发后始尽。故其气亦必赋人，发泄一尽始散。使男女偶秉此气而生者，在上则不能成仁人君子，下亦不能为大凶大恶。置之于万万人中，其聪俊灵秀之气，则在万万人之上；其乖僻邪谬不近人情之态，又在万万人之下。若生于公侯富贵之家，则为情痴情种；若生于诗书清贫之族，则为逸士高人；纵再偶生于薄祚寒门，断不能为走卒健仆，甘遭庸人驱制驾驭，必为奇优名娼。如前代之许由、陶潜、阮籍、嵇康、刘伶、王谢二族、顾虎头、陈后主、唐明皇、宋徽宗、刘庭芝、温飞卿、米南宫、石曼卿、柳耆卿、秦少游，近日之倪云林、唐伯虎、祝枝山，再如李龟年、黄幡绰、敬新磨、卓文君、红拂、薛涛、崔莺、朝云之流，此皆易地则同之人也。”</p><h2 id="第四章">第四章</h2><p>再看看贾雨村这个人，他也是有高度象征性的一个人，象征这个世界上每一个为求功名利禄不择手段往上爬的凡俗之人。</p><p>贾雨村刚刚做官，还不太清楚。葫芦僧就讲，贾、王、史、薛这四大家族是互相牵连、互相庇护的，你一动薛家，其他家族就暗中使力，根本没法去办他们。的确是！后来薛蟠又打死人，贾家去讲讲情也就过了。</p><p>为官为政的这些人，像贾雨村者也是很典型的，一旦发迹，若从前是贫贱出身，他不愿意人家知道他的过去，哪个不长心眼儿的去碰他的过去的话，砍掉！因为他要维持现在的形象，掩掉过去。后来贾家没落的时候，贾雨村果然对有恩的贾家加踹一脚，这正是官场反复无情的写照。</p><h2 id="第五章">第五章</h2><p>无常，我们看起来好像是个悲剧，在佛家看，人所有的一切就是如此。佛教很理性地看待人生，看待一切的事物，没有永远存在的东西，因为时间会破坏一切、会毁灭一切，有时间的转动，春夏秋冬的转动，就会有无常现象。人生无常，什么都无常。所以，“生关死劫谁能躲？闻说道，西方宝树唤婆娑，上结着长生果。”这是惜春的醒悟！佛教的传说，佛陀释迦牟尼圆寂的时候，是在两棵宝树中间，那两棵宝树叫作“娑罗”。佛教对“婆娑”两字另有所解，长生果，佛家讲修成正果，要悟道了以后，从而解脱。最后惜春出家了，她是找到解脱最彻底的一个人物。</p><p>王熙凤，机关算尽太聪明，她最精明，涉入红尘也最深，《好了歌》讲：“世人都晓神仙好，只有金银忘不了！”等到聚多之时缘尽了。凤姐放高利贷，到处攒钱，聚敛多时，一下子抄家抄得精光，一场空欢喜！她在贾府最得宠，掌大权，抄家的原因之一是她放高利贷给抄出来了，一世的颜面丢得精光。“反算了卿卿性命”，后来王熙凤的下场也很悲惨。</p><p>对照一下，惜春走一条路，王熙凤走另外一条路，一个出世，一个入世，两种不同的道路。甄士隐跟贾雨村，也是出世和入世的辩证。曹雪芹从未批判哪方，他只是写出各人选的道路。儒家的入世在《红楼梦》的结局也很重要，贾府衰败，王熙凤死了，也还有接班人，谁呢？薛宝钗，她把贾府重新扛起来，这就是人生。</p><p>中国人的人生，常常有三种哲学的循环。年轻的时候有所求，中年醒悟，晚年一切看开。有这三种哲学，才是完整的中国文化，三者相克相生，互用互补。 《红楼梦》曲最后收尾的曲子：</p><blockquote><p>为官的，家业凋零；富贵的，金银散尽；有恩的，死里逃生；无情的，分明报应。欠命的，命已还；欠泪的，泪已尽。冤冤相报实非轻，分离聚合皆前定。欲知命短问前生，老来富贵也真侥幸。看破的，遁入空门；痴迷的，枉送了性命。好一似食尽鸟投林，落了片白茫茫大地真干净！</p></blockquote><h2 id="第16回">第16回</h2><p>儒家跟道家人生观，宇宙、社会的看法，互相冲突。《红楼梦》的悲剧，并不是一个突发性的意外，而是人生必然的过程，</p><h2 id="第22回">第22回</h2><p>宝玉出的灯谜：南面而坐，北面而朝，“象忧亦忧，象喜亦喜”。 佛家有一句话说镜花水月，一切都是幻象。宝玉看到的一切，由色入空，一切都是幻象。</p><h2 id="参考">参考</h2><ol type="1"><li>https://weread.qq.com/web/reader/3c432fc05d0f283c488450e</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;第一回&quot;&gt;第一回&lt;/h2&gt;
&lt;p&gt;空空道人有批注：“因空见色，由色生情，传情入色，自色悟空，遂易名为情僧……”&lt;/p&gt;
&lt;p&gt;“因空见色”，是说本来就白茫茫一片，什么都没有，（六祖慧能：本来无一物），因为我们的幻觉，看到好多好多的现象，“由色生情”，情多了就会陷进</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="道家" scheme="https://chiechie.github.io/tags/%E9%81%93%E5%AE%B6/"/>
    
    <category term="佛家" scheme="https://chiechie.github.io/tags/%E4%BD%9B%E5%AE%B6/"/>
    
    <category term="人生哲学" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E7%94%9F%E5%93%B2%E5%AD%A6/"/>
    
    <category term="儒家" scheme="https://chiechie.github.io/tags/%E5%84%92%E5%AE%B6/"/>
    
  </entry>
  
  <entry>
    <title>概率图简介</title>
    <link href="https://chiechie.github.io/2021/06/19/data_structure/graph/gailvtu/"/>
    <id>https://chiechie.github.io/2021/06/19/data_structure/graph/gailvtu/</id>
    <published>2021-06-19T08:52:21.000Z</published>
    <updated>2021-06-28T02:12:21.520Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念">基本概念</h2><p>概率图模型(Probabilistic Graphical Model，PGM)，简称图模型(Graphical Model，GM)，首先是一种概率模型，其次，它用图结构描述多个随机变量之间的依赖关系，它是研究高维空间中的概率模型的一种有用工具。</p><p>概率图模型有三个基本问题: 表示问题，学习问题和推断问题</p><p><img src="https://img.mubu.com/document_image/e3036b4e-18d3-4e73-b996-eafe4a4c08d1-3380623.jpg" /></p><h3 id="表示问题">1. 表示问题</h3><p>表示问题，即对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。</p><h4 id="有向图模型directed-graphical-model">有向图模型(Directed Graphical model)</h4><p>有向图模型(Directed Graphical model)，也称为贝叶斯网络(Bayesian Network)，或信念网络(Belief Network，BN)，是指用有向图来表示概率分布的图模型。</p><figure><img src="https://img.mubu.com/document_image/75c9420a-058f-4005-a4b2-4609e7987c1b-3380623.jpg" alt="贝叶斯网络" /><figcaption aria-hidden="true">贝叶斯网络</figcaption></figure><p>很多机器学习模型可以用有向图模型来描述，比如</p><ul><li>朴素贝叶斯分类器</li><li>隐马尔可夫模型</li><li>深度信念网络(sigmoid 信念网络)</li></ul><h4 id="无向图模型">无向图模型</h4><p>无向图模型也称为马尔可夫随机场(Markov Random Field，MRF)或 马尔可夫网络(Markov Network)，是一类用无向图来描述一组具有局部马尔可夫性质的随机向量 X 的联合概率分布的模型。</p><figure><img src="https://img.mubu.com/document_image/85a18510-051c-4339-a808-4d3759095432-3380623.jpg" alt="无向图模型" /><figcaption aria-hidden="true">无向图模型</figcaption></figure><ul><li><p>如果(G,X)满足局部马尔可夫性质， 即一个变量 Xk 在给定它的邻居的情况下独立于所有其它变量，</p></li><li><p>无向图模型的概率分解</p><ul><li>因子分解 :无向图中的的联合概率可以分解为一系列定义在最大团上的非负函数的乘积形式。</li><li>吉布斯分布 :无向图模型和吉布斯分布是一致的。吉布斯分布一定满足马尔可夫随机场的条件独立性质，并且马尔可夫随机场的概率分布一定可以表示成吉布斯分布。</li></ul></li><li><p>常见的无向图模型:很多经典的机器学习模型可以使用无向图模型来描述，比如</p><ul><li>对数线性模型(Log-Linear Model)或最大熵模型(Maximum Entropy Model)</li><li>条件随机场(Conditional Random Field，CRF)</li><li>玻尔兹曼机</li><li>受限玻尔兹曼机等</li></ul></li><li><p>有向图和无向图之间的转换:</p><ul><li>无向图模型可以表示有向图模型无法表示的一些依赖关系，比如循环依赖;</li><li>无向图模型不能表示有向图模型能够表示的某些关系，比如因果关系。</li></ul></li></ul><h3 id="推断问题">2. 推断问题</h3><p>推断问题，即在已知部分变量时，计算其它变量的后验概率分布。</p><p>图模型的推断问题可以转换为求任意一个变量子集的边际概率分布问题。</p><h4 id="精确推断">精确推断</h4><p>精确推断的方法有变量消除法和信念传播方法</p><p>信念传播(Belief Propagation，BP)算法，也称为和积(Sum-Product)算法 或消息传递(Message Passing)算法，是将变量消除法中的和积(Sum-Product) 操作看作是消息，并保存起来，这样可以节省大量的计算资源。</p><p>链式结构上的的信念传播算法</p><p><img src="https://img.mubu.com/document_image/bd0b3e40-76a5-42e2-8716-799247eddc32-3380623.jpg" /></p><p>树结构上的信念传播算法：</p><ul><li>从叶子节点到根节点依次计算并传递 消息;</li><li>从根节点开始到叶子节点，依次计算并传递消息;</li><li>在每个节点上 计算所有接收消息的乘积(如果是无向图还需要归一化)，就得到了所有变量的 边际概率。</li></ul><p>环路信念传播</p><h4 id="近似推断">近似推断</h4><p>图模型中有些变量的局部条件分布可能非常复杂，或其积分无法计算。需要使用数值方法来近似，比如，变分法和采样法</p><h5 id="变分法">变分法</h5><p>变分法(Variational Method)是引入一个变分分布(通常是比较简单的分布)来近似复杂的局部条件概率，然后通过迭代的方法计算后验分布。</p><h5 id="采样法">采样法</h5><p>采样法(SamplingMethod) 是通过simulation的方式来采集符合某个分布 p(x) 的一些样本，并通过这些样本来估计和这个分布有关的运算，比如期望等。</p><ol type="1"><li>拒绝采样(Rejection Sampling):也叫接受-拒绝采样(Acceptance-RejectionSampling)：假设原始分布 p(x) 难以直接采样，可引入一个容易采样的分布 q(x)， 一般称为提议分布(Proposal Distribution)，然后以某个标准来拒绝一部分的样本使得最终采集的样本服从分布p(x)。</li></ol><p><img src="https://img.mubu.com/document_image/f6ae17af-597a-4445-abbf-2c2b0a5bce67-3380623.jpg" /></p><ol start="2" type="1"><li>重要性采样(Importance Sampling):通过引入重要性权重，将分布p(x)下f(x)的期望变为在分布q(x)下f(x)w(x)的期望.如果采样的目的是计算分布 p(x)下函数f(x)的期望，那么实际上抽取的样本不需要严格服从分布p(x)。也可以通过另一个分布，即提议分布q(x)，直接采样并估计。</li><li>马尔可夫链蒙特卡罗(Markov Chain Monte Carlo，MCMC): 在高维空间中拒绝采样和重要性采样效率低，<ul><li>Metropolis-Hastings算法，以及两个特例Metropolis算法和吉布斯采样(Gibbs Sampling)</li><li>Metropolis算法，假设MH算法中的提议分布是对称的</li><li>吉布斯采样(Gibbs Sampling)，用全条件概率(Full Conditional Probability)作为提议分布来依次对每个维度 进行采样，并设置接受率为A = 1。</li></ul></li></ol><blockquote><p>蒙特卡罗的基本思想可以归结为，根据一个已知概率密度函数为 p(x) 的 分布来计算函数 f (x) 的期望</p></blockquote><h3 id="学习问题">3. 学习问题</h3><p>概率图模型的学习包括：图结构的学习和参数的学习</p><p>参数的学习，即参数估计问题，可分为含隐变量的参数估计和不含因变量的参数估计：</p><ul><li><p>不含隐变量的参数估计：</p><ul><li>如果图模型中不包含隐变量，即所有变量都是可观测的，那么网络参数一般可以直接通过最大似然来进行估计。</li><li>有向图模型：所有变量x的联合概率分布可以分解为每个随机变量<span class="math inline">\(x_k\)</span>的局部条件概率 <span class="math inline">\(p(xk |xπk , \theta_k)\)</span> 的连乘形式，其中<span class="math inline">\(\theta_k\)</span>为第 k 个变量的局部 条件概率的参数。</li><li>无向图模型： 所有变量x的联合概率分布可以分解为定义在最 大团上的势能函数的连乘形式。</li></ul></li><li><p>含隐变量的参数估计：</p><ul><li>如果图模型中包含隐变量，即有部分变量是不可观测的，就需要用 EM 算法</li><li>EM 算法的应用例子:高斯混合模型。高斯混合模型(Gaussian Mixture Model，GMM)是由多个高斯分布组成的模型，其密度函数为多个高 斯密度函数的加权组合。 <img src="https://img.mubu.com/document_image/3476923c-5528-4dd4-aa4d-ae10183bf377-3380623.jpg" /> <img src="https://img.mubu.com/document_image/28d53640-0eb6-4a17-81c6-ffa8e4dbb289-3380623.jpg" /></li></ul></li></ul><h2 id="概率图模型与机器学习">概率图模型与机器学习</h2><ul><li>很多机器学习模型都可以归结为概率模型，即建模输入和输 出之间的条件概率分布。</li><li>图模型提供了一种新的角度来解释机器学习模 型，并且这种角度有很多优点，比如了解不同机器学习模型之间的联系，方便设计新模型等。</li><li>概率图模型中最基本的假设是条件独立性。图形化表示直观地描述了随机 变量之间的条件独立性，有利于将复杂的概率模型分解为简单模型的组合，并 更好地理解概率模型的表示、推断、学习等方法。</li></ul><h2 id="概率图模型与神经网络">概率图模型与神经网络</h2><p>概率图模型和神经网络有着类似的网络结构，但两者不同。</p><ul><li>节点<ul><li>概率图模型的节点是随机变量，其图结构的主要功能是用来描述变量 之间的依赖关系，一般是稀疏连接。使用图模型的好处是可以有效进行统计推 断。</li><li>神经网络中的节点是神经元，是一个计算节点。如果将神经网络中每个神经元看做是一个binary随机变量，那神经网络就变成一个sigmoid信念网络。</li></ul></li><li>变量的含义<ul><li>图模型中的每个变量一般有着明确的解释，<strong>变量之间依赖关系一般是人工来定义</strong>。</li><li>神经网络中的单个神经元则没有直观的解释</li></ul></li><li>生成模型与判别模型<ul><li>神经网络是判别模型，直接用来分类</li><li>图模型不但可以是判别模型，也可以是生成模型。生成模型不但可以用来生成样本，也可以通过贝叶斯公式用来做分类。</li></ul></li><li>学习方法<ul><li>神经网络参数学习的目标为交叉熵或平方误差等损失函数。</li><li>图模型的参数学习的目标函数为似然函数或条件似然函数，若包含隐变量则通常通过EM算法来求解。</li></ul></li></ul><p>神经网络和概率图模型的结合：</p><ul><li><p>用神经网络强大的表示能力来建模图模型中的</p><ul><li>推断问题：比如变分自编码器</li><li>生成问题：比如生成对抗网络，第</li><li>势能函数：比如 LSTM+CRF模型[Lample et al., 2016, Ma and Hovy, 2016]</li></ul></li><li><p>用图模型的算法来解决复杂结构神经网络中的学习和推断问题</p><ul><li>图结构神经网络(Graph Neural Network)</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本概念&quot;&gt;基本概念&lt;/h2&gt;
&lt;p&gt;概率图模型(Probabilistic Graphical Model，PGM)，简称图模型(Graphical Model，GM)，首先是一种概率模型，其次，它用图结构描述多个随机变量之间的依赖关系，它是研究高维空间中的概率</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="贝叶斯网络" scheme="https://chiechie.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
    <category term="概率图" scheme="https://chiechie.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE/"/>
    
    <category term="贝叶斯优化" scheme="https://chiechie.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>哲学导论</title>
    <link href="https://chiechie.github.io/2021/06/18/reading_notes/zhexue/zhexue-summart/"/>
    <id>https://chiechie.github.io/2021/06/18/reading_notes/zhexue/zhexue-summart/</id>
    <published>2021-06-18T09:01:52.000Z</published>
    <updated>2021-06-28T02:11:59.256Z</updated>
    
    <content type="html"><![CDATA[<h1 id="定义">定义</h1><p>哲学，就我对这个词的理解来说，乃是某种介乎神学与科学之间的东西。它和神学一样，包含着人类对于那些迄今仍为科学知识所不能肯定之事物的思考；但它又像科学一样，是诉之于人类的理性而不是诉之于权威的，不论是传统的权威还是启示的权威。一切确切的知识都属于科学；一切涉及超乎确切知识之外的教条都属于神学。但介乎神学与科学之间还有一片受到双方攻击的无人之域，这片无人之域就是哲学。---罗素</p><h1 id="哲学的主分支">哲学的主分支</h1><p>哲学的主分支：形而上学、知识论、伦理学、逻辑学和美学 - <a href="https://zh.wikipedia.org/wiki/%E9%82%8F%E8%BC%AF%E5%AD%B8">逻辑学</a> - <a href="https://zh.wikipedia.org/wiki/%E5%BD%A2%E8%80%8C%E4%B8%8A%E5%AD%B8">形而上学</a>/<a href="https://zh.wikipedia.org/wiki/%E5%AE%87%E5%AE%99%E8%AB%96">宇宙论</a> - <a href="https://zh.wikipedia.org/wiki/%E7%9F%A5%E8%AD%98%E8%AB%96">知识论</a> - <a href="https://zh.wikipedia.org/wiki/%E5%80%AB%E7%90%86%E5%AD%B8">伦理学</a>/<a href="https://zh.wikipedia.org/wiki/%E5%83%B9%E5%80%BC%E8%AB%96">价值论</a> - <a href="https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%AD%B8">美学</a></p><h1 id="特殊分支">特殊分支</h1><p>哲学的特殊分支：是交叉学科的哲学研究</p><ul><li><a href="https://zh.wikipedia.org/wiki/%E5%85%83%E5%93%B2%E5%AD%A6">后设哲学</a></li><li><a href="https://zh.wikipedia.org/wiki/%E5%AE%97%E6%95%99%E5%93%B2%E5%AD%A6">宗教哲学</a></li><li><a href="https://zh.wikipedia.org/wiki/%E5%BF%83%E9%9D%88%E5%93%B2%E5%AD%B8">心灵哲学</a></li><li><a href="https://zh.wikipedia.org/wiki/%E8%AF%AD%E8%A8%80%E5%93%B2%E5%AD%A6">语言哲学</a></li><li><a href="https://zh.wikipedia.org/wiki/%E7%A7%91%E5%AD%A6%E5%93%B2%E5%AD%A6">科学哲学</a>： 现代西方科学哲学的中心是<strong>科学方法论</strong>问题，具体包括<ul><li>统计哲学：统计假设检验（证伪），贝叶斯</li><li>数学哲学：</li><li>物理哲学</li><li>化学哲学</li><li>生物哲学</li><li>医学哲学</li><li>心理学哲学</li><li>经济哲学</li><li>社会科学哲学：代表之一---马克思</li></ul></li><li><a href="https://zh.wikipedia.org/wiki/%E6%94%BF%E6%B2%BB%E5%93%B2%E5%AD%A6">政治哲学</a></li><li><a href="https://zh.wikipedia.org/wiki/%E6%B3%95%E5%BE%8B%E5%93%B2%E5%AD%B8">法律哲学</a></li></ul><h2 id="西方的-科学哲学发展路径">西方的 科学哲学发展路径</h2><ul><li>到50年代为止，一直是作为“正统科学哲学”的<strong>逻辑实证主义</strong>占主导地位</li><li>从50年代末兴起，在60年代发展和完成：波普尔的<strong>批判理性主义</strong>和库恩-拉卡托斯的<strong>历史主义</strong>科学哲学。</li><li>至70年代，出现了两股发展趋势。<ul><li>一股是<strong>复兴</strong>和发展“正统<strong>的”逻辑主义方法论</strong>，</li><li>另一股就是法伊尔阿本德的<strong>非理性主义</strong>，它在很大程度上是把<strong>历史主义</strong>中的非理性因素贯彻到极端程度的产物。</li></ul></li><li>本书对<strong>逻辑实证主义</strong>和波普尔、库恩与拉卡托斯的科学哲学等现代西方理性主义科学哲学学说一一作了批判，并在这个批判中阐发了自己的科学哲学——<strong>多元主义方法论</strong></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;定义&quot;&gt;定义&lt;/h1&gt;
&lt;p&gt;哲学，就我对这个词的理解来说，乃是某种介乎神学与科学之间的东西。它和神学一样，包含着人类对于那些迄今仍为科学知识所不能肯定之事物的思考；但它又像科学一样，是诉之于人类的理性而不是诉之于权威的，不论是传统的权威还是启示的权威。一切确切的</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="哲学" scheme="https://chiechie.github.io/tags/%E5%93%B2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>关于AGI？</title>
    <link href="https://chiechie.github.io/2021/06/17/meditation/about-AGI/"/>
    <id>https://chiechie.github.io/2021/06/17/meditation/about-AGI/</id>
    <published>2021-06-17T09:04:32.000Z</published>
    <updated>2021-06-28T02:11:37.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="chiechies-reflection">chiechie's reflection</h2><ol type="1"><li>理论突破不是技术爆发的必经之路，很多技术都没有理论，比如古代时候的发明，造船，造纸，造火药，爱迪森发明电灯泡，还有现代的量子计算机和深度学习，理论完备个P。</li><li>大部分发明都是做实验，通过试错迭代而产生的,而很多情况下理论都是滞后的。</li></ol><h2 id="无人驾驶现状">无人驾驶现状</h2><ol type="1"><li>一个技术在工业化生产之前，一般要经历三个阶段, 理论突破，技术验证，工程化<ol type="1"><li>理论突破，科学家在理论上证明其可行性，即产品要达到一个什么样的性能，在技术上是一定可以实现的；</li><li>技术突破，研究机构突破技术实现上壁垒，做出来达到预期的Demo；</li><li>工程化，主要解决产品设计，方案优化，功能完备，性能提升，良品率，鲁棒性，可用性提升，大规模复制的技术准备，成本降低等工程问题。</li></ol></li><li>通用无人驾驶在第一，第二阶段还彻底整明白的情况下，由资本驱动直接进入第三阶段.</li></ol><h2 id="agi的理论先行者">AGI的理论先行者</h2><ol type="1"><li>目前强化学习是最被看好的方向。</li><li>强化学习本质上是演化论的思路，跟自然界一样，给定一个reward，让机器放肆的学习探索。</li><li>目前还需要解决的问题是计算资源的问题。</li><li>自然界的演化大部分时间是平稳状态，只有极少数黑天鹅事件决定了演化的方向。</li><li>在计算机中模拟agent的演化过程，和自然界的演化类似，需要忍受长期的无秩序无进展。</li><li>如果全世界的资源往这方面倾斜，可能可以加快黑天鹅事件出现。</li><li>理论突破不是AGI成熟的必要条件，有可能是2者互相促进发展。</li><li>目前收到关注较多的公司--自动标注样本公司scale</li></ol><h2 id="参考">参考</h2><ol type="1"><li>https://www.zhihu.com/question/404870865/answer/1324577689</li><li>https://www.zhihu.com/question/464616760/answer/1940847401</li><li><a href="https://www.sciencedirect.com/science/article/pii/S0004370221000862">reward is enough ,david silver</a></li><li><a href="https://www.notboring.co/p/scale-rational-in-the-fullness-of?token=eyJ1c2VyX2lkIjoxNzY2NjQ2OSwicG9zdF9pZCI6Mzc4NDY2MzEsIl8iOiJrTHFWcCIsImlhdCI6MTYyNDQyNDU4NCwiZXhwIjoxNjI0NDI4MTg0LCJpc3MiOiJwdWItMTAwMjUiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.zWmuXLn35R720iDVtX7yTuTpM4kCMk25457XzZN8_Ks">not boring</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;chiechies-reflection&quot;&gt;chiechie&#39;s reflection&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;理论突破不是技术爆发的必经之路，很多技术都没有理论，比如古代时候的发明，造船，造纸，造火药，爱迪森发明电灯泡，还有现代的量子计算机</summary>
      
    
    
    
    <category term="沉思录" scheme="https://chiechie.github.io/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="人工智能" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="科技史" scheme="https://chiechie.github.io/tags/%E7%A7%91%E6%8A%80%E5%8F%B2/"/>
    
    <category term="人工智障" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E9%9A%9C/"/>
    
  </entry>
  
  <entry>
    <title>关于Scale AI</title>
    <link href="https://chiechie.github.io/2021/06/16/reading_notes/reality/about-scale/"/>
    <id>https://chiechie.github.io/2021/06/16/reading_notes/reality/about-scale/</id>
    <published>2021-06-16T02:39:20.000Z</published>
    <updated>2021-06-30T07:30:46.238Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>看到Packy激情满满写了一篇关于Scale AI的公关文，比较好奇这个年轻的小公司有何特别之处，为什么会受到资本的一致认可。</p><p>印象中Scale AI就是一家硅谷的数据标注公司，主要靠外包给第三方国家，赚取中间费用，这种公司有什么竞争力呢？跟大厂比起来？</p></blockquote><h2 id="总结">总结</h2><ol type="1"><li><p>Scale AI创立至今5年，最近一次估值73亿$。它跟stripe有点像，在一个蓬勃发展的行业，找到一个关键但是不起眼的小地方，深耕细作。</p></li><li><p>刚开始成立的时候，Scale AI专注做数据标注，但是他们的野心不限于此，他们想做更好用的数据标注工具，即人机互助的方式，Human-in-the-Loop（HIL）。</p></li><li><p>Scale AI的故事是：随着标记数据越来越多，标注数据的边际成本会降低，因为标记样本越多，标记机器人的性能越好，能hold的事情就越多，需要分配出去的工作就越少。</p><blockquote><p>这里有一个问题，怎么确定，哪些是机器可以搞定的，哪些是机器搞不定的？这个是不是又要多一层人工监督了？</p><p>可以从环境中获取到反馈，也可以人工抽查。</p><p>总的来说，都不是很完善的方案。</p></blockquote></li><li><p>Scale AI还有一个亮点，他的客户群体很分散，从国防部到无人驾驶公司，eg OpenAI, Airbnb and Lyft。这有一个好处就是。AI公司去去留留，竞争之后，留下了最有效的ml应用场景，但是Scale AI始终有机会，因为是做基础设施的。</p></li><li><p>除了标注API，Scale AI还做了一个样本调试的产品--Nucleus，data debugging SaaS product。目的是做全链路基建嘛。 <img src="./img.png" alt="Nucleus" /></p></li><li><p>Scale AI的CEO Alexandr Wang 说的一段话</p><blockquote><p>At Scale, we’re building the foundation to enable organizations to manage the entire AI lifecycle. Whether they have an AI team in-house or need a fully managed models-as-a-service approach, we partner with our customers to build their strategy from the ground up and ensure they have the infrastructure in place to systematically deliver highly-performant models.</p></blockquote></li></ol><p>简单点说，他们的方法是，帮助企业从0到1落地一个AI应用--不管这个企业内部有没有AI团队--企业能够使用这个基建更高效地构建模型. 就是win-win,后生可畏.</p><ol start="10" type="1"><li><p>对于没有AI团队的传统企业，Scale AI定制化合作（跟必示一样），客户例如Brex和Flexport。</p><figure><img src="./img1.png" alt="img.png" /><figcaption aria-hidden="true">img.png</figcaption></figure></li><li><p>这里有一个小故事：Brex doesn’t have an AI team working on the model; they outsource it to Scale. .Brex一开始找的是专门做OCR的大公司，帮他们做发票文本提取，但是效果一般（“they were all mediocre.” ）。后面找到scale，通过深入合作定制化了一个模型，效果100%。后面沉淀出了一个产品document。</p></li><li><p>scale的产品还有蛮多的：地图/文档/图像。。 <img src="img2.png" alt="img.png" /></p></li></ol><blockquote><p>btw, 知乎上有一个相关提问--如何评价Scale AI？大家似乎认识非常有限（只知道它是做数据标注的），还讨论的热火朝天。由此可见的，噪声膨胀的速度远远超过信息增长的速度。</p></blockquote><h2 id="chiechies-reflection">chiechie's reflection</h2><ol type="1"><li><p>packy这篇文章的公司分析框架蛮好的，后面分析科技公司可以套用：</p><ul><li>行业介绍：The State of AI and ML</li><li>公司介绍：Getting to Scale.</li><li>相同模式的成功案例对比：Scaling Like Stripe.</li><li>关于公司的负面观点：The Bear Case for Scale.</li><li>关于公司的正面观点：The Bull Case for Scale.</li><li>前景展望：Scale’s Compounding Vision.</li></ul></li><li><p>AI市场未来还有这么大的成长空间，当前只有8%的公司应用了AI技术,看到这个数字有点吃惊。但是，即便如此，AI从业人员的需求也不用过分乐观估计。</p></li><li><p>AI应用目前还在探索期。Scale AI比较鸡贼，让AI公司在前面探路，让他们相互pk，把有价值可落地的场景摸索清楚，自己在后面提供军火库，妥妥的赢家。</p></li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.notboring.co/p/scale-rational-in-the-fullness-of">not boring</a></li><li><a href="https://zhuanlan.zhihu.com/p/384012257">吴恩达发起新型竞赛范式！模型固定，只调数据？</a></li><li><a href="https://scale.com/blog/series-e">Scale AI’s Series E: Deploying AI Across Every Industry</a></li><li><a href="https://dashboard.scale.com/nucleus/">nucleus</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;看到Packy激情满满写了一篇关于Scale AI的公关文，比较好奇这个年轻的小公司有何特别之处，为什么会受到资本的一致认可。&lt;/p&gt;
&lt;p&gt;印象中Scale AI就是一家硅谷的数据标注公司，主要靠外包给第三方国家，赚取中间费用，这种公司有什么竞争</summary>
      
    
    
    
    <category term="投资" scheme="https://chiechie.github.io/categories/%E6%8A%95%E8%B5%84/"/>
    
    
    <category term="人工智能" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="投资" scheme="https://chiechie.github.io/tags/%E6%8A%95%E8%B5%84/"/>
    
    <category term="行业研究" scheme="https://chiechie.github.io/tags/%E8%A1%8C%E4%B8%9A%E7%A0%94%E7%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯网络推断</title>
    <link href="https://chiechie.github.io/2021/06/15/data_structure/graph/bayesnetwork/"/>
    <id>https://chiechie.github.io/2021/06/15/data_structure/graph/bayesnetwork/</id>
    <published>2021-06-15T06:28:18.000Z</published>
    <updated>2021-06-28T01:34:19.335Z</updated>
    
    <content type="html"><![CDATA[<p>如何应用贝叶斯网络来做决策？</p><ul><li>首先需要将先验知识表达成一个因果图--贝叶斯网络，（准确来说贝叶斯网络的边不仅仅表达因果关系，他表达所有的信息传播的方向）。</li><li>做推断</li></ul><ol type="1"><li><p>构造贝叶斯网络 <img src="img.png" alt="img.png" /></p></li><li><p>推断</p></li></ol><p>已知P(R,W,S,C）, 求P(r)</p><h3 id="枚举法">枚举法</h3><ol type="1"><li>先推到条件概率分布</li></ol><figure><img src="img_1.png" alt="img_1.png" /><figcaption aria-hidden="true">img_1.png</figcaption></figure><ol start="2" type="1"><li><img src="img_2.png" title="fig:" alt="img_2.png" /></li></ol><h3 id="变量消除法">变量消除法</h3><figure><img src="img_3.png" alt="img_3.png" /><figcaption aria-hidden="true">img_3.png</figcaption></figure><figure><img src="img_4.png" alt="img_4.png" /><figcaption aria-hidden="true">img_4.png</figcaption></figure><p>所有非query变量祖先的变量，都应该被消去，当然不是真的消去啦，是对该消去变量求和，然后变成一个新的因子f。</p><p>依次迭代，直到没有可以消除的变量。</p><figure><img src="img_5.png" alt="img_5.png" /><figcaption aria-hidden="true">img_5.png</figcaption></figure><h3 id="贝叶斯网络中的依赖性">贝叶斯网络中的依赖性</h3><ol type="1"><li>每一个随机变量都是条件独立于他的非后代节点，给定他的父母节点时，</li><li>如下，给定A，B时，C和D是独立的。</li></ol><figure><img src="img_6.png" alt="img_6.png" /><figcaption aria-hidden="true">img_6.png</figcaption></figure><ol start="3" type="1"><li>每个随机变量独立其他任何变量，当给定他的马尔可夫毯（Markov Blanket）<ul><li>父亲，孩子，以及孩子的父母亲（配偶）</li></ul></li></ol><figure><img src="img_7.png" alt="Markov Blanket" /><figcaption aria-hidden="true">Markov Blanket</figcaption></figure><h2 id="参考">参考</h2><ol type="1"><li><a href="https://www.youtube.com/watch?v=TuGDMj43ehw">Bayesian Networks-youtuybe</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如何应用贝叶斯网络来做决策？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先需要将先验知识表达成一个因果图--贝叶斯网络，（准确来说贝叶斯网络的边不仅仅表达因果关系，他表达所有的信息传播的方向）。&lt;/li&gt;
&lt;li&gt;做推断&lt;/li&gt;
&lt;/ul&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;构</summary>
      
    
    
    
    <category term="数据结构" scheme="https://chiechie.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="贝叶斯网络" scheme="https://chiechie.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>为什么要工作？</title>
    <link href="https://chiechie.github.io/2021/06/14/meditation/why-do-We-work/"/>
    <id>https://chiechie.github.io/2021/06/14/meditation/why-do-We-work/</id>
    <published>2021-06-14T09:28:02.000Z</published>
    <updated>2021-06-28T00:33:05.522Z</updated>
    
    <content type="html"><![CDATA[<p>类似的问题，</p><ul><li>我们高中为什么要学数理化和文言文？长大之后并没有用上。</li><li>面试造火箭工作拧螺丝。</li></ul><p>工作除了挣钱，还能带来点别的东西，这个东西是什么呢？似乎做完了才知道。</p><p>最近的一份工作，本来是想提升技术，但是事实上，并没有磨练技术的机会，大部分时间是在做产品设计。做产品设计的过程中，越做越困惑，本能地想要找到答案，先在本领域里面寻找，但是大部分所谓的专家，写的文章都是空洞的方法论，言之无物。我不满意，遂去寻找其他领域的文章，比如哲学，社会科学，心理学，经济学。我发现，产品设计上遇到的一些问题，追问到其本质，似乎跟其他领域也想通，原来大家都在研究差不多的问题。</p><p>潜移默化，思考问题的方式发生了改变，从追求触手可及的答案（捷径往往不是正确的道路），到追求更深层次的更本质的问题，当然也更花时间和精力。 从之前迷信权威，到现在delay judgement。</p><p>得知东隅，失之西隅。</p><p>再想一想高中学习的那些东西，可能真不见得有什么实际收益，但是在不可见的维度，可能还是有收获的。比如磨练了我们的意志力，让我们做事情能够吃苦耐劳，或者对性格的其他方面有一些潜在的影响。这个影响显然不是设计高考制度的人想出来的，而是人在做事，在实践的过程中，意外得到的，可能去种地也会起到同样的作用。只有事后，自己才可以总结出来。</p><p>一些联想：观察到事件A和事件B在现实中存在某种若隐若现的关联时，不一定A和B就有直接的因果关系。背后可能存在着某个状态不可见的混杂因子。很多情况下，因为我们没有足够的人生经验，所以做事之前，我们都不知道这个混杂因子是个什么，做完之后才恍然大悟。</p><p>因缘际会，凡事用心投入，大概率会是有好结果的吧，这个结果可能发生在另外一件事情上。</p><p>"做之前想的再多没用，做着做着就知道意义了"。现在似乎有点明白这个意思，人的能力太有限，世界变幻莫测，哪有可能预测到事情的演化方向。直到走完这条路，并且回过头来看这条路，才知道终点是什么，才能总结出这一路的意义是什么。</p><blockquote><p>莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？ 一蓑烟雨任平生。</p><p>料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。</p><p>-- 定风波 苏轼</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;类似的问题，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们高中为什么要学数理化和文言文？长大之后并没有用上。&lt;/li&gt;
&lt;li&gt;面试造火箭工作拧螺丝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;工作除了挣钱，还能带来点别的东西，这个东西是什么呢？似乎做完了才知道。&lt;/p&gt;
&lt;p&gt;最近的一份工作，本来是</summary>
      
    
    
    
    <category term="沉思录" scheme="https://chiechie.github.io/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="阅读" scheme="https://chiechie.github.io/tags/%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>《社会心理学》读书笔记</title>
    <link href="https://chiechie.github.io/2021/06/13/reading_notes/reality/shehuixinlixue/"/>
    <id>https://chiechie.github.io/2021/06/13/reading_notes/reality/shehuixinlixue/</id>
    <published>2021-06-13T01:24:54.000Z</published>
    <updated>2021-06-28T00:33:05.525Z</updated>
    
    <content type="html"><![CDATA[<h2 id="社会中的自我">1. 社会中的自我</h2><h3 id="焦点效应">焦点效应</h3><ol type="1"><li>人总是高估他们内在状态外露的程度，但是事实上，他人并没有你想象的那样注意你。</li><li>焦点效应（spotlight effect）：把自己看作世界中心，直觉地高估别人对我们的关注程度。几个例子：</li></ol><ul><li>会聚焦在自我与当前社会环境的差异性，会关注差别最大的那个维度。</li><li>自我服务：当关系出现问题时，个体通常会把责任更多推到配偶身上，情况好转时，确认为自己起了更重要的作用。</li><li>自我关注引发的很多行为：我们关注他人的行为和期望，并随之调整自己的行为。</li><li>在不同的关系中，自我也在发生变化。</li></ul><ol start="2" type="1"><li><p>透明度错觉（illusion of transoarency）：我们能敏锐觉察到自己的情绪，很自然地表现出来，并且直觉的认为别人会通过我们的表情觉察到我们的情绪。</p><blockquote><p>其实不是的，1是我们的表现可能模糊不清，2是别人的关注点可能在他自己。</p></blockquote></li><li><p>公众心理疏忽（public mental slips）：如果我们五一冒犯了别人，我们可能懊恼，但是别人经常注意不到，即使注意到也可能很快就会忘记。</p></li><li><p>我们的大多数行为不受意识控制，是自动。显示的自我会制定长期计划，设定目标和进行约束，设想各种可能，将自己和他们相比较，管理自己的reputation和社会关系，但是更多时候可能成为幸福的障碍。</p></li></ol><h3 id="我是谁">我是谁</h3><ol type="1"><li>self-schemas：对自己的认识，比如聪明，自省，勤奋，怀疑，个人主义。</li><li>self-schemas影响我们对社会信息的加工，也影响我们感知，回忆和评价他人/自己。</li><li>我们对跟self-schemas相关的信息会特别关注。</li><li>possible selves：可能的自我，梦想中自我的样子，比如富有，充满激情，自信，丛容，智慧。</li><li>possible selves会激发一种我们渴望的生活愿景，对我们产生巨大的激励作用，或促进我们避免成为自己害怕的样子。</li></ol><h3 id="社会自我">社会自我</h3><ol type="1"><li>基因对人格和自我概念有重要的影响，社会经验也有：扮演的角色，社会同一性，和别人的比较，成功和失败，他人如何评价我们，周围的文化。</li><li>扮演的角色：我们为这个角色说了很多好话，不知不觉，我们会越来越相信这些话，为这些话提供证据。就这样，角色扮演变成了事实。</li><li>社会比较：判断自己是否聪明/智慧？通过社会比较。生活的大部分是围绕比较进行的，因此人们可能会因为别人的失败而暗自高兴。当人嗯攀爬成功的阶梯时，通常会向上看，将自己与做的更好的人比。面对竞争时，常常认为竞争对手本来就有一些优势，以此来保护我们业已动摇的自尊。</li><li>别人的评价：镜像自我--别人对我们的看法，我们把别人作为镜子，我们认为的别人眼中的我，据此来认识自己。换句话说，重要的不是别人实际上如何评价我们，而是我们想象中他们如何评价我们。</li></ol><h3 id="自我与文化">自我与文化</h3><ol type="1"><li>对于部分群体，“个人主义”十分盛行，他们的额经历大部分是这样：青春期与负米分离，各异开始依靠自己，定义独立的自我。即使个体来到一片陌生的土地上，他的特性：有特殊能力，特点，价值和梦想的个体也会保留。当经历过富裕，地位改变，城市化和大众传媒后，个人主义开始迅速发展，</li><li>亚洲文化则更重视集体主义（collectivism）。</li><li>保守派，经济上的个人主义（不要征税）和道德上的集体主义（制定法律来约束不道德）</li><li>自由主义者，经济上的集体主义（支持全民健康保障）和道德上的个人主义（不要用法律来约束我）</li><li>许多文化似乎正在走向个人主义。</li><li>究竟是人们更加关注自我，所以喜欢听关注自我的歌曲，还是反过来？还是存在混杂因子？</li></ol><h3 id="文化与认知">文化与认知</h3><ol type="1"><li>东亚人的思维更具有整体性，从人际关系和环境的角度思考人和物。东方人不重视表达自己的独特性，更重视分享，很少强调个人的选择和自由。</li><li>西方的文化强调个体的力量，个体的价值。</li><li>相互依赖的自我是多个自我的组合，如为人子女的我，职场中的我，作为朋友的我。一个具有相互依赖自我的人会有强烈的归属感，当跟组织分开之后，会丢失掉自我定义。相互依赖的自我存在于社会关系中，社会生活的目标是协调和支持群体。相互依赖的自我聚焦寻求社会支持，比独立自我更深入地融入他人。在相互依赖的文化中，</li><li>自我概念会适应环境，如果一生都与一群人交往，身边人对你的影响很重要；反之，如果每几年换一个环境，身边的人就没有那么重要，而「自我」变成忠实伙伴。“无论你去哪儿，你就是你”。</li></ol><h3 id="自我认识">自我认识</h3><ol type="1"><li>我们对自己行为的解释，在原因有点微妙时候，通常答案都是错误的，我们会忽视重要因素，而夸大一些无关紧要的因素。</li></ol><h2 id="社会关系">社会关系</h2><ol type="1"><li>内疚感：没有帮助别人或者说谎时，会产生内疚感，人会通过在其他方面的行动来弥补这一种感觉。</li><li>经历过极度悲痛的人，会经历一种强烈的自我关注时期。</li><li>悲痛并且自我关注的人，较少意愿去帮助他人；悲痛并且关注别人的人，较多意愿去帮助他人。</li><li>快乐的人更愿意帮助别人。</li><li>帮助行为可以缓解不好的情绪，也可以维持好的情绪。比如给某人指路之后，自我感觉会更好。接着，好的心情又回产生积极的思维，从而指导在其他事情上产生积极的行为。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;社会中的自我&quot;&gt;1. 社会中的自我&lt;/h2&gt;
&lt;h3 id=&quot;焦点效应&quot;&gt;焦点效应&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;人总是高估他们内在状态外露的程度，但是事实上，他人并没有你想象的那样注意你。&lt;/li&gt;
&lt;li&gt;焦点效应（spotlight effe</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="心理学" scheme="https://chiechie.github.io/tags/%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>《行为经济学》读书笔记</title>
    <link href="https://chiechie.github.io/2021/06/12/reading_notes/economics/xingweijingjixue/"/>
    <id>https://chiechie.github.io/2021/06/12/reading_notes/economics/xingweijingjixue/</id>
    <published>2021-06-12T01:26:37.000Z</published>
    <updated>2021-06-28T00:33:05.523Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第二讲">第二讲</h1><h2 id="第二部分-社会网络研究">第二部分 社会网络研究</h2><h3 id="选择选择什么">选择选择什么？</h3><ol type="1"><li>“群体选择”:群体之间的竞争可能使内部合作的群体最终胜出并淘汰那些内部不合作的群体.群体选择这一主题兼具个体视角和群体视角。</li><li>"利他行为"：体为增加群体的适存度而降低自己的适存度，它的行为是利他行为。适存度，fitness。</li><li>生物学家怎样测度呢？他们用后代与母代之比，来测度适存度。如果一个人的生存策略有利于增加自己的适存度，那么在一名旁观者看来，他的后代的数目必须逐渐超过母代的数目，这就是繁衍，否则就是消亡</li><li>行为模式是可观测的，行为动机是不可观测的。策略是对行为模式的一种描述，所以是可观测的。心理学描述心理事实，包括动机，通常是不可观测的。如生物学家那样，行为经济学试图做的，是从可观测的“显型”策略，推测那些不可观测的“基因型”行为动机。</li><li>什么是传统：：一切经过长期历史的检验有利于生存和繁衍的知识。其中可交流的传统就是常识。传统的大部分不能交流，只能模仿。</li><li>为什么蚂蚁懂得合作没有淘汰人类？蚂蚁行为的驱动力量是族群惯性，但是他没有灵活性。人的行为驱动力量一部分是族群惯性，一部分是灵活性，这样就使得可以保持创新的同时，又能够保持一定的合作秩序。</li><li>在古尔德看来，漫长的地球演化史，首先由一些漫长的稳态构成，其次，这些稳态之间有一些瞬间的黑天鹅事件打破既有的均衡态，然后陷入新的均衡态，再等待新的黑天鹅事件</li><li>人类的出现，就是其中一个黑天鹅事件，它只占演化史的一个瞬间，是大约900万年以内的事情</li><li>古尔德的结论是：历史从来不是决定论的，由许多偶然因素决定历史路径，只在事后才可能知道 自然选择的力量从来不是惟一的演化力量，类似地，为什么最优越的物种只在以往漫长历史的最后一秒之内发生呢？</li><li>在多因多果的网络里，科学家怎样令人信服地指出哪一条因果链条是最重要的呢？理想方法是通过实验，在实验中，我们可以只让一条因果链发生作用，我们控制所有其他因果关系不许它们起作用</li></ol><h3 id="社会网络研究什么">社会网络研究什么？</h3><ol type="1"><li>如何界定一项知识的重要性？用途。手段的质，依赖于目的本身。</li></ol><h3 id="爱因斯坦的自由论">爱因斯坦的自由论</h3><ol type="1"><li>整体论可爱，但不可信，因为它缺少可行的研究方法。个体论可信，却不可爱。</li><li>爱因斯坦:创造是个体的，从来就是个体行为。但是自由，却是整体的. 没有宽容的社会，个人自由也将消失。</li></ol><h3 id="哈耶克的涌现秩序">哈耶克的涌现秩序</h3><ol type="1"><li><p>马克思理论的逻辑矛盾在第一卷里尚未出现，非要在第二卷和第三卷才成为无法挣脱的内在矛盾。</p></li><li><p>马克思主义和哈耶克：哈耶克论证，没有一个微观主体（个体或群体），不论多么聪明，有能力预先知道从大量哪怕是极简单的微观主体的相互作用中涌现出来的宏观秩序是怎样的。这当然意味着社会计划的不可能和理性的狂妄。理性实在很渺小，不可能完成社会主义者赋予它的资源有效配置任务。</p></li><li><p>人类互动问题的复杂性，时间与结构的复制以及涌现，宏观秩序的涌现</p></li><li><p>哈耶克的“涌现秩序"：演化理论，假以时日，从大量的简单局部结构之间的相互作用中必能涌现更复杂的结构。</p><blockquote><p>涌现秩序的思想，可以溯源至柏格森和怀特海。</p></blockquote></li><li><p>哈耶克使用的关键词：简单结构、复制过程、优胜劣汰、适应性。</p></li><li><p>哈耶克的社会演化基本原理：单子复杂交往涌现宏观秩序的不可预见性。</p><ul><li>“单子”，是莱布尼茨的概念，它们如“素数”那样单纯，具有不可再分性。只不过，莱布尼茨假设单子之间没有交往，而哈耶克假设单子之间的交往是事物演化的原因。</li></ul></li><li><p>多主体仿真（multi-agent simuiation）:一种模拟不确定性的环境与社会变迁的程序.哈佛的诺瓦克小组在使用软件仿真时，将每一个agent都设置为具有最简单的行为规则。然后，一个环境中有数千个agent，相互作用之后，宏观秩序涌现出来，这个宏观秩序是不可预期的。最初研究这样的仿真程序的，是2005年与奥曼分享诺贝尔经济学奖的谢林，这一程序称为“谢林程序”。</p></li><li><p>《解释的程度》中，哈耶克的论述十分接近“社会现象是多因多果”这一见解。</p></li><li><p>《复杂现象论》中，哈耶克全面论述了他设想的演化理论的各要素。目前，社会网络的研究方法，是最适合研究哈耶克涌现秩序的实证方法。</p></li><li><p>哈耶克对于统计学的看法：通过消除复杂性来处理大量数据。</p></li></ol><h3 id="多因多果模型">多因多果模型</h3><ol type="1"><li><p>多因多果网络中，那一条因果链条是重要的？以及如何论证出这个结论是对的？前者靠直觉，或者说taste/判断力。后者用控制变量的方法。</p><blockquote><p>社会心理学和行为经济学都会用到控制变量的方法来确定因果关系。</p><p>判断力哪里来？靠日积月累的实践。</p></blockquote></li><li></li></ol><h1 id="第三讲">第三讲</h1><ol type="1"><li>杨格的“策略学习的不可能性定理”：</li></ol><h1 id="参考">参考</h1><ol type="1"><li><a href="https://weread.qq.com/web/reader/b48321a058a8aeb48d182ac">行为经济学讲义-演化论视角</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;第二讲&quot;&gt;第二讲&lt;/h1&gt;
&lt;h2 id=&quot;第二部分-社会网络研究&quot;&gt;第二部分 社会网络研究&lt;/h2&gt;
&lt;h3 id=&quot;选择选择什么&quot;&gt;选择选择什么？&lt;/h3&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;“群体选择”:群体之间的竞争可能使内部合作的群体最终胜出并淘汰那些</summary>
      
    
    
    
    <category term="经济学" scheme="https://chiechie.github.io/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
    
    <category term="经济学" scheme="https://chiechie.github.io/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
    <category term="行为经济学" scheme="https://chiechie.github.io/tags/%E8%A1%8C%E4%B8%BA%E7%BB%8F%E6%B5%8E%E5%AD%A6/"/>
    
    <category term="幂律" scheme="https://chiechie.github.io/tags/%E5%B9%82%E5%BE%8B/"/>
    
    <category term="哈耶克" scheme="https://chiechie.github.io/tags/%E5%93%88%E8%80%B6%E5%85%8B/"/>
    
    <category term="群体选择" scheme="https://chiechie.github.io/tags/%E7%BE%A4%E4%BD%93%E9%80%89%E6%8B%A9/"/>
    
  </entry>
  
  <entry>
    <title>关于图数据结构，图模型和图算法</title>
    <link href="https://chiechie.github.io/2021/06/11/meditation/graph-summary/"/>
    <id>https://chiechie.github.io/2021/06/11/meditation/graph-summary/</id>
    <published>2021-06-11T10:42:27.000Z</published>
    <updated>2021-06-26T10:56:35.487Z</updated>
    
    <content type="html"><![CDATA[<ol type="1"><li>图数据是一种数据结构，对于图数据有一些常见的任务，比如单源最短路径，等。</li><li>当图数据的点和边具备一定的含义，如边代表概率，这个时候就是一个概率图模型。通常用于对多个变量之间的因果关系或者相关关系进行建模。</li><li>概率图模型可以细分为有向图和无向图。有向图比如贝叶斯网络，无向图比条件随机场（CRF）。</li><li>有向图又可进一步细分，如果边的方向代表因果关系，就是一个因果图。通常因果图都是需要专家构建的。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;图数据是一种数据结构，对于图数据有一些常见的任务，比如单源最短路径，等。&lt;/li&gt;
&lt;li&gt;当图数据的点和边具备一定的含义，如边代表概率，这个时候就是一个概率图模型。通常用于对多个变量之间的因果关系或者相关关系进行建模。&lt;/li&gt;
&lt;li&gt;概率</summary>
      
    
    
    
    <category term="沉思录" scheme="https://chiechie.github.io/categories/%E6%B2%89%E6%80%9D%E5%BD%95/"/>
    
    
    <category term="人工智能" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="图数据" scheme="https://chiechie.github.io/tags/%E5%9B%BE%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础1 数据表示和神经网络基础</title>
    <link href="https://chiechie.github.io/2021/06/10/AI/machine_learning/deeplearning/dl-basic0/"/>
    <id>https://chiechie.github.io/2021/06/10/AI/machine_learning/deeplearning/dl-basic0/</id>
    <published>2021-06-10T08:35:36.000Z</published>
    <updated>2021-06-24T01:38:53.493Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据表示">数据表示</h1><p>在深度学习中如何表示现实中的事物？</p><p>Let’s make data tensors more concrete with a few examples similar to what you’ll encounter later. The data you’ll manipulate will almost always fall into one of the following categories:</p><ul><li>Vector data:2D tensors of shape(samples, features)</li><li>Timeseries data or sequence data:3D tensors of shape (samples, timesteps, features)</li><li>Images:4D tensors of shape(samples,height,width,channels) or (samples,channels, height, width)</li><li>Video:5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)</li></ul><h2 id="表数据">表数据</h2><ul><li>两个轴：samples axis 和 features axis.</li><li>文本数据, 假设词典长度为2k，每一个doc可以表示为1个2k维的向量，位置的值代表词在文本中出现的次数。</li><li>500个文件可以存储为(500, 20000).</li></ul><h2 id="时间序列数据">时间序列数据</h2><ul><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2F_vMGBboznU.png?alt=media&amp;token=73c12f97-acdb-4680-b4eb-79a9a07581f9" /></li></ul><p>时间序列数据一般表示成3-d的张亮，tf中使用3d张量存储，(samples, timestamp, features) 每一个sample可以被编码成一个2d张量， 具体的两个例子</p><ol type="1"><li>股票数据：每年有250个交易日，每个交易日的交易时长有390分钟，每个分钟可以抽取3个重要特征：当前价格，上一分钟最高成交价格，上一分钟最低价格<ul><li>以每一天的交易数据为1个样本，构建的样本的shape为(250,390,3)</li></ul></li><li>TWEET数据：一条twitter长度不超过256，每个位置的字符来自128个assical码中的一个。每一条twitter的shape为（256， 128），1 百万 tweets 的shape为(1000000, 280, 128)</li></ol><h2 id="图像数据">图像数据</h2><p>图像数据一般表示成4维tensor，一个图像数据就是一个3d张量</p><p><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FD24ghIvE2L.png?alt=media&amp;token=4a68bc5a-7b95-46d2-8418-deed053d4301" /></p><ul><li>tensorflow： (samples, height, width, color_depth)</li><li>theano：(samples, color_depth, height, width).</li><li>keras两者都支持</li></ul><h2 id="视频数据的表示">视频数据的表示</h2><p>视频数据表示成5维张量：(samples, frames, height, width, color_depth).</p><h1 id="模型超参">模型超参</h1><h2 id="结构超参数">结构超参数</h2><p>结构超参数包括层数、层的类别、层的大小等数值。以一个卷积层为例，其中的超参数包括卷积核 (filter) 的大小，卷积核的数量，步长 (stride) 的大小，unit：单元，hidden_layers: 隐藏层，cell：单元 ，dropout_rate丢弃率。这些超参数决定了神经网络的结构。</p><h3 id="layers的基本类型">layers的基本类型</h3><p>keras中layer的基本类型：</p><ul><li>卷积(Conv1D)</li><li>池化(MaxPooling1D/GlobalAveragePooling1D)</li><li>dropout(Dropout)</li><li>线性全连接层(Dense)</li><li>high-level层(LSTM)</li></ul><h3 id="卷积层">卷积层</h3><ol type="1"><li>卷积的维度有1Ｄ，2Ｄ，3D, 区别在于?</li></ol><p>输入数据的shape以及卷积核如何滑动</p><ul><li><img src="https://miro.medium.com/max/1621/1*aBN2Ir7y2E-t2AbekOtEIw.png" title="fig:" alt="I１维卷积和２维卷积的区别" /></li><li>1维卷积（Conv1D）：只有1个维度可以滑动，但是另一维度也是有待估参数的</li><li>2维卷积（Conv2D）：有两个维度可以滑动，</li><li>相同点：从参数视角都是2维的，每一个卷积kernel，参数个数是 height × width +１　（off_ set）</li></ul><ol start="2" type="1"><li>什么情况下，1d比2d好用呢？其中2个维度上，做卷积没有意义(high,close,open,close)</li></ol><h3 id="关于dropout">关于dropout</h3><p>keras中对dropout的处理，因为训练阶段需要dropout ，但是inference阶段不需要dropout，keras中如何设置？</p><p>Keras does this by default. In Keras dropout is disabled in test mode. You can look at the code<a href="https://github.com/keras-team/keras/blob/dc95ceca57cbfada596a10a72f0cb30e1f2ed53b/keras/layers/core.py#L109">here</a> and see that they use the dropped input in training and the actual input while testing.</p><p>As far as I know you have to build your own training function from the layers and specify the training flag to predict with dropout (e.g. its not possible to specify a training flag for the predict functions). This is a problem in case you want to do GANs, which use the intermediate output for training and also train the network as a whole, due to a divergence between generated training images and generated test images.</p><h2 id="算法超参数">算法超参数</h2><p>算法超参数包括学习率 (learning rate),批量大小 (batch size)、epoch数量(迭代周期个数)、正则，损失，激活函数等。由于神经网络的非凸性，用不同的算法超参数会得到不同的解。</p><h3 id="归一化">归一化</h3><p>归一化层，目前主要有这几个方法：</p><ul><li>Batch Normalization（BN， 2015年）</li><li>Layer Normalization（LN， 2016年）</li><li>Instance Normalization（IN， 2017年）</li><li>Group Normalization（GN， 2018年）</li><li>Switchable Normalization（SN， 2018年）；</li></ul><figure><img src="img_1.png" alt="几种归一化方法g" /><figcaption aria-hidden="true">几种归一化方法g</figcaption></figure><h4 id="问题1-transformer-为什么要使用ln而不是-bn">问题1 transformer 为什么要使用LN而不是 BN？</h4><p>在<a href="https://arxiv.org/pdf/2003.07845.pdf">paper: Rethinking Batch Normalization in Transformers</a>中, 作者对比了cv和nlp的BN, 得出的结论是在nlp数据上基于batch的统计信息不稳定性过大(相比cv的数据)，导致bn在nlp上效果差。相比之下layer norm能够带来更稳定的统计信息，有利于模型学习</p><p>Batch Normalization主要的问题是计算归一化统计量时计算的样本数太少，在RNN等动态模型中不能很好的反映全局统计分布信息，而Layer Normalization根据样本的特征数做归一化，是batch size无关的，只取决于隐层节点的数量，较多的隐层节点数量能保证Layer Normalization归一化统计分布信息的代表性。</p><h4 id="问题2.-in直观上怎么理解">问题2. IN直观上怎么理解？</h4><p>在计算机视觉中，IN本质上是一种Style Normalization，它的作用相当于把不同的图片统一成一种风格。另外，既然IN和BN都会统一图片的风格，那么在Generator里加IN或BN应该是不利于生成风格多样的图片的，论文中也进行了展示：</p><p><img src="https://pic2.zhimg.com/v2-235433127838fca762ebd10511de9ca7_b.jpg" /></p><p>图e是在generator中加了BN的结果，图f是在generator中加了IN的结果。果然崩了，IN崩得尤其厉害。</p><h3 id="激活函数">激活函数</h3><ul><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2F4_fjPJ90ir.png?alt=media&amp;token=9fb9e321-aed2-4c7b-bb6b-4762b7a38c81" /></li><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FADUH_mebAQ.png?alt=media&amp;token=65efc150-0f7b-42f9-8657-1ca131bfd8b5" /></li><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FMWiLJVveHv.png?alt=media&amp;token=55bc24ce-0531-4702-8d3d-394595bd4d6e" /></li><li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FFztLjU8MxY.png?alt=media&amp;token=d7421445-afca-439e-85be-8631db133a41" /></li><li>https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/</li></ul><h3 id="损失函数">损失函数</h3><p>tensorflow中的损失函数:</p><ul><li>sparse_categorical_crossentropy: target是interger list，形状n*1；</li><li>categorical_crossentropy: target 是one-hot vector，target的shape跟模型输出一致，是n*k(类)。<ul><li><span class="math display">\[C E(x)=-\sum\limits_{i=1}^{C} y_{i} \log f_{i}(x)\]</span></li></ul></li><li>binary_crossentropy: target 是interger list。 <span class="math display">\[B C E(x)_{i}=-\left[y_{i} \log f_{i}(x)+\left(1-y_{i}\right) \log \left(1-f_{i}(x)\right)\right]\]</span></li><li>如果输出标签维度为1，只能使用binary_crossentropy，否则程序会报错。不能直接使用categorical_crossentropy 或者sparse_categorical_crossentropy</li><li>如果输出标签维度为K：使用categorical_crossentropy 或者sparse_categorical_crossentropy</li><li>单标签多分类（multi-class）：softmax + CE</li><li>二分类：sigmoid + BCE</li><li>多标签多分类（multi-label）的情况：sigmoid + BCE</li><li>最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</li></ul><h4 id="优化算法">优化算法</h4><h1 id="参考资料">参考资料</h1><ol type="1"><li><a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization</a></li><li><a href="https://arxiv.org/pdf/1607.06450v1.pdf">Layer Normalizaiton</a></li><li><a href="https://arxiv.org/pdf/1607.08022.pdf">Instance Normalization</a></li><li><a href="https://github.com/DmitryUlyanov/texture_nets">code</a></li><li><a href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a></li><li><a href="https://arxiv.org/pdf/1806.10779.pdf">Switchable Normalization</a></li><li><a href="https://github.com/switchablenorms/Switchable-Normalization">code</a></li><li><a href="https://blog.csdn.net/liuxiao214/article/details/81037416">有公式推导，写的很棒</a></li><li><a href="https://www.jianshu.com/p/05de1f989790">用书比喻图像很好理解</a></li><li><a href="https://zhuanlan.zhihu.com/p/61248211">Conditional Batch Normalization 详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/57875010">从Style的角度理解Instance Normalization</a></li><li><a href="https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf">在时序数据上应用conv1D</a></li><li>https://zhuanlan.zhihu.com/p/57875010</li><li>https://stackoverflow.com/questions/47787011/how-to-disable-dropout-while-prediction-in-keras</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;数据表示&quot;&gt;数据表示&lt;/h1&gt;
&lt;p&gt;在深度学习中如何表示现实中的事物？&lt;/p&gt;
&lt;p&gt;Let’s make data tensors more concrete with a few examples similar to what you’ll encount</summary>
      
    
    
    
    <category term="AI" scheme="https://chiechie.github.io/categories/AI/"/>
    
    
    <category term="深度学习" scheme="https://chiechie.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="最佳实践" scheme="https://chiechie.github.io/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    
    <category term="low level" scheme="https://chiechie.github.io/tags/low-level/"/>
    
  </entry>
  
  <entry>
    <title>《the book of why》读书笔记-7</title>
    <link href="https://chiechie.github.io/2021/06/10/reading_notes/the-book-of-why/judea-pearl_the-book-of-why7/"/>
    <id>https://chiechie.github.io/2021/06/10/reading_notes/the-book-of-why/judea-pearl_the-book-of-why7/</id>
    <published>2021-06-10T00:33:38.000Z</published>
    <updated>2021-06-24T07:10:06.042Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>the book of why 的第7章 征服干预之峰</p></blockquote><h2 id="前门规则和后门调整">前门规则和后门调整</h2><ol type="1"><li>do条件概率和条件概率的区别是：当有混杂因子存在时，使用条件概率可能得出假的相关性。</li><li>那么除了随机对照试验，还有什么办法能够排除掉混杂因子带来的影响呢？后门调整和前门标准。后门调整即使用便相关系数代替相关系数，前门调整是，在特殊的因果模型的情况下，使用条件概率代替do算子。 <img src="./img.png" alt="img.png" /></li></ol><h2 id="参考">参考</h2><ol type="1"><li><a href="http://bayes.cs.ucla.edu/WHY/why-intro.pdf">the book of why-微信读书</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;the book of why 的第7章 征服干预之峰&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;前门规则和后门调整&quot;&gt;前门规则和后门调整&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;do条件概率和条件概率的区别是：当有混杂因子存在时，</summary>
      
    
    
    
    <category term="AI" scheme="https://chiechie.github.io/categories/AI/"/>
    
    
    <category term="人工智能" scheme="https://chiechie.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="哲学" scheme="https://chiechie.github.io/tags/%E5%93%B2%E5%AD%A6/"/>
    
    <category term="根因分析" scheme="https://chiechie.github.io/tags/%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90/"/>
    
    <category term="因果分析" scheme="https://chiechie.github.io/tags/%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90/"/>
    
    <category term="因果推断" scheme="https://chiechie.github.io/tags/%E5%9B%A0%E6%9E%9C%E6%8E%A8%E6%96%AD/"/>
    
    <category term="贝叶斯" scheme="https://chiechie.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>多世界理论</title>
    <link href="https://chiechie.github.io/2021/06/09/reading_notes/phisics/duoshijie/"/>
    <id>https://chiechie.github.io/2021/06/09/reading_notes/phisics/duoshijie/</id>
    <published>2021-06-09T06:26:46.000Z</published>
    <updated>2021-06-28T02:09:13.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总结">总结</h2><ol type="1"><li>多世界理论是埃弗里特（Hugh Everett III, 1930 - 1982）在1957年提出的. 这个理论的核心是：量子理论是一个自洽的理论，不需要借助经典概念和任何额外的假设就能描述和解释一切现象，包括我们日常生活感受的经典世界和量子测量。根据这个理论，波包永远也不会塌缩，任何物体无论大小都是量子的，都可以用波函数描述。 整个宇宙的波函数<span class="math inline">\(\Psi\)</span>可以写成很多个分量的叠加 <span class="math display">\[\Psi=\varphi_{1}+\varphi_{2}+\ldots+\varphi_{n}+\ldots\]</span></li><li>其中的每一个分量<span class="math inline">\(\varphi_{n}\)</span>代表一个平行世界，这些平行世界真实存在、平行演化、互不干扰，偶尔发生干涉。 3 对多世界理论持反对意见的人其实无法给出任何令人信服的理由，主要是心理上无法接受其他平行世界的存在。</li><li>埃弗里特一开始就指出了波包塌缩理论的逻辑缺陷：当存在两个或两个以上观察者的时候，观察者们会给出不同的波包塌缩时间。埃弗里特用了一个例子来说明这个逻辑缺陷，他的例子和维格纳朋友佯谬类似。</li></ol><h2 id="薛定谔的猫猫">薛定谔的猫猫</h2><ol type="1"><li>薛定谔猫是一个关于量子测量的思想实验。在薛定谔的最初设想中，被测量的是正在衰变的放射性元素，这里用光子的偏振代替。 <img src="img.png" alt="img.png" /></li><li>每一个光子有两个可能的偏振态，水平偏振和垂直偏振。一般情况下，光子既不是水平偏振也不是垂直偏振，而是处于它们的叠加态。（类似复数）</li><li>现在假设有一个封闭的实验室，里面有一只猫猫，和光子偏振的测量设备。实验开始时，一个光子入射穿方解石，出来后落在检测屏上。如果落在检测评上方，什么也不会发生，猫猫依然是活的，如果落在下方，会触发打开毒气瓶的开关，把猫猫毒死。</li><li>光子落在屏幕上之后，猫猫和光子发生了纠缠。</li><li>现在让我们用量子力学来描述这个实验。实验开始前，实验室的波函数可以写成: <span class="math display">\[\left|\Psi_{0}\right\rangle=(\alpha|H\rangle+\beta|V\rangle) \otimes \mid live cat \rangle\]</span> 这时猫和光子没有任何关联。实验结束后，实验室的波函数变成了: <span class="math display">\[\begin{aligned}\left|\Psi_{1}\right\rangle&gt;=\alpha &amp;|H\rangle \otimes \mid \text { live cat }\rangle+\\ &amp;\beta|V\rangle \otimes \mid \text { dead cat }\rangle \end{aligned}\]</span></li><li>现在的问题是，上面的波函数有两部分：在第一部分里，猫是活的；在第二部分里，猫是死的。那这猫究竟是活的还是死的呢？如果你重复这个实验1000次，每次实验结束后打开实验室查看猫的死活，那么你会发现大约有 1000 |α|2次猫是活的，大约1000 | β |2次猫是死的。你不会看到一只既活又死的猫。</li><li>这个实验的本质是，波函数里明明有两个实验结果，当我们去观察时，却只能看到一个。为什么会这样? <img src="img_1.png" alt="img_1.png" /></li><li>在量子力学的框架下，如何解释这个现象？波包塌缩理论：当我们打开实验室去观察时，波函数发生了塌缩。它有<span class="math inline">\(\alpha^2\)</span>的几率发生如下塌缩： <span class="math display">\[\left|\Psi_{0}\right\rangle=\alpha|H\rangle \otimes \mid live cat \rangle\]</span> <span class="math inline">\(\beta^2\)</span>的几率发生如下塌缩： <span class="math display">\[\left|\Psi_{0}\right\rangle=\beta|V\rangle \otimes \mid dead cat \rangle\]</span> 也就是说，有一部分波函数会神秘的消失。</li><li>虽然波包塌缩理论能解释实验结果，但有很多缺点，比如没有描述塌缩究竟怎样发生的，持续了多长时间</li><li>另外一种理论就是多世界理论，系统的波函数<span class="math inline">\(\Psi\)</span>中两个分量，分别代表两个不同世界：一种世界里面，猫猫是活的，另外一种世界，猫猫是死的。这两种世界一样真实，并行存在。由于量子力学的演化是线性的，这两种世界会独立演化，互不影响。当观测者打开实验室的门，在一种世界里，他会看到一直活着的猫猫，感觉不到死猫，这个时候，似乎波函数发生了公式描述的塌缩。在另外一个世界，他会看到相反的景象，这似乎象征着波函数发生了第二种塌缩。从这个角度看，波包塌缩其实是多世界理论的推论，迄今没有任何实验可以推翻多世界理论。</li><li>当一个世界一分为二时，多世界理论是不是预示着能量不守恒了？并没有，分裂之后，只是几率变化了。分类之后的<span class="math inline">\(\Psi_1\)</span>表示，一个实验室有两种状态，一种状态里光子是水品偏振和猫是活的，另外一种状态里，光子是垂直偏振，猫猫却死了。这时，仍然只有一个光子，和一只猫。测量，让猫猫和光子的状态发生了纠缠，并没有增加数量 11。 量子图灵机非常推崇多世界理论，这个宇宙有无限个世界，跟0到1之间的实数一样多，对于薛定谔猫猫实验，一开始有无限多个世界，他们都完全相同。光子偏振测量结束后，这无穷多个世界分成了两组，分别由<span class="math inline">\(\Psi_1\)</span>中的两个分量描述，两组所占的比重分别为</li><li>多世界理论的核心：整个宇宙都是由微观粒子构成，宏观物体和围观粒子没有本质区别，他们也由波函数描述。一个波函数的不同分量代表不同的世界，每个分量都同样真实的，他们并行存在。相对于其他量子力学的解释，多世界理论不需要在量子力学的基本框架之外的额外假设。</li></ol><h2 id="谁对谁错">谁对谁错?</h2><blockquote><p>以薛定谔猫为例介绍了波包塌缩理论和多世界理论，它们都能解释现有实验结果。而且迄今为止没有人能设计一个实验来区别这两种理论，比如直接观测到另外一个世界或波包塌缩过程。那么究竟哪一个理论是正确的呢？实验是科学理论的试金石，现在这块试金石失效了，我们该如何判断这两种理论的对错？埃弗里特在他长论文的最后一个附录里谈到了物理理论的对错优劣问题，他认为除了寻求实验的验证，我们还应该考查理论的逻辑性、有用性、简单性等。下面我们就从这些方面详细对比波包塌缩理论和多世界理论。</p></blockquote><h3 id="逻辑性">逻辑性</h3><p>埃弗里特指出波包塌缩理论的逻辑缺陷：当存在两个或两个以上观察者的时候，观察者们会给出不同的波包塌缩时间。埃弗里特用了一个例子来说明这个逻辑缺陷，他的例子和维格纳朋友佯谬类似。在图3描述的实验中，我们把猫换成观察者爱丽丝，同时移走毒气瓶。如果光子落在检测屏上方，她就记录“上”；如果光子落在检测屏下方，她就记录“下”。对于爱丽丝来说，波包塌缩在光子和检测屏碰撞的一刻就已经发生了，因为她每次都明确地观察到了光子落在了上方或下方。现在假设实验室外面还有一个观察者鲍勃，在实验完成前，他和实验室没有任何相互作用。对于鲍勃来说，在他打开实验室以前，实验室由如下波函数描述 <span class="math display">\[\left|\Psi_{3}\right\rangle=\alpha|H\rangle \otimes|\mathrm{Up}\rangle+\beta|V\rangle \otimes \mid Down \rangle \]</span> 波包还没塌缩。假设鲍勃在实验结束后一周打开了实验室，这时波包塌缩了。他对爱丽丝的记录没有任何异议，但他坚持说波包塌缩是在他进入实验室的一刻发生的，爱丽丝当然不同意。于是矛盾产生了。这就是波包塌缩理论的逻辑缺陷。文献[10]对这个逻辑问题有更详细的描述。</p><h3 id="简单性">简单性</h3><p>这里指的是概念和理论框架的简单。多世界理论显然在概念上更简单，除了量子力学的基本框架，它不需要任何额外的假设。而波包塌缩是量子力学基本框架之外的一个额外假设。在量子力学里，量子态都是随时间进行连续和幺正的演化；而波包塌缩是不连续和非幺正的。因此，波包塌缩必须是一个独立的假设，不可能从量子力学的基本框架推出来。这个假设非常的不物理：塌缩是怎样的物理过程？经历了多长时间？如何界定外部观察者？这些问题迄今没有令人满意的答案。当我们考虑整个宇宙时，波包塌缩假说显得更是不合理。一方面，宇宙的外部没有观察者，所以整个宇宙的波函数应该连续地随时间幺正演化，不会经历随机的波包塌缩；另一方面，我们人类又时时刻刻在进行类似图3的实验，波包在随机的塌缩。这样宇宙的波函数由于我们的观察不再随时间幺正演化。多世界理论则不会导致这样矛盾的结果。</p><h3 id="刚性">刚性</h3><p>如果一个理论不能随意改动，我们就说这个理论具有刚性。多世界理论在概念上更简单，因为它不需要任何额外的假设。这也使它具有了刚性。量子力学已经被大量实验证实，它的基本理论框架是不能随意改动的。如果你能改动量子力学的基本框架，那你一定在引导一场新的物理革命。与之相反，波包塌缩理论显然是可以随意改动的。比如，我可以假设波包塌缩有一个中间过程：任何波函数都先塌缩为一个等权重量子态然后再塌缩到目标态。根据这个假设，公式(6) 描述的塌缩应该改写为</p><p><span class="math display">\[\left|\Psi_{1}\right\rangle \rightarrow \frac{1}{\sqrt{2}}(|H\rangle \otimes \mid live cat \rangle+|V\rangle \otimes \mid dead cat \left.\rangle\right) \rightarrow|H\rangle \otimes \mid live cat \rangle\]</span></p><p>这个修改后的波包塌缩理论同样能解释图3中的实验结果和其他量子测量结果。更糟糕的是，我们可以对波包塌缩理论做任意类似的改动。所以波包塌缩理论刚性非常差，完全是一团可以被随意揉捏的面。笔者认为任何真理和美都具有类似的刚性。一首动听的乐曲具有刚性：它的每一个音符都不能随意改动；一首美的诗具有刚性：它的每一个字都不能随意改动；一朵漂亮的鲜花具有刚性：花瓣的颜色、形状等都不能随意变动，所以花店老板总是对她店里的鲜花小心呵护。由于篇幅限制，这里不展开讨论了。</p><h3 id="有用性">有用性</h3><p>波包塌缩理论似乎只能用来解释类似图3中的量子测量实验，笔者不清楚它还有什么其他用处。多世界理论则帮助笔者解开了一些长久的困惑。按照进化论，我们人类是由非常初级的化学分子一步一步进化而来。每一步进化都是一个小概率事件，所以最后一步一步进化成具有智慧的人，概率是非常、非常小的。即使整个进化过程的时间很长，依然让人觉得不好接受：这么小概率的事件居然发生了。按照多世界理论，这种进化则是必然的，因为在多世界理论中任何小概率事件都会实现。波函数|Ψ1〉中有两个分量：无论α多小，活猫的世界都存在；无论β多小，死猫的世界都存在。也就是说，宇宙从来都不做选择，它只是按照比重不停地分裂为更多的世界。基于这个认识，让我们再来看生物进化。每一步进化其实都是一个化学反应过程：早期的进化是小分子组合成大分子，后来则是基因突变。而任何化学反应都是一个量子演化过程。按照多世界理论，无论事件发生的概率多小它都会发生。概率小只是说明在众多的世界里只有很少一部分生活着我们这样的智慧生物，绝大多数世界里地球上没有生命或者生命处于极其初始的状态。</p><h2 id="自由意志和多世界">自由意志和多世界</h2><p>按照经典物理，一旦初始条件给定，系统随后的演化就是唯一确定的。形象点说就是你所做的一切都是命中注定的，你的一生在你出生的那个时刻就已经确定了，自由意志只是一种假象。从经典物理理论出发，这个观点真是难以辩驳。后来有了量子力学，理论框架里含有内禀的几率，自由意志似乎不再是假象。但仔细一想，即使有了量子力学，自由意志似乎依然是假象。按照波包塌缩理论，几率只发生在外部观察者进行观察时。整个宇宙并没有外部观察者，宇宙的波函数依然按照量子力学进行确定的演化，所以自由意志依然是假象。但是按照多世界理论，整个宇宙的波函数虽然在确定地演化，但是它却在不停地分裂成不同的世界，观察者究竟感受和经历哪一个特定的世界是随机的，他不确定自己的未来是属于哪一个世界。这样，对于每一个世界来说，人是有自由意志的。于是我们有了一个非常美妙的结论：在一个确定演化的宇宙里存在真正的自由意志。</p><h2 id="世界是量子的">世界是量子的</h2><p>量子力学是一场颠覆性的物理革命，彻底推翻了很多经典的概念。但是非常有意思的是，几乎所有的量子力学的创立者都始终没有彻底摆脱经典物理的枷锁。这个问题的根源是波函数。波函数是抽象的希尔伯特空间中的一个向量，它和我们感知的世界没有直接的联系。为了建立波函数和现实世界的联系，这些伟大的物理学家开始回头在经典物理中寻求答案。爱因斯坦认为波函数根本就不能完整地描述自然，最终的理论一定是一个经典的理论，即所谓的隐变量理论。以玻尔和海森伯为代表的哥本哈根学派虽然认为波函数完整地描述了自然，但波函数和现实世界的联系需要通过经典仪器来完成。但最令人迷惑的事情是，没有一篇文章或一本书清晰明确阐明哥本哈根学派，他们的观点弥散在玻尔和他门徒无穷的文章和讲话里，每次他们似乎在谈论同一个观点，仔细一读似乎又总是有些变化。朗道在他的《量子力学》第一节中写了这样一段话，量子力学在物理理论中占有一个很不平常的地位；它把经典力学作为一种极限情形而包含之，但在它的自身表述中，同时又需要这一极限情形。</p><p>这或许能概括哥本哈根学派关于量子力学和经典力学之间关系的模棱两可的态度吧。哥本哈根学派无论怎么表述他们的观点，有一点很明确，他们没有彻底摆脱经典力学的枷锁。德布罗意试图将波函数解释成一种经典波，发展了导波理论，后来玻姆独立发展这个理论。这个理论认为粒子本质上是经典的。薛定谔似乎是这些量子先贤中唯一一位不走回头路的。他曾经在1952年的一次演讲中批评了当时流行的哥本哈根学派，指出波函数中的每一个分量都可能同时存在[11]。这当然就是多世界理论的本质。但是很遗憾，薛定谔并没有进一步发展这个想法。</p><h2 id="参考">参考</h2><ol type="1"><li><a href="https://weibo.com/ttarticle/p/show?id=2309404646458711212151#_0">埃弗里特和他的多世界理论-吴飙-weibo</a></li><li><a href="https://www.phy.pku.edu.cn/wubiao/dfiles/pw/aifulitehetadeduoshijielilun.pdf">埃弗里特和他的多世界理论-吴飙-paper</a></li><li><a href="https://www.phy.pku.edu.cn/wubiao/pxsj.htm">吴飙-主页</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;多世界理论是埃弗里特（Hugh Everett III, 1930 - 1982）在1957年提出的. 这个理论的核心是：量子理论是一个自洽的理论，不需要借助经典概念和任何额外的假设就能描述和解释一切现象</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="物理" scheme="https://chiechie.github.io/tags/%E7%89%A9%E7%90%86/"/>
    
    <category term="平行世界" scheme="https://chiechie.github.io/tags/%E5%B9%B3%E8%A1%8C%E4%B8%96%E7%95%8C/"/>
    
    <category term="自由意志" scheme="https://chiechie.github.io/tags/%E8%87%AA%E7%94%B1%E6%84%8F%E5%BF%97/"/>
    
    <category term="薛定谔的猫猫" scheme="https://chiechie.github.io/tags/%E8%96%9B%E5%AE%9A%E8%B0%94%E7%9A%84%E7%8C%AB%E7%8C%AB/"/>
    
  </entry>
  
  <entry>
    <title>《编程:隐匿在计算机硬件背后的语言 》的读书笔记</title>
    <link href="https://chiechie.github.io/2021/06/08/reading_notes/computer/coding/"/>
    <id>https://chiechie.github.io/2021/06/08/reading_notes/computer/coding/</id>
    <published>2021-06-08T01:09:12.000Z</published>
    <updated>2021-06-11T02:39:26.044Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二进制加法器">12 二进制加法器</h2><ol type="1"><li>电路中的逻辑门能实现二进制加法。</li></ol><blockquote><p>逻辑门是认为设计的一种电路走线，为了映射到逻辑上的加法运算。</p><p>逻辑门是链接物理世界和信息世界的桥梁，信息世界以逻辑求和为根基，开始构建信息世界的高楼大厦。</p></blockquote><h2 id="存储器组织">16 存储器组织</h2><ol type="1"><li><p>我们习惯于将可能用到的事物先存起来，在需要时将它们取出。从技术角度来讲，这个过程称为先存储后访问。存储器的职责和作用就在于此，它负责保障这两个过程之间信息完好无损。</p></li><li><p>存储信息都要利用不同种类的存储器。比如，保存文本信息的不二之选就是纸张，而磁带则更适于存储音乐和电影。</p></li><li><p>电报继电器（Telegraph Relays）——以一定形式组织起来构成逻辑门，然后再形成触发器——同样具备保存信息的能力</p></li><li><p>随机访问存储器（RAM）一断电，数据就消失了。因此随机访问存储器也叫易失性（volatile）存储器。</p><blockquote><p>一个辛辛苦苦装满65,536字节珍贵数据的64 K×8 RAM阵列，如果断掉电源，首先所有的电磁铁都将因为没有电流而失去磁性，随着“梆”的一声，金属片将弹回原位，所有继电器将还原到未触发状态。RAM中存储的数据呢？它们将如风中残烛般消失在黑暗中</p></blockquote></li></ol><h2 id="从算盘到芯片">18 从算盘到芯片</h2><h2 id="两种微处理器">19 两种微处理器</h2><ol type="1"><li><p>微处理器将中央处理器的所有组建整合到一个硅芯片上。</p></li><li><p>第一个微处理器，intel 4004，包含了2300个晶体管</p></li><li><p>三十年过去了，一个微处理器中的晶体管数量逼近1000w个。</p></li><li><p>摩托罗拉公司，从20实际50年代生产半导体和晶体管，在1974年8月推出了6800处理器。</p></li><li><p>得克萨斯仪器在1974年，推处4位的处理器TMS1000，</p></li><li><p>英特尔为8080最初定的价格为360美元，8080是一个8位的微处理器，它包括6000个晶体管，运行的时钟频率为2 MHz，寻址空间为64 KB。</p></li><li><p>这些芯片被称为“单芯片微处理器”（single-chip microprocessors</p></li><li><p>除了处理器之外，计算机还需要其他一些设备，至少要包括存储器（RAM），输入设备（键盘），输出设备（显示屏），以及其他一些能把所有构件连接在一块的芯片（主板，机箱）。</p></li><li><p>通过观测芯片的输入、输出信号，特别是芯片的指令集来理解微处理器的工作原理。</p></li><li><p>微处理器通过管脚提供了处理器的输入和输出访问接入点 <img src="img.png" alt="img.png" /></p></li><li><p>所有电气或电子设备都需要某种电源来供电</p></li><li><p>8080的一个特殊的地方就是它需要三种电源电压：管脚20必须接到5V的电压；管脚11需要接到-5V的电压；管脚28需接12V的电压；管脚2接地。（英特尔在1976年发布了8085芯片，目的就是简化对这些电源的要求）.</p><ul><li>从芯片引出的箭头表明这是一个输出（output）信号，这种信号由微处理器控制，计算机的其他芯片对该信号响应。</li><li>指向芯片的箭头表明该信号是一个输入（input）信号，该信号由其他芯片发出，并由8080芯片对其响应。还一些管脚既是输入又是输出。</li></ul></li><li><p>8080有16个用于寻址的输出信号，标记为A0～A15，因此它的可寻址空间大小为<span class="math inline">\(2^16\)</span></p></li><li><p>在计算机中，所有的指令都是3个字节长（1个字节=8个bit），包括1字节的操作码和2字节的地址，指令长什么样？：</p><ul><li>有些指令使8080从存储器的一个特定地址读取字节到微处理器，</li><li>有些指令使8080将一个字节从微处理器写入存储器的特定地址；</li><li>还有些指令使8080在其内部执行而不需要访问RAM</li></ul><p>8080执行完第一条指令后，接着从「存储器」读取第二条指令，并依此类推。这些指令组合在一起构成了计算机程序</p></li><li><p>计算机的指令集包括两条非常重要的指令，我们称之为加载（Load）和保存（Store），每条指令占3个字节</p><ul><li>在Load指令中，第一个字节是操作码，其后的两个字节是要加载的操作数的16位地址。当处理器执行加载指令时，会把该指定地址中的字节加载到累加器。</li><li>当Store指令被执行时，累加器中的内容被保存到该指令指定的地址中。</li></ul></li><li><p>8080芯片的微处理器的内部除累加器外还设置了6个寄存器（register），每个寄存器可以存放一个8位的数。这些寄存器和累加器非常相似，事实上累加器被视为一种特殊的寄存器。</p></li><li><p>寄存器是计算机必不可少的部件吗？理论上不是。很多计算机程序都同时用到多个数据，将这些数据存放在寄存器比存放在存储器更便于访问，因为程序访问内存的次数越少其执行速度就越快。</p></li><li><p>利用指令，可以方便地把一个寄存器存放的数据转移到另一个寄存器</p></li><li><p>有一种存储器叫堆栈：以从底部到顶部的顺序把数据存入存储器，并以相反的顺序把数据从堆栈中取出，因此该技术也称作后进先出存储器（last-in-first-out，LIFO）。在计算机中使用堆栈技术是十分方便的。通常把将数据存入堆栈的过程称作压入（push），把从堆栈取出数据的过程称作弹出（pop）。</p></li><li><p>堆栈的功能是怎样实现的呢？首先，堆栈其实就是一段普通的RAM存储空间，只是这段空间相对独立不另作他用。8080微处理器设置了一个专门的16位寄存器对这段存储空间寻址，这个特殊的寄存器称为堆栈指针（SP，Stack Pointer）。</p></li><li><p>在8080中，执行PUSH指令实际上是把16位的数据保存到堆栈，执行POP指令是把这些数据从堆栈中取回至寄存器。</p></li><li><p>每执行一条PUSH指令，堆栈都会增加两个字节，这可能会导致程序出现一些小错误——堆栈可能会不断增大，最终覆盖掉存储器中保存的程序所必需的代码或数据。这种错误被称作堆栈上溢（stackoverflow）。类似的，如果在程序中过多地使用了POP指令，则会过早地取完堆栈中的数据从而导致类似的错误，这种情况称为堆栈下溢（stack underflow）。</p></li><li><p>根据摩尔定律（Moore’s Law），微处理器中的晶体管数量每18个月翻一倍，增加的这些大量的晶体管用来做什么呢？</p><ul><li>一些晶体管用来适应处理器不断增加的数据宽度——从4位、8位、16位到32位；</li><li>另一些新增的晶体管用来应对新的指令。</li></ul></li><li><p>现代处理器主要使用2种策略来提高运行速度:</p><ul><li>一种就是流水线技术（pipelining），即处理器在执行一条指令的同时读取下一条指令，</li><li>还有一种是Cache（高速缓冲存储器），它是一个设置在处理器内部，访问速度非常快的RAM阵列，用来存放处理器最近要执行的指令</li></ul><p>这两种策略都需要在处理器内部增加更多的逻辑组件和晶体管。</p></li></ol><h2 id="ascii码和字符转换">20　ASCII码和字符转换</h2><ol type="1"><li>存储器中唯一可以存储的东西是bit</li><li>如果我们想把一段文本存下来该怎么办呢？将文本表示成数字。</li><li>可以为每一个字母赋予一个唯一的编码，具有这种功能的系统叫 字符编码集（Coded CharacterSet），系统内的每个独立编码称为字符编码（Character Codes）</li><li>莫尔斯码: 常用字符的编码较短，而不常用字符的编码较长。这样的编码非常适合电报系统，但并不适用于计算机.</li><li>对一个句子进行编码后得到的连续字符通常被称为文本字符串（string）</li><li>电传打字机键盘上的每一个键实质上都起到了转换器的作用，它负责产生二进制编码并且通过输出电缆逐位传输出去</li><li>尽管ASCII码是计算机领域最重要的标准,但是它是太美国化了，在其他国家ASCII码并不适用。</li><li>近几十年来出现了许多不同版本的扩展的ASCII码，多个不同的版本严重影响了编码的一致性，导致了混淆和不兼容。ASCII码被扩展到极致，有的甚至可以对中文、日文和韩文进行编码</li><li>从1988年开始，几大著名计算机公司合作研究出一种用来替代ASCII码的编码系统，取名为Unicode（统一化字符编码标准）。相对于ASCII的7位编码，Unicode采用了16位编码，每一个字符需要2个字节。</li><li>Unicode编码不是从零开始设计的，前128个字符编码——即0000h～007Fh——与ASCII码是一致的</li><li>对于Unicode来讲，它唯一的问题，就是它改变了字符与存储空间之间“单字符，单字节”的等价对应关系。采用ASCII编码方式存储的著作《怒火之花》，其所占据的存储空间约为1 MB。而如果采用Unicode编码，约占2 MB。为了使编码系统兼容，Unicode在存储空间上付出了相应的代价。</li></ol><blockquote><p>UTF-8是unicode的升级版，减少存储空间。</p></blockquote><p>你看到的unicode字符集是这样的编码表： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">I 0049</span><br><span class="line">t 0074</span><br><span class="line">&#x27; 0027</span><br><span class="line">s 0073</span><br><span class="line">  0020</span><br><span class="line">知 77e5</span><br><span class="line">乎 4e4e</span><br><span class="line">日 65e5</span><br><span class="line">报 62a5</span><br></pre></td></tr></table></figure></p><p>每一个字符对应一个十六进制数字。计算机只懂二进制，因此，严格按照unicode的方式(UCS-2)，应该这样存储： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">I 00000000 01001001</span><br><span class="line">t 00000000 01110100</span><br><span class="line">&#x27; 00000000 00100111</span><br><span class="line">s 00000000 01110011</span><br><span class="line">  00000000 00100000</span><br><span class="line">知 01110111 11100101</span><br><span class="line">乎 01001110 01001110</span><br><span class="line">日 01100101 11100101</span><br><span class="line">报 01100010 10100101</span><br></pre></td></tr></table></figure></p><p>这个字符串总共占用了18个字节，但是对比中英文的二进制码，可以发现，英文前9位都是0！浪费啊，浪费硬盘，浪费流量。怎么办？UTF。</p><p>于是，”It's 知乎日报“就变成了： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">I 01001001</span><br><span class="line">t 01110100</span><br><span class="line">&#x27; 00100111</span><br><span class="line">s 01110011</span><br><span class="line">  00100000</span><br><span class="line">知 11100111 10011111 10100101</span><br><span class="line">乎 11100100 10111001 10001110</span><br><span class="line">日 11100110 10010111 10100101</span><br><span class="line">报 11100110 10001010 10100101</span><br></pre></td></tr></table></figure> 和上边的方案对比一下，英文短了，每个中文字符却多用了一个字节。但是整个字符串只用了17个字节，比上边的18个短了一点点。</p><p><img src="img_1.png" alt="img_1.png" /> ## 参考</p><ol type="1"><li><a href="https://weread.qq.com/web/reader/64e32bf071fd5a9164ece6b">编程：隐匿在计算机硬件背后的语言</a></li><li>https://www.zhihu.com/question/23374078/answer/65352538</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;二进制加法器&quot;&gt;12 二进制加法器&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;电路中的逻辑门能实现二进制加法。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;逻辑门是认为设计的一种电路走线，为了映射到逻辑上的加法运算。&lt;/p&gt;
&lt;p&gt;逻辑门是链接物理世</summary>
      
    
    
    
    <category term="阅读" scheme="https://chiechie.github.io/categories/%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="编程" scheme="https://chiechie.github.io/tags/%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
</feed>
