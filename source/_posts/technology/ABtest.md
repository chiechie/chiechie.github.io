---
title: ABtest
author: chiechie
mathjax: true
date: 2021-03-27 23:43:55
tags:
categories:
---
> 现在有很多平台都在做AB test，甚至还做成了一个变形金刚--实验平台，对于不同的推荐对象，给出不同的推荐方案
> 
> 正好目前在调研模型在线更新的流程和方案，似乎像是同一类需求。
> 
> 到底是不是同一类需求？是的话，实验平台的解决方案是什么？我们是否可以借鉴？
> 
> 下面来探讨一下

## 为什么要做A/B测试？

A/B测试很好的避免了由于时间不同而引起的访客样本属性变化的问题，它能让两个版本面对属性相同或近似的访客群体，基于这种情况对所获得的数据才有可比性，分析结果才更加准确、可靠。

A一般指实验组, B一般指 对照组，对照组一般指基线，一般都会有模型。

1. treatment A 一般是重点考察对象，可以是最新版本的模型；treatment B是用来比对A的效果的，可以是上一个版本的模型/基线策略，
2. 为了排除用户群1和用户群2的群体差异，一般也会做BAtest。

    ABtest： treatment A 在用户群1测试，treatment B 在用户群2测试。

    BAtest：treatment B 在用户群1测试，treatment A 在用户群2测试。

3. 实际痛点：多数情况下流量不够，发现实验上线达不到预期（基线策略）直接下线。

## 来自推荐一线的工程师

zhouwei:
做AB实验我总结需要重点考虑三个点：

1. 置信度. 分配多少流量才能保证实验数据可信。
2. 流量分布. 如何保证实验流量和大盘流量的分布一致性。
3. 观察周期. 实验流量需要观察多久结论正确。

## 来自chiechie-其他场景的算法工程师

推荐和一般场景的区别是，前者的流量是稀缺的。除此之外，两者本质都是在做线上模型对比

1. 推荐：用户群1在一个时间点只能接受1个treatment，要么是A要么是B, 测试次数是有限的资源。
2. 其他场景：比如异常检测，测试数据集1在一个时间点可接受多个treatment。可以无线测，反正是一个莫得感情的信息流。


## 总结下

大家要解决的问题是相同的

1. 有不同的模型（解决方案）
2. 模型有持续更新的版本

需要根据线上情况选择性能最好的模型